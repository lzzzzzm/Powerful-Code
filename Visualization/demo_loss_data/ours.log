2024-05-07 10:32:40,074 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.0 (default, Nov  6 2019, 21:49:08) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 4090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.6, V11.6.55
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.9.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.25.1
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc4+ad20d41
spconv2.0: True
------------------------------------------------------------

2024-05-07 10:32:41,086 - mmdet - INFO - Distributed training: True
2024-05-07 10:32:42,076 - mmdet - INFO - Config:
point_cloud_range = [-40, -40, -1.0, 40, 40, 5.4]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDatasetOccpancy'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='PrepareImageInputs',
        is_train=True,
        data_config=dict(
            cams=[
                'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
            ],
            Ncams=6,
            input_size=(256, 704),
            src_size=(900, 1600),
            resize=(-0.06, 0.11),
            rot=(-5.4, 5.4),
            flip=True,
            crop_h=(0.0, 0.0),
            resize_test=0.0),
        sequential=True),
    dict(type='LoadAnnotations'),
    dict(
        type='LoadOccGTFromFileCVPR2023',
        scale_1_2=True,
        scale_1_4=True,
        scale_1_8=True),
    dict(
        type='BEVAug',
        bda_aug_conf=dict(
            rot_lim=(0, 0),
            scale_lim=(1.0, 1.0),
            flip_dx_ratio=0.5,
            flip_dy_ratio=0.5),
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='PointToMultiViewDepth',
        downsample=1,
        grid_config=dict(
            x=[-40, 40, 0.8],
            y=[-40, 40, 0.8],
            z=[-1, 5.4, 0.8],
            depth=[2.0, 42.0, 0.5]),
        load_semantic=False),
    dict(type='LoadRaysFromMultiViewImage', downsample=16, rays_number=1000),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=[
            'img_inputs', 'gt_depth', 'voxel_semantics', 'voxel_semantics_1_2',
            'voxel_semantics_1_4', 'voxel_semantics_1_8', 'rays_bundle',
            'depth_map', 'flip_dx', 'flip_dy'
        ])
]
test_pipeline = [
    dict(
        type='PrepareImageInputs',
        data_config=dict(
            cams=[
                'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
            ],
            Ncams=6,
            input_size=(256, 704),
            src_size=(900, 1600),
            resize=(-0.06, 0.11),
            rot=(-5.4, 5.4),
            flip=True,
            crop_h=(0.0, 0.0),
            resize_test=0.0),
        sequential=True),
    dict(type='LoadAnnotations'),
    dict(
        type='BEVAug',
        bda_aug_conf=dict(
            rot_lim=(0, 0),
            scale_lim=(1.0, 1.0),
            flip_dx_ratio=0.5,
            flip_dy_ratio=0.5),
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        is_train=False),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img_inputs'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/bevdetv3-nuscenes_infos_half_train.pkl',
        pipeline=[
            dict(
                type='PrepareImageInputs',
                is_train=True,
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(256, 704),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True),
            dict(type='LoadAnnotations'),
            dict(
                type='LoadOccGTFromFileCVPR2023',
                scale_1_2=True,
                scale_1_4=True,
                scale_1_8=True),
            dict(
                type='BEVAug',
                bda_aug_conf=dict(
                    rot_lim=(0, 0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='PointToMultiViewDepth',
                downsample=1,
                grid_config=dict(
                    x=[-40, 40, 0.8],
                    y=[-40, 40, 0.8],
                    z=[-1, 5.4, 0.8],
                    depth=[2.0, 42.0, 0.5]),
                load_semantic=False),
            dict(
                type='LoadRaysFromMultiViewImage',
                downsample=16,
                rays_number=1000),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=[
                    'img_inputs', 'gt_depth', 'voxel_semantics',
                    'voxel_semantics_1_2', 'voxel_semantics_1_4',
                    'voxel_semantics_1_8', 'rays_bundle', 'depth_map',
                    'flip_dx', 'flip_dy'
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True,
        stereo=True,
        filter_empty_gt=False,
        img_info_prototype='bevdet4d',
        multi_adj_frame_id_cfg=(1, 9, 1)),
    val=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/bevdetv3-nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='PrepareImageInputs',
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(256, 704),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True),
            dict(type='LoadAnnotations'),
            dict(
                type='BEVAug',
                bda_aug_conf=dict(
                    rot_lim=(0, 0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                is_train=False),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img_inputs'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR',
        stereo=True,
        filter_empty_gt=False,
        img_info_prototype='bevdet4d',
        multi_adj_frame_id_cfg=(1, 9, 1)),
    test=dict(
        type='NuScenesDatasetOccpancy',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/bevdetv3-nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='PrepareImageInputs',
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(256, 704),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                sequential=True),
            dict(type='LoadAnnotations'),
            dict(
                type='BEVAug',
                bda_aug_conf=dict(
                    rot_lim=(0, 0),
                    scale_lim=(1.0, 1.0),
                    flip_dx_ratio=0.5,
                    flip_dy_ratio=0.5),
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                is_train=False),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img_inputs'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR',
        stereo=True,
        filter_empty_gt=False,
        img_info_prototype='bevdet4d',
        multi_adj_frame_id_cfg=(1, 9, 1)))
evaluation = dict(
    interval=4,
    pipeline=[
        dict(
            type='PrepareImageInputs',
            data_config=dict(
                cams=[
                    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                ],
                Ncams=6,
                input_size=(256, 704),
                src_size=(900, 1600),
                resize=(-0.06, 0.11),
                rot=(-5.4, 5.4),
                flip=True,
                crop_h=(0.0, 0.0),
                resize_test=0.0),
            sequential=True),
        dict(type='LoadAnnotations'),
        dict(
            type='BEVAug',
            bda_aug_conf=dict(
                rot_lim=(0, 0),
                scale_lim=(1.0, 1.0),
                flip_dx_ratio=0.5,
                flip_dy_ratio=0.5),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            is_train=False),
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(type='Collect3D', keys=['points', 'img_inputs'])
            ])
    ])
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d'
load_from = 'ckpts/forward_projection-r50-4d-stereo-pretrained.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
class_weights = [
    0.0727, 0.0692, 0.0838, 0.0681, 0.0601, 0.0741, 0.0823, 0.0688, 0.0773,
    0.0681, 0.0641, 0.0527, 0.0655, 0.0563, 0.0558, 0.0541, 0.0538, 0.0468
]
data_config = dict(
    cams=[
        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',
        'CAM_BACK', 'CAM_BACK_RIGHT'
    ],
    Ncams=6,
    input_size=(256, 704),
    src_size=(900, 1600),
    resize=(-0.06, 0.11),
    rot=(-5.4, 5.4),
    flip=True,
    crop_h=(0.0, 0.0),
    resize_test=0.0)
grid_config = dict(
    x=[-40, 40, 0.8],
    y=[-40, 40, 0.8],
    z=[-1, 5.4, 0.8],
    depth=[2.0, 42.0, 0.5])
downsample_rate = 16
multi_adj_frame_id_cfg = (1, 9, 1)
numC_Trans = 80
grid_config_bevformer = dict(
    x=[-40, 40, 0.8], y=[-40, 40, 0.8], z=[-1, 5.4, 0.8])
bev_h_ = 100
bev_w_ = 100
bev_z = 8
backward_num_layer = [1, 2, 2]
backward_numC_Trans = 80
_dim_ = 160
_pos_dim_ = 40
_ffn_dim_ = 320
_num_levels_ = 1
num_stage = 3
num_classes = 18
empty_idx = 17
intermediate_pred_loss_weight = [0.5, 0.25, 0.125]
model = dict(
    type='SparseMSFBOcc',
    scene_aabb=[-40, -40, -1.0, 40, 40, 5.4],
    backward_num_layer=[1, 2, 2],
    num_stage=3,
    bev_w=100,
    bev_h=100,
    bev_z=8,
    num_classes=18,
    class_weights=[
        0.0727, 0.0692, 0.0838, 0.0681, 0.0601, 0.0741, 0.0823, 0.0688, 0.0773,
        0.0681, 0.0641, 0.0527, 0.0655, 0.0563, 0.0558, 0.0541, 0.0538, 0.0468
    ],
    empty_idx=17,
    intermediate_pred_loss_weight=[0.5, 0.25, 0.125],
    forward_projection=dict(
        type='BEVDetStereoForwardProjection',
        align_after_view_transfromation=False,
        return_intermediate=True,
        num_adj=8,
        img_backbone=dict(
            type='ResNet',
            depth=50,
            num_stages=4,
            out_indices=(0, 2, 3),
            frozen_stages=-1,
            norm_cfg=dict(type='BN', requires_grad=True),
            norm_eval=False,
            with_cp=True,
            style='pytorch'),
        img_neck=dict(
            type='CustomFPN',
            in_channels=[1024, 2048],
            out_channels=256,
            num_outs=1,
            start_level=0,
            out_ids=[0]),
        img_view_transformer=dict(
            type='LSSVStereoForwardPorjection',
            grid_config=dict(
                x=[-40, 40, 0.8],
                y=[-40, 40, 0.8],
                z=[-1, 5.4, 0.8],
                depth=[2.0, 42.0, 0.5]),
            input_size=(256, 704),
            in_channels=256,
            out_channels=80,
            sid=False,
            collapse_z=False,
            loss_depth_weight=0.05,
            depthnet_cfg=dict(
                use_dcn=False, aspp_mid_channels=96, stereo=True, bias=5.0),
            downsample=16),
        img_bev_encoder_backbone=dict(
            type='CustomResNet3D',
            numC_input=720,
            num_layer=[1, 2, 2],
            with_cp=False,
            num_channels=[80, 160, 320],
            stride=[1, 2, 2],
            backbone_output_ids=[0, 1, 2]),
        img_bev_encoder_neck=dict(
            type='FPN',
            in_channels=[80, 160, 320],
            out_channels=80,
            start_level=0,
            num_outs=3,
            relu_before_extra_convs=True,
            add_extra_convs='on_output',
            conv_cfg=dict(type='Conv3d'),
            norm_cfg=dict(type='BN3d')),
        pre_process=dict(
            type='CustomResNet3D',
            numC_input=80,
            with_cp=False,
            num_layer=[1],
            num_channels=[80],
            stride=[1],
            backbone_output_ids=[0])),
    backward_projection=dict(
        type='SparseBEVFormerBackwardProjection',
        bev_h=100,
        bev_w=100,
        in_channels=80,
        out_channels=80,
        pc_range=[-40, -40, -1.0, 40, 40, 5.4],
        transformer=dict(
            type='SparseBEVFormer',
            use_cams_embeds=False,
            embed_dims=80,
            encoder=dict(
                type='SparseBEVFormerEncoder',
                num_layers=2,
                pc_range=[-40, -40, -1.0, 40, 40, 5.4],
                grid_config=dict(
                    x=[-40, 40, 0.8], y=[-40, 40, 0.8], z=[-1, 5.4, 0.8]),
                data_config=dict(
                    cams=[
                        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                        'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                    ],
                    Ncams=6,
                    input_size=(256, 704),
                    src_size=(900, 1600),
                    resize=(-0.06, 0.11),
                    rot=(-5.4, 5.4),
                    flip=True,
                    crop_h=(0.0, 0.0),
                    resize_test=0.0),
                return_intermediate=False,
                predictor_in_channels=80,
                predictor_out_channels=80,
                predictor_num_calsses=18,
                transformerlayers=dict(
                    type='SparseBEVFormerEncoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiScaleDeformableAttention',
                            embed_dims=80,
                            dropout=0.0,
                            num_levels=1),
                        dict(
                            type='OA_SpatialCrossAttention',
                            pc_range=[-40, -40, -1.0, 40, 40, 5.4],
                            dbound=[2.0, 42.0, 0.5],
                            dropout=0.0,
                            deformable_attention=dict(
                                type='OA_MSDeformableAttention',
                                embed_dims=80,
                                num_points=8,
                                num_Z_anchors=8,
                                num_levels=1),
                            embed_dims=80)
                    ],
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=80,
                        feedforward_channels=320,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    feedforward_channels=320,
                    ffn_dropout=0.0,
                    operation_order=('self_attn', 'predictor', 'cross_attn',
                                     'norm', 'ffn', 'norm')))),
        positional_encoding=dict(
            type='CustormLearnedPositionalEncoding',
            num_feats=40,
            row_num_embed=100,
            col_num_embed=100)),
    occupancy_head=dict(
        type='OccupancyHead',
        in_channels=80,
        out_channels=80,
        num_classes=18,
        empty_idx=17,
        class_weights=[
            0.0727, 0.0692, 0.0838, 0.0681, 0.0601, 0.0741, 0.0823, 0.0688,
            0.0773, 0.0681, 0.0641, 0.0527, 0.0655, 0.0563, 0.0558, 0.0541,
            0.0538, 0.0468
        ],
        up_sample=True),
    nerf_head=dict(
        type='NeRFHead',
        in_channels=80,
        out_channels=80,
        render_dep_loss=dict(type='SiLogLoss', loss_weight=1.0)))
bda_aug_conf = dict(
    rot_lim=(0, 0), scale_lim=(1.0, 1.0), flip_dx_ratio=0.5, flip_dy_ratio=0.5)
share_data_config = dict(
    type='NuScenesDatasetOccpancy',
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    modality=dict(
        use_lidar=False,
        use_camera=True,
        use_radar=False,
        use_map=False,
        use_external=False),
    stereo=True,
    filter_empty_gt=False,
    img_info_prototype='bevdet4d',
    multi_adj_frame_id_cfg=(1, 9, 1))
test_data_config = dict(
    pipeline=[
        dict(
            type='PrepareImageInputs',
            data_config=dict(
                cams=[
                    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',
                    'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'
                ],
                Ncams=6,
                input_size=(256, 704),
                src_size=(900, 1600),
                resize=(-0.06, 0.11),
                rot=(-5.4, 5.4),
                flip=True,
                crop_h=(0.0, 0.0),
                resize_test=0.0),
            sequential=True),
        dict(type='LoadAnnotations'),
        dict(
            type='BEVAug',
            bda_aug_conf=dict(
                rot_lim=(0, 0),
                scale_lim=(1.0, 1.0),
                flip_dx_ratio=0.5,
                flip_dy_ratio=0.5),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            is_train=False),
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ],
                    with_label=False),
                dict(type='Collect3D', keys=['points', 'img_inputs'])
            ])
    ],
    ann_file='data/nuscenes/bevdetv3-nuscenes_infos_val.pkl',
    type='NuScenesDatasetOccpancy',
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    modality=dict(
        use_lidar=False,
        use_camera=True,
        use_radar=False,
        use_map=False,
        use_external=False),
    stereo=True,
    filter_empty_gt=False,
    img_info_prototype='bevdet4d',
    multi_adj_frame_id_cfg=(1, 9, 1))
key = 'test'
optimizer = dict(type='AdamW', lr=0.0001, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=5, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=200,
    warmup_ratio=0.001,
    step=[100])
runner = dict(type='EpochBasedRunner', max_epochs=24)
custom_hooks = [
    dict(type='MEGVIIEMAHook', init_updates=10560, priority='NORMAL')
]
gpu_ids = range(0, 8)

2024-05-07 10:32:42,078 - mmdet - INFO - Set random seed to 0, deterministic: False
2024-05-07 10:32:44,082 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2024-05-07 10:32:44,206 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,207 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,207 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,208 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,209 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,209 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,210 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,211 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,212 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,213 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,214 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,215 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,217 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,220 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,223 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,226 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2024-05-07 10:32:44,242 - mmdet - INFO - initialize CustomFPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2024-05-07 10:32:44,252 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

forward_projection.img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

forward_projection.img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

forward_projection.img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_neck.lateral_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

forward_projection.img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_neck.lateral_convs.1.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

forward_projection.img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

forward_projection.img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.reduce_conv.0.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.reduce_conv.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.reduce_conv.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.reduce_conv.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_conv.weight - torch.Size([80, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.bn.weight - torch.Size([27]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.bn.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_mlp.fc1.weight - torch.Size([256, 27]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_mlp.fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_mlp.fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_se.conv_reduce.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_se.conv_reduce.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_se.conv_expand.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_se.conv_expand.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_mlp.fc1.weight - torch.Size([256, 27]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_mlp.fc1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_mlp.fc2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_mlp.fc2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_se.conv_reduce.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_se.conv_reduce.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_se.conv_expand.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.context_se.conv_expand.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.0.weight - torch.Size([80, 80, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.2.weight - torch.Size([80, 80, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.2.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.conv1.weight - torch.Size([256, 336, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.downsample.weight - torch.Size([256, 336, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.0.downsample.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp1.atrous_conv.weight - torch.Size([96, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp1.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp1.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp2.atrous_conv.weight - torch.Size([96, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp2.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp2.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp3.atrous_conv.weight - torch.Size([96, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp3.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp3.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp4.atrous_conv.weight - torch.Size([96, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp4.bn.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.aspp4.bn.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.global_avg_pool.1.weight - torch.Size([96, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.global_avg_pool.2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.global_avg_pool.2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.conv1.weight - torch.Size([256, 480, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.4.weight - torch.Size([80, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_view_transformer.depth_net.depth_conv.4.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.0.conv.weight - torch.Size([80, 80, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.1.conv.weight - torch.Size([80, 160, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.2.conv.weight - torch.Size([80, 320, 1, 1, 1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.0.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.1.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.2.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.conv.weight - torch.Size([80, 720, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.conv.weight - torch.Size([80, 720, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.conv.weight - torch.Size([160, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.conv.weight - torch.Size([160, 160, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.conv.weight - torch.Size([160, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.conv.weight - torch.Size([160, 160, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.conv.weight - torch.Size([160, 160, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.conv.weight - torch.Size([320, 160, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.conv.weight - torch.Size([320, 320, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.conv.weight - torch.Size([320, 160, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.conv.weight - torch.Size([320, 320, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.conv.weight - torch.Size([320, 320, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv1.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv2.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.conv2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.downsample.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.downsample.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

forward_projection.pre_process_net.layers.0.0.downsample.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.positional_encoding.row_embed.weight - torch.Size([100, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.positional_encoding.col_embed.weight - torch.Size([100, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.cams_embeds - torch.Size([6, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([128, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([320, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([80, 320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.norms.0.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.norms.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.norms.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.0.norms.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([128, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([320, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([80, 320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.norms.0.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.norms.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.norms.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.layers.1.norms.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.bev_conv.conv.weight - torch.Size([80, 80, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.bev_conv.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.predicter.0.weight - torch.Size([160, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.predicter.0.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.predicter.2.weight - torch.Size([144, 160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.transformer.encoder.occ_predictor.predicter.2.bias - torch.Size([144]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.0.bev_embedding.weight - torch.Size([10000, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.positional_encoding.row_embed.weight - torch.Size([50, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.positional_encoding.col_embed.weight - torch.Size([50, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.cams_embeds - torch.Size([6, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([320, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([80, 320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.norms.0.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.norms.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.norms.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.0.norms.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([320, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([80, 320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.norms.0.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.norms.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.norms.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.layers.1.norms.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.bev_conv.conv.weight - torch.Size([80, 80, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.bev_conv.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.predicter.0.weight - torch.Size([160, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.predicter.0.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.predicter.2.weight - torch.Size([72, 160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.transformer.encoder.occ_predictor.predicter.2.bias - torch.Size([72]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.1.bev_embedding.weight - torch.Size([2500, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.positional_encoding.row_embed.weight - torch.Size([25, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.positional_encoding.col_embed.weight - torch.Size([25, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.cams_embeds - torch.Size([6, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([64, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([32, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([16, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([80, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([320, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([80, 320]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.norms.0.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.norms.0.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.norms.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.layers.0.norms.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.bev_conv.conv.weight - torch.Size([80, 80, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.bev_conv.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.predicter.0.weight - torch.Size([160, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.predicter.0.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.predicter.2.weight - torch.Size([36, 160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.transformer.encoder.occ_predictor.predicter.2.bias - torch.Size([36]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

backward_projection_list.2.bev_embedding.weight - torch.Size([625, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv1.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv2.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.voxel_conv.conv2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.predicter.0.weight - torch.Size([160, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.predicter.0.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.predicter.2.weight - torch.Size([18, 160]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

occupancy_head.predicter.2.bias - torch.Size([18]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv1.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv1.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv1.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv2.conv.weight - torch.Size([80, 80, 3, 3, 3]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.voxel_conv.conv2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.density_predicter.0.weight - torch.Size([40, 80]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.density_predicter.0.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.density_predicter.2.weight - torch.Size([1, 40]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  

nerf_head.density_predicter.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of SparseMSFBOcc  
2024-05-07 10:32:44,277 - mmdet - INFO - Model:
SparseMSFBOcc(
  (focal_loss): CustomFocalLoss()
  (forward_projection): BEVDetStereoForwardProjection(
    (img_backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): ResLayer(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer2): ResLayer(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer3): ResLayer(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
      (layer4): ResLayer(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      )
    )
    init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
    (img_neck): CustomFPN(
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ConvModule(
          (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (fpn_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
    (img_view_transformer): LSSVStereoForwardPorjection(
      (depth_net): DepthNet(
        (reduce_conv): Sequential(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (context_conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
        (bn): BatchNorm1d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (depth_mlp): Mlp(
          (fc1): Linear(in_features=27, out_features=256, bias=True)
          (act): ReLU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (depth_se): SELayer(
          (conv_reduce): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU()
          (conv_expand): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (context_mlp): Mlp(
          (fc1): Linear(in_features=27, out_features=256, bias=True)
          (act): ReLU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (context_se): SELayer(
          (conv_reduce): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU()
          (conv_expand): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (cost_volumn_net): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (depth_conv): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(336, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (downsample): Conv2d(336, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): ASPP(
            (aspp1): _ASPPModule(
              (atrous_conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU()
            )
            (aspp2): _ASPPModule(
              (atrous_conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)
              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU()
            )
            (aspp3): _ASPPModule(
              (atrous_conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)
              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU()
            )
            (aspp4): _ASPPModule(
              (atrous_conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)
              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (relu): ReLU()
            )
            (global_avg_pool): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (3): ReLU()
            )
            (conv1): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (4): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (img_bev_encoder_neck): FPN(
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv3d(80, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): ConvModule(
          (conv): Conv3d(160, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): ConvModule(
          (conv): Conv3d(320, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (fpn_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): ConvModule(
          (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): ConvModule(
          (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
          (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
    (img_bev_encoder_backbone): CustomResNet3D(
      (layers): Sequential(
        (0): Sequential(
          (0): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(720, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (downsample): ConvModule(
              (conv): Conv3d(720, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
        )
        (1): Sequential(
          (0): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(80, 160, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(160, 160, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (downsample): ConvModule(
              (conv): Conv3d(80, 160, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
          (1): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(160, 160, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(160, 160, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
        )
        (2): Sequential(
          (0): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(160, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (downsample): ConvModule(
              (conv): Conv3d(160, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
          (1): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
    (pre_process_net): CustomResNet3D(
      (layers): Sequential(
        (0): Sequential(
          (0): BasicBlock3D(
            (conv1): ConvModule(
              (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (conv2): ConvModule(
              (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (downsample): ConvModule(
              (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
              (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (relu): ReLU(inplace=True)
          )
        )
      )
    )
  )
  (backward_projection_list): ModuleList(
    (0): SparseBEVFormerBackwardProjection(
      (positional_encoding): CustormLearnedPositionalEncoding(num_feats=40, row_num_embed=100, col_num_embed=100)
      (transformer): SparseBEVFormer(
        (encoder): SparseBEVFormerEncoder(
          (layers): ModuleList(
            (0): SparseBEVFormerEncoderLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                  (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                  (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
                (1): OA_SpatialCrossAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (deformable_attention): OA_MSDeformableAttention(
                    (sampling_offsets): Linear(in_features=80, out_features=128, bias=True)
                    (attention_weights): Linear(in_features=80, out_features=64, bias=True)
                    (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  )
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=80, out_features=320, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=320, out_features=80, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              )
            )
            (1): SparseBEVFormerEncoderLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                  (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                  (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
                (1): OA_SpatialCrossAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (deformable_attention): OA_MSDeformableAttention(
                    (sampling_offsets): Linear(in_features=80, out_features=128, bias=True)
                    (attention_weights): Linear(in_features=80, out_features=64, bias=True)
                    (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  )
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=80, out_features=320, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=320, out_features=80, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (occ_predictor): OccPredictor(
            (bev_conv): ConvModule(
              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (activate): ReLU(inplace=True)
            )
            (predicter): Sequential(
              (0): Linear(in_features=80, out_features=160, bias=True)
              (1): Softplus(beta=1, threshold=20)
              (2): Linear(in_features=160, out_features=144, bias=True)
            )
          )
        )
      )
      (bev_embedding): Embedding(10000, 80)
    )
    (1): SparseBEVFormerBackwardProjection(
      (positional_encoding): CustormLearnedPositionalEncoding(num_feats=40, row_num_embed=50, col_num_embed=50)
      (transformer): SparseBEVFormer(
        (encoder): SparseBEVFormerEncoder(
          (layers): ModuleList(
            (0): SparseBEVFormerEncoderLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                  (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                  (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
                (1): OA_SpatialCrossAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (deformable_attention): OA_MSDeformableAttention(
                    (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                    (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                    (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  )
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=80, out_features=320, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=320, out_features=80, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              )
            )
            (1): SparseBEVFormerEncoderLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                  (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                  (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
                (1): OA_SpatialCrossAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (deformable_attention): OA_MSDeformableAttention(
                    (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                    (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                    (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  )
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=80, out_features=320, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=320, out_features=80, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (occ_predictor): OccPredictor(
            (bev_conv): ConvModule(
              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (activate): ReLU(inplace=True)
            )
            (predicter): Sequential(
              (0): Linear(in_features=80, out_features=160, bias=True)
              (1): Softplus(beta=1, threshold=20)
              (2): Linear(in_features=160, out_features=72, bias=True)
            )
          )
        )
      )
      (bev_embedding): Embedding(2500, 80)
    )
    (2): SparseBEVFormerBackwardProjection(
      (positional_encoding): CustormLearnedPositionalEncoding(num_feats=40, row_num_embed=25, col_num_embed=25)
      (transformer): SparseBEVFormer(
        (encoder): SparseBEVFormerEncoder(
          (layers): ModuleList(
            (0): SparseBEVFormerEncoderLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=80, out_features=64, bias=True)
                  (attention_weights): Linear(in_features=80, out_features=32, bias=True)
                  (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
                (1): OA_SpatialCrossAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (deformable_attention): OA_MSDeformableAttention(
                    (sampling_offsets): Linear(in_features=80, out_features=32, bias=True)
                    (attention_weights): Linear(in_features=80, out_features=16, bias=True)
                    (value_proj): Linear(in_features=80, out_features=80, bias=True)
                  )
                  (output_proj): Linear(in_features=80, out_features=80, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=80, out_features=320, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=320, out_features=80, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (occ_predictor): OccPredictor(
            (bev_conv): ConvModule(
              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (activate): ReLU(inplace=True)
            )
            (predicter): Sequential(
              (0): Linear(in_features=80, out_features=160, bias=True)
              (1): Softplus(beta=1, threshold=20)
              (2): Linear(in_features=160, out_features=36, bias=True)
            )
          )
        )
      )
      (bev_embedding): Embedding(625, 80)
    )
  )
  (occupancy_head): OccupancyHead(
    (voxel_conv): BasicBlock3D(
      (conv1): ConvModule(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (conv2): ConvModule(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (predicter): Sequential(
      (0): Linear(in_features=80, out_features=160, bias=True)
      (1): Softplus(beta=1, threshold=20)
      (2): Linear(in_features=160, out_features=18, bias=True)
    )
    (focal_loss): CustomFocalLoss()
  )
  (nerf_head): NeRFHead(
    (voxel_conv): BasicBlock3D(
      (conv1): ConvModule(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (conv2): ConvModule(
        (conv): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        (bn): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (density_predicter): Sequential(
      (0): Linear(in_features=80, out_features=40, bias=True)
      (1): Softplus(beta=1, threshold=20)
      (2): Linear(in_features=40, out_features=1, bias=True)
    )
    (density): Sigmoid()
    (render_dep_loss): SiLogLoss()
  )
)
2024-05-07 10:32:47,715 - mmdet - INFO - load checkpoint from local path: ckpts/forward_projection-r50-4d-stereo-pretrained.pth
2024-05-07 10:32:47,872 - mmdet - WARNING - The model and loaded state dict do not match exactly

size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.0.weight: copying a param with shape torch.Size([59, 59, 3, 3]) from checkpoint, the shape in current model is torch.Size([80, 80, 3, 3]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.0.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.weight: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.running_mean: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.1.running_var: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.2.weight: copying a param with shape torch.Size([59, 59, 3, 3]) from checkpoint, the shape in current model is torch.Size([80, 80, 3, 3]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.2.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.weight: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.running_mean: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.cost_volumn_net.3.running_var: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
size mismatch for forward_projection.img_view_transformer.depth_net.depth_conv.0.conv1.weight: copying a param with shape torch.Size([256, 315, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 336, 3, 3]).
size mismatch for forward_projection.img_view_transformer.depth_net.depth_conv.0.downsample.weight: copying a param with shape torch.Size([256, 315, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 336, 1, 1]).
size mismatch for forward_projection.img_view_transformer.depth_net.depth_conv.4.weight: copying a param with shape torch.Size([59, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([80, 256, 1, 1]).
size mismatch for forward_projection.img_view_transformer.depth_net.depth_conv.4.bias: copying a param with shape torch.Size([59]) from checkpoint, the shape in current model is torch.Size([80]).
unexpected key in source state_dict: forward_projection.pts_bbox_head.shared_conv.conv.weight, forward_projection.pts_bbox_head.shared_conv.bn.weight, forward_projection.pts_bbox_head.shared_conv.bn.bias, forward_projection.pts_bbox_head.shared_conv.bn.running_mean, forward_projection.pts_bbox_head.shared_conv.bn.running_var, forward_projection.pts_bbox_head.shared_conv.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.reg.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.reg.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.reg.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.reg.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.reg.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.reg.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.reg.1.weight, forward_projection.pts_bbox_head.task_heads.0.reg.1.bias, forward_projection.pts_bbox_head.task_heads.0.height.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.height.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.height.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.height.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.height.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.height.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.height.1.weight, forward_projection.pts_bbox_head.task_heads.0.height.1.bias, forward_projection.pts_bbox_head.task_heads.0.dim.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.dim.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.dim.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.dim.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.dim.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.dim.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.dim.1.weight, forward_projection.pts_bbox_head.task_heads.0.dim.1.bias, forward_projection.pts_bbox_head.task_heads.0.rot.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.rot.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.rot.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.rot.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.rot.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.rot.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.rot.1.weight, forward_projection.pts_bbox_head.task_heads.0.rot.1.bias, forward_projection.pts_bbox_head.task_heads.0.vel.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.vel.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.vel.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.vel.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.vel.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.vel.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.vel.1.weight, forward_projection.pts_bbox_head.task_heads.0.vel.1.bias, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.conv.weight, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.bn.weight, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.bn.bias, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.bn.running_mean, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.bn.running_var, forward_projection.pts_bbox_head.task_heads.0.heatmap.0.bn.num_batches_tracked, forward_projection.pts_bbox_head.task_heads.0.heatmap.1.weight, forward_projection.pts_bbox_head.task_heads.0.heatmap.1.bias, forward_projection.img_bev_encoder_neck.conv.0.weight, forward_projection.img_bev_encoder_neck.conv.1.weight, forward_projection.img_bev_encoder_neck.conv.1.bias, forward_projection.img_bev_encoder_neck.conv.1.running_mean, forward_projection.img_bev_encoder_neck.conv.1.running_var, forward_projection.img_bev_encoder_neck.conv.1.num_batches_tracked, forward_projection.img_bev_encoder_neck.conv.3.weight, forward_projection.img_bev_encoder_neck.conv.4.weight, forward_projection.img_bev_encoder_neck.conv.4.bias, forward_projection.img_bev_encoder_neck.conv.4.running_mean, forward_projection.img_bev_encoder_neck.conv.4.running_var, forward_projection.img_bev_encoder_neck.conv.4.num_batches_tracked, forward_projection.img_bev_encoder_neck.up2.1.weight, forward_projection.img_bev_encoder_neck.up2.2.weight, forward_projection.img_bev_encoder_neck.up2.2.bias, forward_projection.img_bev_encoder_neck.up2.2.running_mean, forward_projection.img_bev_encoder_neck.up2.2.running_var, forward_projection.img_bev_encoder_neck.up2.2.num_batches_tracked, forward_projection.img_bev_encoder_neck.up2.4.weight, forward_projection.img_bev_encoder_neck.up2.4.bias, forward_projection.img_bev_encoder_backbone.layers.0.1.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.0.1.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.0.1.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.0.1.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.1.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.0.1.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.0.1.conv2.weight, forward_projection.img_bev_encoder_backbone.layers.0.1.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.0.1.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.0.1.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.1.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.0.1.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.0.0.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.0.0.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.0.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.0.0.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.0.0.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.0.0.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.0.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.0.0.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.0.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.1.0.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.1.0.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.0.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.1.0.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bias, forward_projection.img_bev_encoder_backbone.layers.1.1.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.1.1.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.1.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.1.1.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.1.1.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.1.1.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.1.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.1.1.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.2.0.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.0.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.2.0.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.2.0.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.2.0.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.0.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.2.0.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bias, forward_projection.img_bev_encoder_backbone.layers.2.1.bn1.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.bn1.bias, forward_projection.img_bev_encoder_backbone.layers.2.1.bn1.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.1.bn1.running_var, forward_projection.img_bev_encoder_backbone.layers.2.1.bn1.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.2.1.bn2.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.bn2.bias, forward_projection.img_bev_encoder_backbone.layers.2.1.bn2.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.1.bn2.running_var, forward_projection.img_bev_encoder_backbone.layers.2.1.bn2.num_batches_tracked, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.weight, forward_projection.pre_process_net.layers.0.1.conv1.weight, forward_projection.pre_process_net.layers.0.1.bn1.weight, forward_projection.pre_process_net.layers.0.1.bn1.bias, forward_projection.pre_process_net.layers.0.1.bn1.running_mean, forward_projection.pre_process_net.layers.0.1.bn1.running_var, forward_projection.pre_process_net.layers.0.1.bn1.num_batches_tracked, forward_projection.pre_process_net.layers.0.1.conv2.weight, forward_projection.pre_process_net.layers.0.1.bn2.weight, forward_projection.pre_process_net.layers.0.1.bn2.bias, forward_projection.pre_process_net.layers.0.1.bn2.running_mean, forward_projection.pre_process_net.layers.0.1.bn2.running_var, forward_projection.pre_process_net.layers.0.1.bn2.num_batches_tracked, forward_projection.pre_process_net.layers.0.0.bn1.weight, forward_projection.pre_process_net.layers.0.0.bn1.bias, forward_projection.pre_process_net.layers.0.0.bn1.running_mean, forward_projection.pre_process_net.layers.0.0.bn1.running_var, forward_projection.pre_process_net.layers.0.0.bn1.num_batches_tracked, forward_projection.pre_process_net.layers.0.0.bn2.weight, forward_projection.pre_process_net.layers.0.0.bn2.bias, forward_projection.pre_process_net.layers.0.0.bn2.running_mean, forward_projection.pre_process_net.layers.0.0.bn2.running_var, forward_projection.pre_process_net.layers.0.0.bn2.num_batches_tracked, forward_projection.pre_process_net.layers.0.0.conv1.weight, forward_projection.pre_process_net.layers.0.0.conv2.weight, forward_projection.pre_process_net.layers.0.0.downsample.weight, forward_projection.pre_process_net.layers.0.0.downsample.bias

missing keys in source state_dict: forward_projection.img_bev_encoder_neck.lateral_convs.0.conv.weight, forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.weight, forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.bias, forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.running_mean, forward_projection.img_bev_encoder_neck.lateral_convs.0.bn.running_var, forward_projection.img_bev_encoder_neck.lateral_convs.1.conv.weight, forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.weight, forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.bias, forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.running_mean, forward_projection.img_bev_encoder_neck.lateral_convs.1.bn.running_var, forward_projection.img_bev_encoder_neck.lateral_convs.2.conv.weight, forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.weight, forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.bias, forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.running_mean, forward_projection.img_bev_encoder_neck.lateral_convs.2.bn.running_var, forward_projection.img_bev_encoder_neck.fpn_convs.0.conv.weight, forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.weight, forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.bias, forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.running_mean, forward_projection.img_bev_encoder_neck.fpn_convs.0.bn.running_var, forward_projection.img_bev_encoder_neck.fpn_convs.1.conv.weight, forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.weight, forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.bias, forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.running_mean, forward_projection.img_bev_encoder_neck.fpn_convs.1.bn.running_var, forward_projection.img_bev_encoder_neck.fpn_convs.2.conv.weight, forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.weight, forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.bias, forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.running_mean, forward_projection.img_bev_encoder_neck.fpn_convs.2.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.conv.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.bias, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.0.conv1.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.conv.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.bias, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.0.conv2.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.conv.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.weight, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.bias, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.0.0.downsample.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.conv.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.0.conv1.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.conv.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.0.conv2.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.conv.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.weight, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.bias, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.0.downsample.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.conv.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.bias, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.1.conv1.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.conv.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.weight, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.bias, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.1.1.conv2.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.conv.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.bias, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.0.conv1.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.conv.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.bias, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.0.conv2.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.conv.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.weight, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.bias, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.0.downsample.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.conv.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.bias, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.1.conv1.bn.running_var, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.conv.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.weight, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.bias, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.running_mean, forward_projection.img_bev_encoder_backbone.layers.2.1.conv2.bn.running_var, forward_projection.pre_process_net.layers.0.0.conv1.conv.weight, forward_projection.pre_process_net.layers.0.0.conv1.bn.weight, forward_projection.pre_process_net.layers.0.0.conv1.bn.bias, forward_projection.pre_process_net.layers.0.0.conv1.bn.running_mean, forward_projection.pre_process_net.layers.0.0.conv1.bn.running_var, forward_projection.pre_process_net.layers.0.0.conv2.conv.weight, forward_projection.pre_process_net.layers.0.0.conv2.bn.weight, forward_projection.pre_process_net.layers.0.0.conv2.bn.bias, forward_projection.pre_process_net.layers.0.0.conv2.bn.running_mean, forward_projection.pre_process_net.layers.0.0.conv2.bn.running_var, forward_projection.pre_process_net.layers.0.0.downsample.conv.weight, forward_projection.pre_process_net.layers.0.0.downsample.bn.weight, forward_projection.pre_process_net.layers.0.0.downsample.bn.bias, forward_projection.pre_process_net.layers.0.0.downsample.bn.running_mean, forward_projection.pre_process_net.layers.0.0.downsample.bn.running_var, backward_projection_list.0.positional_encoding.row_embed.weight, backward_projection_list.0.positional_encoding.col_embed.weight, backward_projection_list.0.transformer.cams_embeds, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.attention_weights.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.attention_weights.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.value_proj.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.value_proj.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.output_proj.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.0.output_proj.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.output_proj.weight, backward_projection_list.0.transformer.encoder.layers.0.attentions.1.output_proj.bias, backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.1.weight, backward_projection_list.0.transformer.encoder.layers.0.ffns.0.layers.1.bias, backward_projection_list.0.transformer.encoder.layers.0.norms.0.weight, backward_projection_list.0.transformer.encoder.layers.0.norms.0.bias, backward_projection_list.0.transformer.encoder.layers.0.norms.1.weight, backward_projection_list.0.transformer.encoder.layers.0.norms.1.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.attention_weights.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.attention_weights.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.value_proj.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.value_proj.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.output_proj.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.0.output_proj.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.output_proj.weight, backward_projection_list.0.transformer.encoder.layers.1.attentions.1.output_proj.bias, backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.1.weight, backward_projection_list.0.transformer.encoder.layers.1.ffns.0.layers.1.bias, backward_projection_list.0.transformer.encoder.layers.1.norms.0.weight, backward_projection_list.0.transformer.encoder.layers.1.norms.0.bias, backward_projection_list.0.transformer.encoder.layers.1.norms.1.weight, backward_projection_list.0.transformer.encoder.layers.1.norms.1.bias, backward_projection_list.0.transformer.encoder.occ_predictor.bev_conv.conv.weight, backward_projection_list.0.transformer.encoder.occ_predictor.bev_conv.conv.bias, backward_projection_list.0.transformer.encoder.occ_predictor.predicter.0.weight, backward_projection_list.0.transformer.encoder.occ_predictor.predicter.0.bias, backward_projection_list.0.transformer.encoder.occ_predictor.predicter.2.weight, backward_projection_list.0.transformer.encoder.occ_predictor.predicter.2.bias, backward_projection_list.0.bev_embedding.weight, backward_projection_list.1.positional_encoding.row_embed.weight, backward_projection_list.1.positional_encoding.col_embed.weight, backward_projection_list.1.transformer.cams_embeds, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.attention_weights.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.attention_weights.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.value_proj.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.value_proj.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.output_proj.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.0.output_proj.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.output_proj.weight, backward_projection_list.1.transformer.encoder.layers.0.attentions.1.output_proj.bias, backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.1.weight, backward_projection_list.1.transformer.encoder.layers.0.ffns.0.layers.1.bias, backward_projection_list.1.transformer.encoder.layers.0.norms.0.weight, backward_projection_list.1.transformer.encoder.layers.0.norms.0.bias, backward_projection_list.1.transformer.encoder.layers.0.norms.1.weight, backward_projection_list.1.transformer.encoder.layers.0.norms.1.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.attention_weights.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.attention_weights.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.value_proj.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.value_proj.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.output_proj.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.0.output_proj.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.output_proj.weight, backward_projection_list.1.transformer.encoder.layers.1.attentions.1.output_proj.bias, backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.1.weight, backward_projection_list.1.transformer.encoder.layers.1.ffns.0.layers.1.bias, backward_projection_list.1.transformer.encoder.layers.1.norms.0.weight, backward_projection_list.1.transformer.encoder.layers.1.norms.0.bias, backward_projection_list.1.transformer.encoder.layers.1.norms.1.weight, backward_projection_list.1.transformer.encoder.layers.1.norms.1.bias, backward_projection_list.1.transformer.encoder.occ_predictor.bev_conv.conv.weight, backward_projection_list.1.transformer.encoder.occ_predictor.bev_conv.conv.bias, backward_projection_list.1.transformer.encoder.occ_predictor.predicter.0.weight, backward_projection_list.1.transformer.encoder.occ_predictor.predicter.0.bias, backward_projection_list.1.transformer.encoder.occ_predictor.predicter.2.weight, backward_projection_list.1.transformer.encoder.occ_predictor.predicter.2.bias, backward_projection_list.1.bev_embedding.weight, backward_projection_list.2.positional_encoding.row_embed.weight, backward_projection_list.2.positional_encoding.col_embed.weight, backward_projection_list.2.transformer.cams_embeds, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.attention_weights.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.attention_weights.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.value_proj.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.value_proj.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.output_proj.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.0.output_proj.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.output_proj.weight, backward_projection_list.2.transformer.encoder.layers.0.attentions.1.output_proj.bias, backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.1.weight, backward_projection_list.2.transformer.encoder.layers.0.ffns.0.layers.1.bias, backward_projection_list.2.transformer.encoder.layers.0.norms.0.weight, backward_projection_list.2.transformer.encoder.layers.0.norms.0.bias, backward_projection_list.2.transformer.encoder.layers.0.norms.1.weight, backward_projection_list.2.transformer.encoder.layers.0.norms.1.bias, backward_projection_list.2.transformer.encoder.occ_predictor.bev_conv.conv.weight, backward_projection_list.2.transformer.encoder.occ_predictor.bev_conv.conv.bias, backward_projection_list.2.transformer.encoder.occ_predictor.predicter.0.weight, backward_projection_list.2.transformer.encoder.occ_predictor.predicter.0.bias, backward_projection_list.2.transformer.encoder.occ_predictor.predicter.2.weight, backward_projection_list.2.transformer.encoder.occ_predictor.predicter.2.bias, backward_projection_list.2.bev_embedding.weight, occupancy_head.voxel_conv.conv1.conv.weight, occupancy_head.voxel_conv.conv1.bn.weight, occupancy_head.voxel_conv.conv1.bn.bias, occupancy_head.voxel_conv.conv1.bn.running_mean, occupancy_head.voxel_conv.conv1.bn.running_var, occupancy_head.voxel_conv.conv2.conv.weight, occupancy_head.voxel_conv.conv2.bn.weight, occupancy_head.voxel_conv.conv2.bn.bias, occupancy_head.voxel_conv.conv2.bn.running_mean, occupancy_head.voxel_conv.conv2.bn.running_var, occupancy_head.predicter.0.weight, occupancy_head.predicter.0.bias, occupancy_head.predicter.2.weight, occupancy_head.predicter.2.bias, nerf_head.voxel_conv.conv1.conv.weight, nerf_head.voxel_conv.conv1.bn.weight, nerf_head.voxel_conv.conv1.bn.bias, nerf_head.voxel_conv.conv1.bn.running_mean, nerf_head.voxel_conv.conv1.bn.running_var, nerf_head.voxel_conv.conv2.conv.weight, nerf_head.voxel_conv.conv2.bn.weight, nerf_head.voxel_conv.conv2.bn.bias, nerf_head.voxel_conv.conv2.bn.running_mean, nerf_head.voxel_conv.conv2.bn.running_var, nerf_head.density_predicter.0.weight, nerf_head.density_predicter.0.bias, nerf_head.density_predicter.2.weight, nerf_head.density_predicter.2.bias

2024-05-07 10:32:47,882 - mmdet - INFO - Start running, host: lzm@a8, work_dir: /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d
2024-05-07 10:32:47,883 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) MEGVIIEMAHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2024-05-07 10:32:47,883 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs
2024-05-07 10:32:47,883 - mmdet - INFO - Checkpoints will be saved to /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d by HardDiskBackend.
2024-05-07 10:37:12,032 - mmdet - INFO - Epoch [1][50/440]	lr: 2.458e-05, eta: 15:25:01, time: 5.281, data_time: 0.412, memory: 17157, loss_depth: 0.2795, render_dep_loss_c_1_2: 0.6449, loss_voxel_ce_c_1_2: 14.4973, loss_voxel_sem_scal_c_1_2: 4.8185, loss_voxel_geo_scal_c_1_2: 2.5100, loss_voxel_lovasz_c_1_2: 0.4719, loss_voxel_ce_c_1_4: 6.9881, loss_voxel_sem_scal_c_1_4: 2.2434, loss_voxel_geo_scal_c_1_4: 1.0933, loss_voxel_lovasz_c_1_4: 0.2358, loss_voxel_ce_c_1_8: 4.0391, loss_voxel_sem_scal_c_1_8: 0.9928, loss_voxel_geo_scal_c_1_8: 0.4292, loss_voxel_lovasz_c_1_8: 0.1169, loss_voxel_ce_c_0: 27.3758, loss_voxel_sem_scal_c_0: 10.4945, loss_voxel_geo_scal_c_0: 5.1975, loss_voxel_lovasz_c_0: 0.9400, loss: 83.3684, grad_norm: 142.4112
2024-05-07 10:41:21,073 - mmdet - INFO - Epoch [1][100/440]	lr: 4.955e-05, eta: 14:54:28, time: 4.981, data_time: 0.223, memory: 17157, loss_depth: 0.2719, render_dep_loss_c_1_2: 0.5488, loss_voxel_ce_c_1_2: 9.0365, loss_voxel_sem_scal_c_1_2: 4.7943, loss_voxel_geo_scal_c_1_2: 2.1816, loss_voxel_lovasz_c_1_2: 0.4703, loss_voxel_ce_c_1_4: 4.3740, loss_voxel_sem_scal_c_1_4: 2.2290, loss_voxel_geo_scal_c_1_4: 0.9514, loss_voxel_lovasz_c_1_4: 0.2350, loss_voxel_ce_c_1_8: 2.7638, loss_voxel_sem_scal_c_1_8: 0.9857, loss_voxel_geo_scal_c_1_8: 0.3882, loss_voxel_lovasz_c_1_8: 0.1166, loss_voxel_ce_c_0: 11.0765, loss_voxel_sem_scal_c_0: 10.1942, loss_voxel_geo_scal_c_0: 4.0253, loss_voxel_lovasz_c_0: 0.9245, loss: 55.5677, grad_norm: 91.4741
2024-05-07 10:45:28,819 - mmdet - INFO - Epoch [1][150/440]	lr: 7.453e-05, eta: 14:40:01, time: 4.955, data_time: 0.218, memory: 17157, loss_depth: 0.2563, render_dep_loss_c_1_2: 0.4801, loss_voxel_ce_c_1_2: 1.9471, loss_voxel_sem_scal_c_1_2: 4.4532, loss_voxel_geo_scal_c_1_2: 1.3855, loss_voxel_lovasz_c_1_2: 0.4544, loss_voxel_ce_c_1_4: 1.0113, loss_voxel_sem_scal_c_1_4: 2.0982, loss_voxel_geo_scal_c_1_4: 0.5769, loss_voxel_lovasz_c_1_4: 0.2277, loss_voxel_ce_c_1_8: 0.7979, loss_voxel_sem_scal_c_1_8: 0.9473, loss_voxel_geo_scal_c_1_8: 0.2752, loss_voxel_lovasz_c_1_8: 0.1138, loss_voxel_ce_c_0: 2.2435, loss_voxel_sem_scal_c_0: 8.9982, loss_voxel_geo_scal_c_0: 3.2230, loss_voxel_lovasz_c_0: 0.8818, loss: 30.3715, grad_norm: 26.6559
2024-05-07 10:49:36,559 - mmdet - INFO - Epoch [1][200/440]	lr: 9.950e-05, eta: 14:30:43, time: 4.955, data_time: 0.217, memory: 17157, loss_depth: 0.2359, render_dep_loss_c_1_2: 0.4609, loss_voxel_ce_c_1_2: 0.6065, loss_voxel_sem_scal_c_1_2: 3.0666, loss_voxel_geo_scal_c_1_2: 1.0689, loss_voxel_lovasz_c_1_2: 0.4106, loss_voxel_ce_c_1_4: 0.3679, loss_voxel_sem_scal_c_1_4: 1.6038, loss_voxel_geo_scal_c_1_4: 0.4228, loss_voxel_lovasz_c_1_4: 0.2067, loss_voxel_ce_c_1_8: 0.2302, loss_voxel_sem_scal_c_1_8: 0.7868, loss_voxel_geo_scal_c_1_8: 0.1716, loss_voxel_lovasz_c_1_8: 0.1054, loss_voxel_ce_c_0: 0.9078, loss_voxel_sem_scal_c_0: 5.9552, loss_voxel_geo_scal_c_0: 2.5941, loss_voxel_lovasz_c_0: 0.7999, loss: 20.0016, grad_norm: 9.3116
2024-05-07 10:53:44,183 - mmdet - INFO - Epoch [1][250/440]	lr: 1.000e-04, eta: 14:23:25, time: 4.952, data_time: 0.221, memory: 17157, loss_depth: 0.2226, render_dep_loss_c_1_2: 0.4475, loss_voxel_ce_c_1_2: 0.5633, loss_voxel_sem_scal_c_1_2: 2.2472, loss_voxel_geo_scal_c_1_2: 0.9332, loss_voxel_lovasz_c_1_2: 0.3919, loss_voxel_ce_c_1_4: 0.3620, loss_voxel_sem_scal_c_1_4: 1.1463, loss_voxel_geo_scal_c_1_4: 0.3450, loss_voxel_lovasz_c_1_4: 0.1889, loss_voxel_ce_c_1_8: 0.2060, loss_voxel_sem_scal_c_1_8: 0.5882, loss_voxel_geo_scal_c_1_8: 0.1470, loss_voxel_lovasz_c_1_8: 0.0936, loss_voxel_ce_c_0: 0.8299, loss_voxel_sem_scal_c_0: 4.6604, loss_voxel_geo_scal_c_0: 2.3062, loss_voxel_lovasz_c_0: 0.7840, loss: 16.4632, grad_norm: 9.4007
2024-05-07 10:57:52,043 - mmdet - INFO - Epoch [1][300/440]	lr: 1.000e-04, eta: 14:17:04, time: 4.949, data_time: 0.213, memory: 17157, loss_depth: 0.2149, render_dep_loss_c_1_2: 0.4322, loss_voxel_ce_c_1_2: 0.5367, loss_voxel_sem_scal_c_1_2: 1.9373, loss_voxel_geo_scal_c_1_2: 0.8643, loss_voxel_lovasz_c_1_2: 0.3812, loss_voxel_ce_c_1_4: 0.3437, loss_voxel_sem_scal_c_1_4: 0.9524, loss_voxel_geo_scal_c_1_4: 0.3189, loss_voxel_lovasz_c_1_4: 0.1814, loss_voxel_ce_c_1_8: 0.1986, loss_voxel_sem_scal_c_1_8: 0.4816, loss_voxel_geo_scal_c_1_8: 0.1312, loss_voxel_lovasz_c_1_8: 0.0874, loss_voxel_ce_c_0: 0.7637, loss_voxel_sem_scal_c_0: 4.1107, loss_voxel_geo_scal_c_0: 2.1996, loss_voxel_lovasz_c_0: 0.7774, loss: 14.9132, grad_norm: 9.2137
2024-05-07 11:02:00,443 - mmdet - INFO - Epoch [1][350/440]	lr: 1.000e-04, eta: 14:12:01, time: 4.976, data_time: 0.215, memory: 17157, loss_depth: 0.2078, render_dep_loss_c_1_2: 0.4267, loss_voxel_ce_c_1_2: 0.5159, loss_voxel_sem_scal_c_1_2: 1.8157, loss_voxel_geo_scal_c_1_2: 0.8400, loss_voxel_lovasz_c_1_2: 0.3749, loss_voxel_ce_c_1_4: 0.3273, loss_voxel_sem_scal_c_1_4: 0.8721, loss_voxel_geo_scal_c_1_4: 0.3121, loss_voxel_lovasz_c_1_4: 0.1781, loss_voxel_ce_c_1_8: 0.1908, loss_voxel_sem_scal_c_1_8: 0.4291, loss_voxel_geo_scal_c_1_8: 0.1269, loss_voxel_lovasz_c_1_8: 0.0843, loss_voxel_ce_c_0: 0.7130, loss_voxel_sem_scal_c_0: 3.8834, loss_voxel_geo_scal_c_0: 2.1486, loss_voxel_lovasz_c_0: 0.7734, loss: 14.2199, grad_norm: 9.2458
2024-05-07 11:06:09,194 - mmdet - INFO - Epoch [1][400/440]	lr: 1.000e-04, eta: 14:07:10, time: 4.975, data_time: 0.218, memory: 17157, loss_depth: 0.2034, render_dep_loss_c_1_2: 0.4216, loss_voxel_ce_c_1_2: 0.5060, loss_voxel_sem_scal_c_1_2: 1.7321, loss_voxel_geo_scal_c_1_2: 0.8150, loss_voxel_lovasz_c_1_2: 0.3701, loss_voxel_ce_c_1_4: 0.3199, loss_voxel_sem_scal_c_1_4: 0.8255, loss_voxel_geo_scal_c_1_4: 0.3028, loss_voxel_lovasz_c_1_4: 0.1759, loss_voxel_ce_c_1_8: 0.1883, loss_voxel_sem_scal_c_1_8: 0.3888, loss_voxel_geo_scal_c_1_8: 0.1203, loss_voxel_lovasz_c_1_8: 0.0819, loss_voxel_ce_c_0: 0.6817, loss_voxel_sem_scal_c_0: 3.7360, loss_voxel_geo_scal_c_0: 2.1000, loss_voxel_lovasz_c_0: 0.7698, loss: 13.7390, grad_norm: 9.5723
2024-05-07 11:09:27,610 - mmdet - INFO - Saving checkpoint at 1 epochs
2024-05-07 11:09:28,926 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_1_ema.pth
2024-05-07 11:13:45,063 - mmdet - INFO - Epoch [2][50/440]	lr: 1.000e-04, eta: 12:53:08, time: 5.121, data_time: 0.397, memory: 17157, loss_depth: 0.1964, render_dep_loss_c_1_2: 0.4126, loss_voxel_ce_c_1_2: 0.4888, loss_voxel_sem_scal_c_1_2: 1.5822, loss_voxel_geo_scal_c_1_2: 0.7879, loss_voxel_lovasz_c_1_2: 0.3620, loss_voxel_ce_c_1_4: 0.3087, loss_voxel_sem_scal_c_1_4: 0.7378, loss_voxel_geo_scal_c_1_4: 0.2938, loss_voxel_lovasz_c_1_4: 0.1713, loss_voxel_ce_c_1_8: 0.1820, loss_voxel_sem_scal_c_1_8: 0.3486, loss_voxel_geo_scal_c_1_8: 0.1154, loss_voxel_lovasz_c_1_8: 0.0792, loss_voxel_ce_c_0: 0.6378, loss_voxel_sem_scal_c_0: 3.4496, loss_voxel_geo_scal_c_0: 2.0423, loss_voxel_lovasz_c_0: 0.7617, loss: 12.9580, grad_norm: 9.2682
2024-05-07 11:17:50,246 - mmdet - INFO - Epoch [2][100/440]	lr: 1.000e-04, eta: 12:53:54, time: 4.904, data_time: 0.155, memory: 17157, loss_depth: 0.1939, render_dep_loss_c_1_2: 0.4101, loss_voxel_ce_c_1_2: 0.4819, loss_voxel_sem_scal_c_1_2: 1.5393, loss_voxel_geo_scal_c_1_2: 0.7741, loss_voxel_lovasz_c_1_2: 0.3592, loss_voxel_ce_c_1_4: 0.3065, loss_voxel_sem_scal_c_1_4: 0.7232, loss_voxel_geo_scal_c_1_4: 0.2883, loss_voxel_lovasz_c_1_4: 0.1702, loss_voxel_ce_c_1_8: 0.1806, loss_voxel_sem_scal_c_1_8: 0.3449, loss_voxel_geo_scal_c_1_8: 0.1132, loss_voxel_lovasz_c_1_8: 0.0786, loss_voxel_ce_c_0: 0.6164, loss_voxel_sem_scal_c_0: 3.3859, loss_voxel_geo_scal_c_0: 2.0116, loss_voxel_lovasz_c_0: 0.7575, loss: 12.7352, grad_norm: 9.2491
2024-05-07 11:21:56,637 - mmdet - INFO - Epoch [2][150/440]	lr: 1.000e-04, eta: 12:54:10, time: 4.927, data_time: 0.155, memory: 17157, loss_depth: 0.1912, render_dep_loss_c_1_2: 0.4082, loss_voxel_ce_c_1_2: 0.4812, loss_voxel_sem_scal_c_1_2: 1.5165, loss_voxel_geo_scal_c_1_2: 0.7627, loss_voxel_lovasz_c_1_2: 0.3569, loss_voxel_ce_c_1_4: 0.3031, loss_voxel_sem_scal_c_1_4: 0.7091, loss_voxel_geo_scal_c_1_4: 0.2850, loss_voxel_lovasz_c_1_4: 0.1684, loss_voxel_ce_c_1_8: 0.1791, loss_voxel_sem_scal_c_1_8: 0.3315, loss_voxel_geo_scal_c_1_8: 0.1107, loss_voxel_lovasz_c_1_8: 0.0781, loss_voxel_ce_c_0: 0.6026, loss_voxel_sem_scal_c_0: 3.3354, loss_voxel_geo_scal_c_0: 1.9873, loss_voxel_lovasz_c_0: 0.7546, loss: 12.5617, grad_norm: 9.0887
2024-05-07 11:26:03,631 - mmdet - INFO - Epoch [2][200/440]	lr: 1.000e-04, eta: 12:53:55, time: 4.940, data_time: 0.156, memory: 17157, loss_depth: 0.1884, render_dep_loss_c_1_2: 0.4060, loss_voxel_ce_c_1_2: 0.4732, loss_voxel_sem_scal_c_1_2: 1.4914, loss_voxel_geo_scal_c_1_2: 0.7655, loss_voxel_lovasz_c_1_2: 0.3552, loss_voxel_ce_c_1_4: 0.2981, loss_voxel_sem_scal_c_1_4: 0.6924, loss_voxel_geo_scal_c_1_4: 0.2848, loss_voxel_lovasz_c_1_4: 0.1671, loss_voxel_ce_c_1_8: 0.1756, loss_voxel_sem_scal_c_1_8: 0.3181, loss_voxel_geo_scal_c_1_8: 0.1112, loss_voxel_lovasz_c_1_8: 0.0771, loss_voxel_ce_c_0: 0.5876, loss_voxel_sem_scal_c_0: 3.3020, loss_voxel_geo_scal_c_0: 1.9819, loss_voxel_lovasz_c_0: 0.7537, loss: 12.4295, grad_norm: 9.3099
2024-05-07 11:30:08,311 - mmdet - INFO - Epoch [2][250/440]	lr: 1.000e-04, eta: 12:52:32, time: 4.893, data_time: 0.162, memory: 17157, loss_depth: 0.1865, render_dep_loss_c_1_2: 0.4062, loss_voxel_ce_c_1_2: 0.4699, loss_voxel_sem_scal_c_1_2: 1.4541, loss_voxel_geo_scal_c_1_2: 0.7508, loss_voxel_lovasz_c_1_2: 0.3536, loss_voxel_ce_c_1_4: 0.2939, loss_voxel_sem_scal_c_1_4: 0.6637, loss_voxel_geo_scal_c_1_4: 0.2810, loss_voxel_lovasz_c_1_4: 0.1655, loss_voxel_ce_c_1_8: 0.1746, loss_voxel_sem_scal_c_1_8: 0.3149, loss_voxel_geo_scal_c_1_8: 0.1092, loss_voxel_lovasz_c_1_8: 0.0766, loss_voxel_ce_c_0: 0.5772, loss_voxel_sem_scal_c_0: 3.2187, loss_voxel_geo_scal_c_0: 1.9551, loss_voxel_lovasz_c_0: 0.7518, loss: 12.2033, grad_norm: 9.1028
2024-05-07 11:34:12,910 - mmdet - INFO - Epoch [2][300/440]	lr: 1.000e-04, eta: 12:50:48, time: 4.893, data_time: 0.165, memory: 17157, loss_depth: 0.1844, render_dep_loss_c_1_2: 0.4049, loss_voxel_ce_c_1_2: 0.4631, loss_voxel_sem_scal_c_1_2: 1.4257, loss_voxel_geo_scal_c_1_2: 0.7470, loss_voxel_lovasz_c_1_2: 0.3527, loss_voxel_ce_c_1_4: 0.2910, loss_voxel_sem_scal_c_1_4: 0.6529, loss_voxel_geo_scal_c_1_4: 0.2795, loss_voxel_lovasz_c_1_4: 0.1654, loss_voxel_ce_c_1_8: 0.1712, loss_voxel_sem_scal_c_1_8: 0.3064, loss_voxel_geo_scal_c_1_8: 0.1086, loss_voxel_lovasz_c_1_8: 0.0762, loss_voxel_ce_c_0: 0.5679, loss_voxel_sem_scal_c_0: 3.1963, loss_voxel_geo_scal_c_0: 1.9436, loss_voxel_lovasz_c_0: 0.7512, loss: 12.0877, grad_norm: 9.5202
2024-05-07 11:38:17,060 - mmdet - INFO - Epoch [2][350/440]	lr: 1.000e-04, eta: 12:48:40, time: 4.883, data_time: 0.162, memory: 17157, loss_depth: 0.1834, render_dep_loss_c_1_2: 0.4024, loss_voxel_ce_c_1_2: 0.4649, loss_voxel_sem_scal_c_1_2: 1.4305, loss_voxel_geo_scal_c_1_2: 0.7480, loss_voxel_lovasz_c_1_2: 0.3510, loss_voxel_ce_c_1_4: 0.2903, loss_voxel_sem_scal_c_1_4: 0.6610, loss_voxel_geo_scal_c_1_4: 0.2799, loss_voxel_lovasz_c_1_4: 0.1646, loss_voxel_ce_c_1_8: 0.1722, loss_voxel_sem_scal_c_1_8: 0.3068, loss_voxel_geo_scal_c_1_8: 0.1085, loss_voxel_lovasz_c_1_8: 0.0758, loss_voxel_ce_c_0: 0.5645, loss_voxel_sem_scal_c_0: 3.1947, loss_voxel_geo_scal_c_0: 1.9443, loss_voxel_lovasz_c_0: 0.7479, loss: 12.0907, grad_norm: 9.3899
2024-05-07 11:42:21,751 - mmdet - INFO - Epoch [2][400/440]	lr: 1.000e-04, eta: 12:46:24, time: 4.894, data_time: 0.160, memory: 17157, loss_depth: 0.1812, render_dep_loss_c_1_2: 0.3960, loss_voxel_ce_c_1_2: 0.4591, loss_voxel_sem_scal_c_1_2: 1.3657, loss_voxel_geo_scal_c_1_2: 0.7330, loss_voxel_lovasz_c_1_2: 0.3488, loss_voxel_ce_c_1_4: 0.2859, loss_voxel_sem_scal_c_1_4: 0.6288, loss_voxel_geo_scal_c_1_4: 0.2738, loss_voxel_lovasz_c_1_4: 0.1637, loss_voxel_ce_c_1_8: 0.1689, loss_voxel_sem_scal_c_1_8: 0.2938, loss_voxel_geo_scal_c_1_8: 0.1055, loss_voxel_lovasz_c_1_8: 0.0753, loss_voxel_ce_c_0: 0.5589, loss_voxel_sem_scal_c_0: 3.0699, loss_voxel_geo_scal_c_0: 1.9029, loss_voxel_lovasz_c_0: 0.7456, loss: 11.7566, grad_norm: 9.2564
2024-05-07 11:45:38,865 - mmdet - INFO - Saving checkpoint at 2 epochs
2024-05-07 11:45:40,147 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_2_ema.pth
2024-05-07 11:49:58,609 - mmdet - INFO - Epoch [3][50/440]	lr: 1.000e-04, eta: 12:10:25, time: 5.168, data_time: 0.399, memory: 17157, loss_depth: 0.1796, render_dep_loss_c_1_2: 0.3952, loss_voxel_ce_c_1_2: 0.4519, loss_voxel_sem_scal_c_1_2: 1.3470, loss_voxel_geo_scal_c_1_2: 0.7289, loss_voxel_lovasz_c_1_2: 0.3466, loss_voxel_ce_c_1_4: 0.2819, loss_voxel_sem_scal_c_1_4: 0.6223, loss_voxel_geo_scal_c_1_4: 0.2722, loss_voxel_lovasz_c_1_4: 0.1622, loss_voxel_ce_c_1_8: 0.1664, loss_voxel_sem_scal_c_1_8: 0.2870, loss_voxel_geo_scal_c_1_8: 0.1049, loss_voxel_lovasz_c_1_8: 0.0744, loss_voxel_ce_c_0: 0.5467, loss_voxel_sem_scal_c_0: 3.0404, loss_voxel_geo_scal_c_0: 1.8886, loss_voxel_lovasz_c_0: 0.7414, loss: 11.6373, grad_norm: 8.7472
2024-05-07 11:54:07,688 - mmdet - INFO - Epoch [3][100/440]	lr: 1.000e-04, eta: 12:10:08, time: 4.982, data_time: 0.223, memory: 17157, loss_depth: 0.1784, render_dep_loss_c_1_2: 0.3926, loss_voxel_ce_c_1_2: 0.4528, loss_voxel_sem_scal_c_1_2: 1.2956, loss_voxel_geo_scal_c_1_2: 0.7194, loss_voxel_lovasz_c_1_2: 0.3429, loss_voxel_ce_c_1_4: 0.2831, loss_voxel_sem_scal_c_1_4: 0.5997, loss_voxel_geo_scal_c_1_4: 0.2680, loss_voxel_lovasz_c_1_4: 0.1610, loss_voxel_ce_c_1_8: 0.1670, loss_voxel_sem_scal_c_1_8: 0.2773, loss_voxel_geo_scal_c_1_8: 0.1026, loss_voxel_lovasz_c_1_8: 0.0736, loss_voxel_ce_c_0: 0.5515, loss_voxel_sem_scal_c_0: 2.9136, loss_voxel_geo_scal_c_0: 1.8707, loss_voxel_lovasz_c_0: 0.7363, loss: 11.3861, grad_norm: 8.8652
2024-05-07 11:58:17,269 - mmdet - INFO - Epoch [3][150/440]	lr: 1.000e-04, eta: 12:09:33, time: 4.992, data_time: 0.226, memory: 17157, loss_depth: 0.1775, render_dep_loss_c_1_2: 0.3942, loss_voxel_ce_c_1_2: 0.4480, loss_voxel_sem_scal_c_1_2: 1.2892, loss_voxel_geo_scal_c_1_2: 0.7175, loss_voxel_lovasz_c_1_2: 0.3429, loss_voxel_ce_c_1_4: 0.2809, loss_voxel_sem_scal_c_1_4: 0.5998, loss_voxel_geo_scal_c_1_4: 0.2695, loss_voxel_lovasz_c_1_4: 0.1611, loss_voxel_ce_c_1_8: 0.1656, loss_voxel_sem_scal_c_1_8: 0.2738, loss_voxel_geo_scal_c_1_8: 0.1031, loss_voxel_lovasz_c_1_8: 0.0734, loss_voxel_ce_c_0: 0.5442, loss_voxel_sem_scal_c_0: 2.9162, loss_voxel_geo_scal_c_0: 1.8649, loss_voxel_lovasz_c_0: 0.7368, loss: 11.3589, grad_norm: 8.8719
2024-05-07 12:02:26,640 - mmdet - INFO - Epoch [3][200/440]	lr: 1.000e-04, eta: 12:08:36, time: 4.987, data_time: 0.215, memory: 17157, loss_depth: 0.1767, render_dep_loss_c_1_2: 0.3957, loss_voxel_ce_c_1_2: 0.4469, loss_voxel_sem_scal_c_1_2: 1.2970, loss_voxel_geo_scal_c_1_2: 0.7253, loss_voxel_lovasz_c_1_2: 0.3439, loss_voxel_ce_c_1_4: 0.2778, loss_voxel_sem_scal_c_1_4: 0.6004, loss_voxel_geo_scal_c_1_4: 0.2709, loss_voxel_lovasz_c_1_4: 0.1612, loss_voxel_ce_c_1_8: 0.1645, loss_voxel_sem_scal_c_1_8: 0.2736, loss_voxel_geo_scal_c_1_8: 0.1038, loss_voxel_lovasz_c_1_8: 0.0734, loss_voxel_ce_c_0: 0.5428, loss_voxel_sem_scal_c_0: 2.9447, loss_voxel_geo_scal_c_0: 1.8796, loss_voxel_lovasz_c_0: 0.7394, loss: 11.4176, grad_norm: 9.6051
2024-05-07 12:06:37,224 - mmdet - INFO - Epoch [3][250/440]	lr: 1.000e-04, eta: 12:07:32, time: 5.011, data_time: 0.213, memory: 17157, loss_depth: 0.1746, render_dep_loss_c_1_2: 0.3912, loss_voxel_ce_c_1_2: 0.4417, loss_voxel_sem_scal_c_1_2: 1.2906, loss_voxel_geo_scal_c_1_2: 0.7104, loss_voxel_lovasz_c_1_2: 0.3408, loss_voxel_ce_c_1_4: 0.2756, loss_voxel_sem_scal_c_1_4: 0.5853, loss_voxel_geo_scal_c_1_4: 0.2654, loss_voxel_lovasz_c_1_4: 0.1594, loss_voxel_ce_c_1_8: 0.1625, loss_voxel_sem_scal_c_1_8: 0.2675, loss_voxel_geo_scal_c_1_8: 0.1015, loss_voxel_lovasz_c_1_8: 0.0727, loss_voxel_ce_c_0: 0.5365, loss_voxel_sem_scal_c_0: 2.9183, loss_voxel_geo_scal_c_0: 1.8400, loss_voxel_lovasz_c_0: 0.7336, loss: 11.2676, grad_norm: 8.9299
2024-05-07 12:10:46,628 - mmdet - INFO - Epoch [3][300/440]	lr: 1.000e-04, eta: 12:06:04, time: 4.988, data_time: 0.218, memory: 17157, loss_depth: 0.1738, render_dep_loss_c_1_2: 0.3899, loss_voxel_ce_c_1_2: 0.4394, loss_voxel_sem_scal_c_1_2: 1.2770, loss_voxel_geo_scal_c_1_2: 0.7140, loss_voxel_lovasz_c_1_2: 0.3411, loss_voxel_ce_c_1_4: 0.2737, loss_voxel_sem_scal_c_1_4: 0.5848, loss_voxel_geo_scal_c_1_4: 0.2653, loss_voxel_lovasz_c_1_4: 0.1596, loss_voxel_ce_c_1_8: 0.1619, loss_voxel_sem_scal_c_1_8: 0.2693, loss_voxel_geo_scal_c_1_8: 0.1011, loss_voxel_lovasz_c_1_8: 0.0729, loss_voxel_ce_c_0: 0.5334, loss_voxel_sem_scal_c_0: 2.8990, loss_voxel_geo_scal_c_0: 1.8435, loss_voxel_lovasz_c_0: 0.7338, loss: 11.2336, grad_norm: 9.6194
2024-05-07 12:14:55,121 - mmdet - INFO - Epoch [3][350/440]	lr: 1.000e-04, eta: 12:04:15, time: 4.970, data_time: 0.225, memory: 17157, loss_depth: 0.1734, render_dep_loss_c_1_2: 0.3875, loss_voxel_ce_c_1_2: 0.4395, loss_voxel_sem_scal_c_1_2: 1.2345, loss_voxel_geo_scal_c_1_2: 0.7062, loss_voxel_lovasz_c_1_2: 0.3402, loss_voxel_ce_c_1_4: 0.2728, loss_voxel_sem_scal_c_1_4: 0.5618, loss_voxel_geo_scal_c_1_4: 0.2638, loss_voxel_lovasz_c_1_4: 0.1591, loss_voxel_ce_c_1_8: 0.1601, loss_voxel_sem_scal_c_1_8: 0.2623, loss_voxel_geo_scal_c_1_8: 0.1008, loss_voxel_lovasz_c_1_8: 0.0724, loss_voxel_ce_c_0: 0.5316, loss_voxel_sem_scal_c_0: 2.8187, loss_voxel_geo_scal_c_0: 1.8344, loss_voxel_lovasz_c_0: 0.7328, loss: 11.0517, grad_norm: 8.7746
2024-05-07 12:19:02,977 - mmdet - INFO - Epoch [3][400/440]	lr: 1.000e-04, eta: 12:02:11, time: 4.957, data_time: 0.219, memory: 17157, loss_depth: 0.1725, render_dep_loss_c_1_2: 0.3892, loss_voxel_ce_c_1_2: 0.4365, loss_voxel_sem_scal_c_1_2: 1.2081, loss_voxel_geo_scal_c_1_2: 0.7018, loss_voxel_lovasz_c_1_2: 0.3386, loss_voxel_ce_c_1_4: 0.2719, loss_voxel_sem_scal_c_1_4: 0.5645, loss_voxel_geo_scal_c_1_4: 0.2624, loss_voxel_lovasz_c_1_4: 0.1587, loss_voxel_ce_c_1_8: 0.1606, loss_voxel_sem_scal_c_1_8: 0.2610, loss_voxel_geo_scal_c_1_8: 0.1003, loss_voxel_lovasz_c_1_8: 0.0723, loss_voxel_ce_c_0: 0.5308, loss_voxel_sem_scal_c_0: 2.7627, loss_voxel_geo_scal_c_0: 1.8178, loss_voxel_lovasz_c_0: 0.7297, loss: 10.9393, grad_norm: 8.7957
2024-05-07 12:22:21,404 - mmdet - INFO - Saving checkpoint at 3 epochs
2024-05-07 12:22:22,685 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_3_ema.pth
2024-05-07 12:26:40,445 - mmdet - INFO - Epoch [4][50/440]	lr: 1.000e-04, eta: 11:37:00, time: 5.153, data_time: 0.324, memory: 17157, loss_depth: 0.1721, render_dep_loss_c_1_2: 0.3873, loss_voxel_ce_c_1_2: 0.4354, loss_voxel_sem_scal_c_1_2: 1.2261, loss_voxel_geo_scal_c_1_2: 0.7050, loss_voxel_lovasz_c_1_2: 0.3383, loss_voxel_ce_c_1_4: 0.2711, loss_voxel_sem_scal_c_1_4: 0.5584, loss_voxel_geo_scal_c_1_4: 0.2636, loss_voxel_lovasz_c_1_4: 0.1582, loss_voxel_ce_c_1_8: 0.1596, loss_voxel_sem_scal_c_1_8: 0.2525, loss_voxel_geo_scal_c_1_8: 0.1002, loss_voxel_lovasz_c_1_8: 0.0716, loss_voxel_ce_c_0: 0.5289, loss_voxel_sem_scal_c_0: 2.8098, loss_voxel_geo_scal_c_0: 1.8235, loss_voxel_lovasz_c_0: 0.7303, loss: 10.9919, grad_norm: 9.4197
2024-05-07 12:30:48,274 - mmdet - INFO - Epoch [4][100/440]	lr: 1.000e-04, eta: 11:35:23, time: 4.957, data_time: 0.124, memory: 17157, loss_depth: 0.1720, render_dep_loss_c_1_2: 0.3867, loss_voxel_ce_c_1_2: 0.4296, loss_voxel_sem_scal_c_1_2: 1.2164, loss_voxel_geo_scal_c_1_2: 0.6997, loss_voxel_lovasz_c_1_2: 0.3375, loss_voxel_ce_c_1_4: 0.2682, loss_voxel_sem_scal_c_1_4: 0.5559, loss_voxel_geo_scal_c_1_4: 0.2606, loss_voxel_lovasz_c_1_4: 0.1578, loss_voxel_ce_c_1_8: 0.1582, loss_voxel_sem_scal_c_1_8: 0.2542, loss_voxel_geo_scal_c_1_8: 0.0990, loss_voxel_lovasz_c_1_8: 0.0716, loss_voxel_ce_c_0: 0.5196, loss_voxel_sem_scal_c_0: 2.7766, loss_voxel_geo_scal_c_0: 1.8005, loss_voxel_lovasz_c_0: 0.7269, loss: 10.8911, grad_norm: 9.1629
2024-05-07 12:34:54,513 - mmdet - INFO - Epoch [4][150/440]	lr: 1.000e-04, eta: 11:33:26, time: 4.925, data_time: 0.124, memory: 17157, loss_depth: 0.1707, render_dep_loss_c_1_2: 0.3866, loss_voxel_ce_c_1_2: 0.4296, loss_voxel_sem_scal_c_1_2: 1.1910, loss_voxel_geo_scal_c_1_2: 0.6902, loss_voxel_lovasz_c_1_2: 0.3368, loss_voxel_ce_c_1_4: 0.2665, loss_voxel_sem_scal_c_1_4: 0.5538, loss_voxel_geo_scal_c_1_4: 0.2574, loss_voxel_lovasz_c_1_4: 0.1578, loss_voxel_ce_c_1_8: 0.1565, loss_voxel_sem_scal_c_1_8: 0.2567, loss_voxel_geo_scal_c_1_8: 0.0976, loss_voxel_lovasz_c_1_8: 0.0718, loss_voxel_ce_c_0: 0.5201, loss_voxel_sem_scal_c_0: 2.7239, loss_voxel_geo_scal_c_0: 1.7822, loss_voxel_lovasz_c_0: 0.7257, loss: 10.7748, grad_norm: 8.6806
2024-05-07 12:39:00,996 - mmdet - INFO - Epoch [4][200/440]	lr: 1.000e-04, eta: 11:31:22, time: 4.930, data_time: 0.124, memory: 17157, loss_depth: 0.1701, render_dep_loss_c_1_2: 0.3855, loss_voxel_ce_c_1_2: 0.4297, loss_voxel_sem_scal_c_1_2: 1.1710, loss_voxel_geo_scal_c_1_2: 0.6927, loss_voxel_lovasz_c_1_2: 0.3352, loss_voxel_ce_c_1_4: 0.2669, loss_voxel_sem_scal_c_1_4: 0.5475, loss_voxel_geo_scal_c_1_4: 0.2574, loss_voxel_lovasz_c_1_4: 0.1566, loss_voxel_ce_c_1_8: 0.1559, loss_voxel_sem_scal_c_1_8: 0.2492, loss_voxel_geo_scal_c_1_8: 0.0976, loss_voxel_lovasz_c_1_8: 0.0711, loss_voxel_ce_c_0: 0.5204, loss_voxel_sem_scal_c_0: 2.6999, loss_voxel_geo_scal_c_0: 1.7730, loss_voxel_lovasz_c_0: 0.7231, loss: 10.7027, grad_norm: 8.3119
2024-05-07 12:43:08,314 - mmdet - INFO - Epoch [4][250/440]	lr: 1.000e-04, eta: 11:29:15, time: 4.946, data_time: 0.126, memory: 17157, loss_depth: 0.1700, render_dep_loss_c_1_2: 0.3840, loss_voxel_ce_c_1_2: 0.4270, loss_voxel_sem_scal_c_1_2: 1.1894, loss_voxel_geo_scal_c_1_2: 0.6910, loss_voxel_lovasz_c_1_2: 0.3363, loss_voxel_ce_c_1_4: 0.2645, loss_voxel_sem_scal_c_1_4: 0.5500, loss_voxel_geo_scal_c_1_4: 0.2580, loss_voxel_lovasz_c_1_4: 0.1569, loss_voxel_ce_c_1_8: 0.1560, loss_voxel_sem_scal_c_1_8: 0.2479, loss_voxel_geo_scal_c_1_8: 0.0982, loss_voxel_lovasz_c_1_8: 0.0711, loss_voxel_ce_c_0: 0.5116, loss_voxel_sem_scal_c_0: 2.7149, loss_voxel_geo_scal_c_0: 1.7691, loss_voxel_lovasz_c_0: 0.7238, loss: 10.7199, grad_norm: 9.1737
2024-05-07 12:47:14,865 - mmdet - INFO - Epoch [4][300/440]	lr: 1.000e-04, eta: 11:26:56, time: 4.931, data_time: 0.127, memory: 17157, loss_depth: 0.1694, render_dep_loss_c_1_2: 0.3874, loss_voxel_ce_c_1_2: 0.4245, loss_voxel_sem_scal_c_1_2: 1.1642, loss_voxel_geo_scal_c_1_2: 0.6883, loss_voxel_lovasz_c_1_2: 0.3337, loss_voxel_ce_c_1_4: 0.2638, loss_voxel_sem_scal_c_1_4: 0.5306, loss_voxel_geo_scal_c_1_4: 0.2558, loss_voxel_lovasz_c_1_4: 0.1556, loss_voxel_ce_c_1_8: 0.1542, loss_voxel_sem_scal_c_1_8: 0.2485, loss_voxel_geo_scal_c_1_8: 0.0972, loss_voxel_lovasz_c_1_8: 0.0709, loss_voxel_ce_c_0: 0.5136, loss_voxel_sem_scal_c_0: 2.6459, loss_voxel_geo_scal_c_0: 1.7664, loss_voxel_lovasz_c_0: 0.7203, loss: 10.5903, grad_norm: 8.0410
2024-05-07 12:51:21,247 - mmdet - INFO - Epoch [4][350/440]	lr: 1.000e-04, eta: 11:24:30, time: 4.928, data_time: 0.139, memory: 17157, loss_depth: 0.1687, render_dep_loss_c_1_2: 0.3817, loss_voxel_ce_c_1_2: 0.4212, loss_voxel_sem_scal_c_1_2: 1.1472, loss_voxel_geo_scal_c_1_2: 0.6840, loss_voxel_lovasz_c_1_2: 0.3326, loss_voxel_ce_c_1_4: 0.2616, loss_voxel_sem_scal_c_1_4: 0.5213, loss_voxel_geo_scal_c_1_4: 0.2556, loss_voxel_lovasz_c_1_4: 0.1549, loss_voxel_ce_c_1_8: 0.1530, loss_voxel_sem_scal_c_1_8: 0.2387, loss_voxel_geo_scal_c_1_8: 0.0968, loss_voxel_lovasz_c_1_8: 0.0703, loss_voxel_ce_c_0: 0.5071, loss_voxel_sem_scal_c_0: 2.6202, loss_voxel_geo_scal_c_0: 1.7464, loss_voxel_lovasz_c_0: 0.7179, loss: 10.4791, grad_norm: 8.3801
2024-05-07 12:55:27,065 - mmdet - INFO - Epoch [4][400/440]	lr: 1.000e-04, eta: 11:21:55, time: 4.916, data_time: 0.140, memory: 17157, loss_depth: 0.1681, render_dep_loss_c_1_2: 0.3821, loss_voxel_ce_c_1_2: 0.4284, loss_voxel_sem_scal_c_1_2: 1.1568, loss_voxel_geo_scal_c_1_2: 0.6805, loss_voxel_lovasz_c_1_2: 0.3334, loss_voxel_ce_c_1_4: 0.2655, loss_voxel_sem_scal_c_1_4: 0.5263, loss_voxel_geo_scal_c_1_4: 0.2534, loss_voxel_lovasz_c_1_4: 0.1557, loss_voxel_ce_c_1_8: 0.1558, loss_voxel_sem_scal_c_1_8: 0.2488, loss_voxel_geo_scal_c_1_8: 0.0959, loss_voxel_lovasz_c_1_8: 0.0708, loss_voxel_ce_c_0: 0.5160, loss_voxel_sem_scal_c_0: 2.6510, loss_voxel_geo_scal_c_0: 1.7458, loss_voxel_lovasz_c_0: 0.7184, loss: 10.5526, grad_norm: 8.7431
2024-05-07 12:58:43,385 - mmdet - INFO - Saving checkpoint at 4 epochs
2024-05-07 12:58:44,844 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_4_ema.pth
2024-05-07 13:03:00,781 - mmdet - INFO - Epoch [5][50/440]	lr: 1.000e-04, eta: 11:02:02, time: 5.117, data_time: 0.349, memory: 17157, loss_depth: 0.1678, render_dep_loss_c_1_2: 0.3806, loss_voxel_ce_c_1_2: 0.4204, loss_voxel_sem_scal_c_1_2: 1.1353, loss_voxel_geo_scal_c_1_2: 0.6804, loss_voxel_lovasz_c_1_2: 0.3306, loss_voxel_ce_c_1_4: 0.2595, loss_voxel_sem_scal_c_1_4: 0.5160, loss_voxel_geo_scal_c_1_4: 0.2525, loss_voxel_lovasz_c_1_4: 0.1539, loss_voxel_ce_c_1_8: 0.1533, loss_voxel_sem_scal_c_1_8: 0.2378, loss_voxel_geo_scal_c_1_8: 0.0954, loss_voxel_lovasz_c_1_8: 0.0696, loss_voxel_ce_c_0: 0.5088, loss_voxel_sem_scal_c_0: 2.5928, loss_voxel_geo_scal_c_0: 1.7372, loss_voxel_lovasz_c_0: 0.7138, loss: 10.4057, grad_norm: 8.0553
2024-05-07 13:07:07,085 - mmdet - INFO - Epoch [5][100/440]	lr: 1.000e-04, eta: 10:59:45, time: 4.926, data_time: 0.170, memory: 17157, loss_depth: 0.1674, render_dep_loss_c_1_2: 0.3802, loss_voxel_ce_c_1_2: 0.4164, loss_voxel_sem_scal_c_1_2: 1.1180, loss_voxel_geo_scal_c_1_2: 0.6764, loss_voxel_lovasz_c_1_2: 0.3297, loss_voxel_ce_c_1_4: 0.2571, loss_voxel_sem_scal_c_1_4: 0.5153, loss_voxel_geo_scal_c_1_4: 0.2528, loss_voxel_lovasz_c_1_4: 0.1533, loss_voxel_ce_c_1_8: 0.1507, loss_voxel_sem_scal_c_1_8: 0.2416, loss_voxel_geo_scal_c_1_8: 0.0963, loss_voxel_lovasz_c_1_8: 0.0696, loss_voxel_ce_c_0: 0.5031, loss_voxel_sem_scal_c_0: 2.5720, loss_voxel_geo_scal_c_0: 1.7225, loss_voxel_lovasz_c_0: 0.7139, loss: 10.3365, grad_norm: 8.2177
2024-05-07 13:11:14,258 - mmdet - INFO - Epoch [5][150/440]	lr: 1.000e-04, eta: 10:57:25, time: 4.935, data_time: 0.170, memory: 17157, loss_depth: 0.1674, render_dep_loss_c_1_2: 0.3815, loss_voxel_ce_c_1_2: 0.4201, loss_voxel_sem_scal_c_1_2: 1.1259, loss_voxel_geo_scal_c_1_2: 0.6766, loss_voxel_lovasz_c_1_2: 0.3303, loss_voxel_ce_c_1_4: 0.2596, loss_voxel_sem_scal_c_1_4: 0.5122, loss_voxel_geo_scal_c_1_4: 0.2530, loss_voxel_lovasz_c_1_4: 0.1538, loss_voxel_ce_c_1_8: 0.1520, loss_voxel_sem_scal_c_1_8: 0.2407, loss_voxel_geo_scal_c_1_8: 0.0949, loss_voxel_lovasz_c_1_8: 0.0696, loss_voxel_ce_c_0: 0.5092, loss_voxel_sem_scal_c_0: 2.5894, loss_voxel_geo_scal_c_0: 1.7175, loss_voxel_lovasz_c_0: 0.7132, loss: 10.3666, grad_norm: 8.7005
2024-05-07 13:15:20,146 - mmdet - INFO - Epoch [5][200/440]	lr: 1.000e-04, eta: 10:54:57, time: 4.926, data_time: 0.194, memory: 17157, loss_depth: 0.1665, render_dep_loss_c_1_2: 0.3802, loss_voxel_ce_c_1_2: 0.4196, loss_voxel_sem_scal_c_1_2: 1.1250, loss_voxel_geo_scal_c_1_2: 0.6770, loss_voxel_lovasz_c_1_2: 0.3303, loss_voxel_ce_c_1_4: 0.2593, loss_voxel_sem_scal_c_1_4: 0.5072, loss_voxel_geo_scal_c_1_4: 0.2519, loss_voxel_lovasz_c_1_4: 0.1533, loss_voxel_ce_c_1_8: 0.1508, loss_voxel_sem_scal_c_1_8: 0.2386, loss_voxel_geo_scal_c_1_8: 0.0949, loss_voxel_lovasz_c_1_8: 0.0698, loss_voxel_ce_c_0: 0.5099, loss_voxel_sem_scal_c_0: 2.5869, loss_voxel_geo_scal_c_0: 1.7164, loss_voxel_lovasz_c_0: 0.7134, loss: 10.3511, grad_norm: 8.2167
2024-05-07 13:19:26,251 - mmdet - INFO - Epoch [5][250/440]	lr: 1.000e-04, eta: 10:52:24, time: 4.922, data_time: 0.185, memory: 17157, loss_depth: 0.1660, render_dep_loss_c_1_2: 0.3769, loss_voxel_ce_c_1_2: 0.4102, loss_voxel_sem_scal_c_1_2: 1.1295, loss_voxel_geo_scal_c_1_2: 0.6686, loss_voxel_lovasz_c_1_2: 0.3297, loss_voxel_ce_c_1_4: 0.2537, loss_voxel_sem_scal_c_1_4: 0.5083, loss_voxel_geo_scal_c_1_4: 0.2482, loss_voxel_lovasz_c_1_4: 0.1530, loss_voxel_ce_c_1_8: 0.1477, loss_voxel_sem_scal_c_1_8: 0.2363, loss_voxel_geo_scal_c_1_8: 0.0934, loss_voxel_lovasz_c_1_8: 0.0694, loss_voxel_ce_c_0: 0.4961, loss_voxel_sem_scal_c_0: 2.5782, loss_voxel_geo_scal_c_0: 1.7003, loss_voxel_lovasz_c_0: 0.7124, loss: 10.2778, grad_norm: 8.1818
2024-05-07 13:23:31,696 - mmdet - INFO - Epoch [5][300/440]	lr: 1.000e-04, eta: 10:49:43, time: 4.909, data_time: 0.183, memory: 17157, loss_depth: 0.1661, render_dep_loss_c_1_2: 0.3769, loss_voxel_ce_c_1_2: 0.4164, loss_voxel_sem_scal_c_1_2: 1.0979, loss_voxel_geo_scal_c_1_2: 0.6753, loss_voxel_lovasz_c_1_2: 0.3299, loss_voxel_ce_c_1_4: 0.2569, loss_voxel_sem_scal_c_1_4: 0.5026, loss_voxel_geo_scal_c_1_4: 0.2507, loss_voxel_lovasz_c_1_4: 0.1532, loss_voxel_ce_c_1_8: 0.1501, loss_voxel_sem_scal_c_1_8: 0.2330, loss_voxel_geo_scal_c_1_8: 0.0944, loss_voxel_lovasz_c_1_8: 0.0692, loss_voxel_ce_c_0: 0.5070, loss_voxel_sem_scal_c_0: 2.5505, loss_voxel_geo_scal_c_0: 1.7185, loss_voxel_lovasz_c_0: 0.7134, loss: 10.2622, grad_norm: 8.1683
2024-05-07 13:27:37,270 - mmdet - INFO - Epoch [5][350/440]	lr: 1.000e-04, eta: 10:46:59, time: 4.911, data_time: 0.183, memory: 17157, loss_depth: 0.1654, render_dep_loss_c_1_2: 0.3770, loss_voxel_ce_c_1_2: 0.4145, loss_voxel_sem_scal_c_1_2: 1.0950, loss_voxel_geo_scal_c_1_2: 0.6630, loss_voxel_lovasz_c_1_2: 0.3258, loss_voxel_ce_c_1_4: 0.2557, loss_voxel_sem_scal_c_1_4: 0.4912, loss_voxel_geo_scal_c_1_4: 0.2455, loss_voxel_lovasz_c_1_4: 0.1508, loss_voxel_ce_c_1_8: 0.1487, loss_voxel_sem_scal_c_1_8: 0.2233, loss_voxel_geo_scal_c_1_8: 0.0918, loss_voxel_lovasz_c_1_8: 0.0682, loss_voxel_ce_c_0: 0.5066, loss_voxel_sem_scal_c_0: 2.5277, loss_voxel_geo_scal_c_0: 1.6864, loss_voxel_lovasz_c_0: 0.7083, loss: 10.1448, grad_norm: 8.7976
2024-05-07 13:31:42,751 - mmdet - INFO - Epoch [5][400/440]	lr: 1.000e-04, eta: 10:44:10, time: 4.910, data_time: 0.187, memory: 17157, loss_depth: 0.1651, render_dep_loss_c_1_2: 0.3762, loss_voxel_ce_c_1_2: 0.4076, loss_voxel_sem_scal_c_1_2: 1.0863, loss_voxel_geo_scal_c_1_2: 0.6645, loss_voxel_lovasz_c_1_2: 0.3264, loss_voxel_ce_c_1_4: 0.2520, loss_voxel_sem_scal_c_1_4: 0.4971, loss_voxel_geo_scal_c_1_4: 0.2469, loss_voxel_lovasz_c_1_4: 0.1519, loss_voxel_ce_c_1_8: 0.1470, loss_voxel_sem_scal_c_1_8: 0.2296, loss_voxel_geo_scal_c_1_8: 0.0930, loss_voxel_lovasz_c_1_8: 0.0683, loss_voxel_ce_c_0: 0.4963, loss_voxel_sem_scal_c_0: 2.5183, loss_voxel_geo_scal_c_0: 1.6798, loss_voxel_lovasz_c_0: 0.7084, loss: 10.1148, grad_norm: 8.6290
2024-05-07 13:35:00,320 - mmdet - INFO - Saving checkpoint at 5 epochs
2024-05-07 13:35:01,612 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_5_ema.pth
2024-05-07 13:39:18,298 - mmdet - INFO - Epoch [6][50/440]	lr: 1.000e-04, eta: 10:27:35, time: 5.132, data_time: 0.374, memory: 17157, loss_depth: 0.1649, render_dep_loss_c_1_2: 0.3759, loss_voxel_ce_c_1_2: 0.4115, loss_voxel_sem_scal_c_1_2: 1.0690, loss_voxel_geo_scal_c_1_2: 0.6668, loss_voxel_lovasz_c_1_2: 0.3260, loss_voxel_ce_c_1_4: 0.2540, loss_voxel_sem_scal_c_1_4: 0.4744, loss_voxel_geo_scal_c_1_4: 0.2471, loss_voxel_lovasz_c_1_4: 0.1510, loss_voxel_ce_c_1_8: 0.1482, loss_voxel_sem_scal_c_1_8: 0.2224, loss_voxel_geo_scal_c_1_8: 0.0933, loss_voxel_lovasz_c_1_8: 0.0683, loss_voxel_ce_c_0: 0.5038, loss_voxel_sem_scal_c_0: 2.4783, loss_voxel_geo_scal_c_0: 1.6936, loss_voxel_lovasz_c_0: 0.7082, loss: 10.0567, grad_norm: 7.7768
2024-05-07 13:43:25,858 - mmdet - INFO - Epoch [6][100/440]	lr: 1.000e-04, eta: 10:25:03, time: 4.951, data_time: 0.192, memory: 17157, loss_depth: 0.1641, render_dep_loss_c_1_2: 0.3798, loss_voxel_ce_c_1_2: 0.4052, loss_voxel_sem_scal_c_1_2: 1.0617, loss_voxel_geo_scal_c_1_2: 0.6562, loss_voxel_lovasz_c_1_2: 0.3249, loss_voxel_ce_c_1_4: 0.2507, loss_voxel_sem_scal_c_1_4: 0.4840, loss_voxel_geo_scal_c_1_4: 0.2446, loss_voxel_lovasz_c_1_4: 0.1502, loss_voxel_ce_c_1_8: 0.1451, loss_voxel_sem_scal_c_1_8: 0.2277, loss_voxel_geo_scal_c_1_8: 0.0921, loss_voxel_lovasz_c_1_8: 0.0683, loss_voxel_ce_c_0: 0.4947, loss_voxel_sem_scal_c_0: 2.4595, loss_voxel_geo_scal_c_0: 1.6642, loss_voxel_lovasz_c_0: 0.7045, loss: 9.9776, grad_norm: 8.1891
2024-05-07 13:47:33,286 - mmdet - INFO - Epoch [6][150/440]	lr: 1.000e-04, eta: 10:22:28, time: 4.949, data_time: 0.188, memory: 17157, loss_depth: 0.1644, render_dep_loss_c_1_2: 0.3734, loss_voxel_ce_c_1_2: 0.4053, loss_voxel_sem_scal_c_1_2: 1.0964, loss_voxel_geo_scal_c_1_2: 0.6563, loss_voxel_lovasz_c_1_2: 0.3269, loss_voxel_ce_c_1_4: 0.2506, loss_voxel_sem_scal_c_1_4: 0.4979, loss_voxel_geo_scal_c_1_4: 0.2436, loss_voxel_lovasz_c_1_4: 0.1514, loss_voxel_ce_c_1_8: 0.1456, loss_voxel_sem_scal_c_1_8: 0.2256, loss_voxel_geo_scal_c_1_8: 0.0908, loss_voxel_lovasz_c_1_8: 0.0679, loss_voxel_ce_c_0: 0.4963, loss_voxel_sem_scal_c_0: 2.5438, loss_voxel_geo_scal_c_0: 1.6622, loss_voxel_lovasz_c_0: 0.7095, loss: 10.1079, grad_norm: 8.2247
2024-05-07 13:51:40,225 - mmdet - INFO - Epoch [6][200/440]	lr: 1.000e-04, eta: 10:19:46, time: 4.939, data_time: 0.191, memory: 17157, loss_depth: 0.1641, render_dep_loss_c_1_2: 0.3712, loss_voxel_ce_c_1_2: 0.4054, loss_voxel_sem_scal_c_1_2: 1.0376, loss_voxel_geo_scal_c_1_2: 0.6567, loss_voxel_lovasz_c_1_2: 0.3247, loss_voxel_ce_c_1_4: 0.2497, loss_voxel_sem_scal_c_1_4: 0.4725, loss_voxel_geo_scal_c_1_4: 0.2437, loss_voxel_lovasz_c_1_4: 0.1501, loss_voxel_ce_c_1_8: 0.1454, loss_voxel_sem_scal_c_1_8: 0.2189, loss_voxel_geo_scal_c_1_8: 0.0918, loss_voxel_lovasz_c_1_8: 0.0682, loss_voxel_ce_c_0: 0.4934, loss_voxel_sem_scal_c_0: 2.4136, loss_voxel_geo_scal_c_0: 1.6579, loss_voxel_lovasz_c_0: 0.7034, loss: 9.8683, grad_norm: 8.4001
2024-05-07 13:55:48,312 - mmdet - INFO - Epoch [6][250/440]	lr: 1.000e-04, eta: 10:17:05, time: 4.961, data_time: 0.190, memory: 17157, loss_depth: 0.1637, render_dep_loss_c_1_2: 0.3738, loss_voxel_ce_c_1_2: 0.4053, loss_voxel_sem_scal_c_1_2: 1.0729, loss_voxel_geo_scal_c_1_2: 0.6592, loss_voxel_lovasz_c_1_2: 0.3240, loss_voxel_ce_c_1_4: 0.2496, loss_voxel_sem_scal_c_1_4: 0.4803, loss_voxel_geo_scal_c_1_4: 0.2443, loss_voxel_lovasz_c_1_4: 0.1501, loss_voxel_ce_c_1_8: 0.1455, loss_voxel_sem_scal_c_1_8: 0.2209, loss_voxel_geo_scal_c_1_8: 0.0911, loss_voxel_lovasz_c_1_8: 0.0677, loss_voxel_ce_c_0: 0.4953, loss_voxel_sem_scal_c_0: 2.4808, loss_voxel_geo_scal_c_0: 1.6629, loss_voxel_lovasz_c_0: 0.7030, loss: 9.9903, grad_norm: 7.7129
2024-05-07 13:59:56,464 - mmdet - INFO - Epoch [6][300/440]	lr: 1.000e-04, eta: 10:14:21, time: 4.963, data_time: 0.195, memory: 17157, loss_depth: 0.1629, render_dep_loss_c_1_2: 0.3758, loss_voxel_ce_c_1_2: 0.4016, loss_voxel_sem_scal_c_1_2: 1.0524, loss_voxel_geo_scal_c_1_2: 0.6556, loss_voxel_lovasz_c_1_2: 0.3236, loss_voxel_ce_c_1_4: 0.2493, loss_voxel_sem_scal_c_1_4: 0.4733, loss_voxel_geo_scal_c_1_4: 0.2435, loss_voxel_lovasz_c_1_4: 0.1507, loss_voxel_ce_c_1_8: 0.1448, loss_voxel_sem_scal_c_1_8: 0.2202, loss_voxel_geo_scal_c_1_8: 0.0911, loss_voxel_lovasz_c_1_8: 0.0677, loss_voxel_ce_c_0: 0.4954, loss_voxel_sem_scal_c_0: 2.4378, loss_voxel_geo_scal_c_0: 1.6631, loss_voxel_lovasz_c_0: 0.7034, loss: 9.9121, grad_norm: 7.7993
2024-05-07 14:04:03,616 - mmdet - INFO - Epoch [6][350/440]	lr: 1.000e-04, eta: 10:11:31, time: 4.943, data_time: 0.188, memory: 17157, loss_depth: 0.1635, render_dep_loss_c_1_2: 0.3759, loss_voxel_ce_c_1_2: 0.4024, loss_voxel_sem_scal_c_1_2: 1.0464, loss_voxel_geo_scal_c_1_2: 0.6501, loss_voxel_lovasz_c_1_2: 0.3236, loss_voxel_ce_c_1_4: 0.2479, loss_voxel_sem_scal_c_1_4: 0.4758, loss_voxel_geo_scal_c_1_4: 0.2416, loss_voxel_lovasz_c_1_4: 0.1499, loss_voxel_ce_c_1_8: 0.1439, loss_voxel_sem_scal_c_1_8: 0.2218, loss_voxel_geo_scal_c_1_8: 0.0908, loss_voxel_lovasz_c_1_8: 0.0678, loss_voxel_ce_c_0: 0.4901, loss_voxel_sem_scal_c_0: 2.4267, loss_voxel_geo_scal_c_0: 1.6485, loss_voxel_lovasz_c_0: 0.7032, loss: 9.8697, grad_norm: 8.0834
2024-05-07 14:08:10,511 - mmdet - INFO - Epoch [6][400/440]	lr: 1.000e-04, eta: 10:08:36, time: 4.938, data_time: 0.188, memory: 17157, loss_depth: 0.1629, render_dep_loss_c_1_2: 0.3743, loss_voxel_ce_c_1_2: 0.4006, loss_voxel_sem_scal_c_1_2: 1.0534, loss_voxel_geo_scal_c_1_2: 0.6529, loss_voxel_lovasz_c_1_2: 0.3241, loss_voxel_ce_c_1_4: 0.2488, loss_voxel_sem_scal_c_1_4: 0.4787, loss_voxel_geo_scal_c_1_4: 0.2428, loss_voxel_lovasz_c_1_4: 0.1499, loss_voxel_ce_c_1_8: 0.1451, loss_voxel_sem_scal_c_1_8: 0.2198, loss_voxel_geo_scal_c_1_8: 0.0913, loss_voxel_lovasz_c_1_8: 0.0675, loss_voxel_ce_c_0: 0.4930, loss_voxel_sem_scal_c_0: 2.4385, loss_voxel_geo_scal_c_0: 1.6430, loss_voxel_lovasz_c_0: 0.7034, loss: 9.8899, grad_norm: 7.9174
2024-05-07 14:11:28,144 - mmdet - INFO - Saving checkpoint at 6 epochs
2024-05-07 14:11:29,422 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_6_ema.pth
2024-05-07 14:15:47,435 - mmdet - INFO - Epoch [7][50/440]	lr: 1.000e-04, eta: 9:54:10, time: 5.158, data_time: 0.391, memory: 17157, loss_depth: 0.1628, render_dep_loss_c_1_2: 0.3725, loss_voxel_ce_c_1_2: 0.4047, loss_voxel_sem_scal_c_1_2: 1.0354, loss_voxel_geo_scal_c_1_2: 0.6500, loss_voxel_lovasz_c_1_2: 0.3220, loss_voxel_ce_c_1_4: 0.2496, loss_voxel_sem_scal_c_1_4: 0.4710, loss_voxel_geo_scal_c_1_4: 0.2412, loss_voxel_lovasz_c_1_4: 0.1489, loss_voxel_ce_c_1_8: 0.1443, loss_voxel_sem_scal_c_1_8: 0.2191, loss_voxel_geo_scal_c_1_8: 0.0895, loss_voxel_lovasz_c_1_8: 0.0672, loss_voxel_ce_c_0: 0.4987, loss_voxel_sem_scal_c_0: 2.4011, loss_voxel_geo_scal_c_0: 1.6393, loss_voxel_lovasz_c_0: 0.6998, loss: 9.8171, grad_norm: 7.3001
2024-05-07 14:19:55,642 - mmdet - INFO - Epoch [7][100/440]	lr: 1.000e-04, eta: 9:51:26, time: 4.964, data_time: 0.199, memory: 17157, loss_depth: 0.1614, render_dep_loss_c_1_2: 0.3732, loss_voxel_ce_c_1_2: 0.3947, loss_voxel_sem_scal_c_1_2: 1.0302, loss_voxel_geo_scal_c_1_2: 0.6413, loss_voxel_lovasz_c_1_2: 0.3229, loss_voxel_ce_c_1_4: 0.2437, loss_voxel_sem_scal_c_1_4: 0.4679, loss_voxel_geo_scal_c_1_4: 0.2380, loss_voxel_lovasz_c_1_4: 0.1495, loss_voxel_ce_c_1_8: 0.1419, loss_voxel_sem_scal_c_1_8: 0.2086, loss_voxel_geo_scal_c_1_8: 0.0889, loss_voxel_lovasz_c_1_8: 0.0669, loss_voxel_ce_c_0: 0.4819, loss_voxel_sem_scal_c_0: 2.3902, loss_voxel_geo_scal_c_0: 1.6196, loss_voxel_lovasz_c_0: 0.7016, loss: 9.7223, grad_norm: 8.5850
2024-05-07 14:24:03,200 - mmdet - INFO - Epoch [7][150/440]	lr: 1.000e-04, eta: 9:48:36, time: 4.951, data_time: 0.198, memory: 17157, loss_depth: 0.1624, render_dep_loss_c_1_2: 0.3736, loss_voxel_ce_c_1_2: 0.4015, loss_voxel_sem_scal_c_1_2: 1.0455, loss_voxel_geo_scal_c_1_2: 0.6549, loss_voxel_lovasz_c_1_2: 0.3227, loss_voxel_ce_c_1_4: 0.2473, loss_voxel_sem_scal_c_1_4: 0.4611, loss_voxel_geo_scal_c_1_4: 0.2433, loss_voxel_lovasz_c_1_4: 0.1492, loss_voxel_ce_c_1_8: 0.1429, loss_voxel_sem_scal_c_1_8: 0.2182, loss_voxel_geo_scal_c_1_8: 0.0908, loss_voxel_lovasz_c_1_8: 0.0676, loss_voxel_ce_c_0: 0.4960, loss_voxel_sem_scal_c_0: 2.4161, loss_voxel_geo_scal_c_0: 1.6593, loss_voxel_lovasz_c_0: 0.7013, loss: 9.8540, grad_norm: 8.3920
2024-05-07 14:28:10,141 - mmdet - INFO - Epoch [7][200/440]	lr: 1.000e-04, eta: 9:45:43, time: 4.939, data_time: 0.196, memory: 17157, loss_depth: 0.1614, render_dep_loss_c_1_2: 0.3736, loss_voxel_ce_c_1_2: 0.3975, loss_voxel_sem_scal_c_1_2: 1.0236, loss_voxel_geo_scal_c_1_2: 0.6459, loss_voxel_lovasz_c_1_2: 0.3217, loss_voxel_ce_c_1_4: 0.2451, loss_voxel_sem_scal_c_1_4: 0.4618, loss_voxel_geo_scal_c_1_4: 0.2407, loss_voxel_lovasz_c_1_4: 0.1487, loss_voxel_ce_c_1_8: 0.1411, loss_voxel_sem_scal_c_1_8: 0.2162, loss_voxel_geo_scal_c_1_8: 0.0898, loss_voxel_lovasz_c_1_8: 0.0667, loss_voxel_ce_c_0: 0.4865, loss_voxel_sem_scal_c_0: 2.3811, loss_voxel_geo_scal_c_0: 1.6328, loss_voxel_lovasz_c_0: 0.6997, loss: 9.7339, grad_norm: 8.0575
2024-05-07 14:32:17,575 - mmdet - INFO - Epoch [7][250/440]	lr: 1.000e-04, eta: 9:42:48, time: 4.949, data_time: 0.198, memory: 17157, loss_depth: 0.1608, render_dep_loss_c_1_2: 0.3731, loss_voxel_ce_c_1_2: 0.3945, loss_voxel_sem_scal_c_1_2: 1.0371, loss_voxel_geo_scal_c_1_2: 0.6412, loss_voxel_lovasz_c_1_2: 0.3210, loss_voxel_ce_c_1_4: 0.2439, loss_voxel_sem_scal_c_1_4: 0.4593, loss_voxel_geo_scal_c_1_4: 0.2377, loss_voxel_lovasz_c_1_4: 0.1479, loss_voxel_ce_c_1_8: 0.1410, loss_voxel_sem_scal_c_1_8: 0.2153, loss_voxel_geo_scal_c_1_8: 0.0891, loss_voxel_lovasz_c_1_8: 0.0667, loss_voxel_ce_c_0: 0.4827, loss_voxel_sem_scal_c_0: 2.4044, loss_voxel_geo_scal_c_0: 1.6218, loss_voxel_lovasz_c_0: 0.6977, loss: 9.7351, grad_norm: 7.4318
2024-05-07 14:36:24,583 - mmdet - INFO - Epoch [7][300/440]	lr: 1.000e-04, eta: 9:39:49, time: 4.940, data_time: 0.200, memory: 17157, loss_depth: 0.1604, render_dep_loss_c_1_2: 0.3686, loss_voxel_ce_c_1_2: 0.3971, loss_voxel_sem_scal_c_1_2: 1.0392, loss_voxel_geo_scal_c_1_2: 0.6480, loss_voxel_lovasz_c_1_2: 0.3207, loss_voxel_ce_c_1_4: 0.2440, loss_voxel_sem_scal_c_1_4: 0.4601, loss_voxel_geo_scal_c_1_4: 0.2404, loss_voxel_lovasz_c_1_4: 0.1475, loss_voxel_ce_c_1_8: 0.1408, loss_voxel_sem_scal_c_1_8: 0.2127, loss_voxel_geo_scal_c_1_8: 0.0897, loss_voxel_lovasz_c_1_8: 0.0664, loss_voxel_ce_c_0: 0.4897, loss_voxel_sem_scal_c_0: 2.4079, loss_voxel_geo_scal_c_0: 1.6354, loss_voxel_lovasz_c_0: 0.6978, loss: 9.7662, grad_norm: 8.5013
2024-05-07 14:40:33,651 - mmdet - INFO - Epoch [7][350/440]	lr: 1.000e-04, eta: 9:36:53, time: 4.982, data_time: 0.199, memory: 17157, loss_depth: 0.1610, render_dep_loss_c_1_2: 0.3723, loss_voxel_ce_c_1_2: 0.4010, loss_voxel_sem_scal_c_1_2: 1.0339, loss_voxel_geo_scal_c_1_2: 0.6535, loss_voxel_lovasz_c_1_2: 0.3206, loss_voxel_ce_c_1_4: 0.2466, loss_voxel_sem_scal_c_1_4: 0.4622, loss_voxel_geo_scal_c_1_4: 0.2425, loss_voxel_lovasz_c_1_4: 0.1486, loss_voxel_ce_c_1_8: 0.1431, loss_voxel_sem_scal_c_1_8: 0.2169, loss_voxel_geo_scal_c_1_8: 0.0907, loss_voxel_lovasz_c_1_8: 0.0672, loss_voxel_ce_c_0: 0.4944, loss_voxel_sem_scal_c_0: 2.3973, loss_voxel_geo_scal_c_0: 1.6412, loss_voxel_lovasz_c_0: 0.6991, loss: 9.7922, grad_norm: 8.3871
2024-05-07 14:44:40,965 - mmdet - INFO - Epoch [7][400/440]	lr: 1.000e-04, eta: 9:33:51, time: 4.946, data_time: 0.198, memory: 17157, loss_depth: 0.1604, render_dep_loss_c_1_2: 0.3713, loss_voxel_ce_c_1_2: 0.3926, loss_voxel_sem_scal_c_1_2: 1.0160, loss_voxel_geo_scal_c_1_2: 0.6341, loss_voxel_lovasz_c_1_2: 0.3186, loss_voxel_ce_c_1_4: 0.2411, loss_voxel_sem_scal_c_1_4: 0.4565, loss_voxel_geo_scal_c_1_4: 0.2356, loss_voxel_lovasz_c_1_4: 0.1470, loss_voxel_ce_c_1_8: 0.1389, loss_voxel_sem_scal_c_1_8: 0.2096, loss_voxel_geo_scal_c_1_8: 0.0884, loss_voxel_lovasz_c_1_8: 0.0658, loss_voxel_ce_c_0: 0.4824, loss_voxel_sem_scal_c_0: 2.3533, loss_voxel_geo_scal_c_0: 1.5933, loss_voxel_lovasz_c_0: 0.6924, loss: 9.5974, grad_norm: 8.1294
2024-05-07 14:47:58,370 - mmdet - INFO - Saving checkpoint at 7 epochs
2024-05-07 14:47:59,666 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_7_ema.pth
2024-05-07 14:52:14,535 - mmdet - INFO - Epoch [8][50/440]	lr: 1.000e-04, eta: 9:20:45, time: 5.096, data_time: 0.352, memory: 17157, loss_depth: 0.1591, render_dep_loss_c_1_2: 0.3670, loss_voxel_ce_c_1_2: 0.3877, loss_voxel_sem_scal_c_1_2: 1.0022, loss_voxel_geo_scal_c_1_2: 0.6360, loss_voxel_lovasz_c_1_2: 0.3181, loss_voxel_ce_c_1_4: 0.2386, loss_voxel_sem_scal_c_1_4: 0.4500, loss_voxel_geo_scal_c_1_4: 0.2357, loss_voxel_lovasz_c_1_4: 0.1464, loss_voxel_ce_c_1_8: 0.1375, loss_voxel_sem_scal_c_1_8: 0.2048, loss_voxel_geo_scal_c_1_8: 0.0879, loss_voxel_lovasz_c_1_8: 0.0657, loss_voxel_ce_c_0: 0.4779, loss_voxel_sem_scal_c_0: 2.3348, loss_voxel_geo_scal_c_0: 1.6044, loss_voxel_lovasz_c_0: 0.6942, loss: 9.5480, grad_norm: 8.0639
2024-05-07 14:56:19,298 - mmdet - INFO - Epoch [8][100/440]	lr: 1.000e-04, eta: 9:17:42, time: 4.895, data_time: 0.160, memory: 17157, loss_depth: 0.1602, render_dep_loss_c_1_2: 0.3708, loss_voxel_ce_c_1_2: 0.3929, loss_voxel_sem_scal_c_1_2: 0.9904, loss_voxel_geo_scal_c_1_2: 0.6374, loss_voxel_lovasz_c_1_2: 0.3183, loss_voxel_ce_c_1_4: 0.2417, loss_voxel_sem_scal_c_1_4: 0.4410, loss_voxel_geo_scal_c_1_4: 0.2360, loss_voxel_lovasz_c_1_4: 0.1465, loss_voxel_ce_c_1_8: 0.1385, loss_voxel_sem_scal_c_1_8: 0.2096, loss_voxel_geo_scal_c_1_8: 0.0878, loss_voxel_lovasz_c_1_8: 0.0665, loss_voxel_ce_c_0: 0.4829, loss_voxel_sem_scal_c_0: 2.3088, loss_voxel_geo_scal_c_0: 1.6035, loss_voxel_lovasz_c_0: 0.6936, loss: 9.5264, grad_norm: 7.7643
2024-05-07 15:00:25,126 - mmdet - INFO - Epoch [8][150/440]	lr: 1.000e-04, eta: 9:14:38, time: 4.917, data_time: 0.157, memory: 17157, loss_depth: 0.1598, render_dep_loss_c_1_2: 0.3723, loss_voxel_ce_c_1_2: 0.3909, loss_voxel_sem_scal_c_1_2: 0.9834, loss_voxel_geo_scal_c_1_2: 0.6419, loss_voxel_lovasz_c_1_2: 0.3189, loss_voxel_ce_c_1_4: 0.2409, loss_voxel_sem_scal_c_1_4: 0.4455, loss_voxel_geo_scal_c_1_4: 0.2374, loss_voxel_lovasz_c_1_4: 0.1476, loss_voxel_ce_c_1_8: 0.1389, loss_voxel_sem_scal_c_1_8: 0.2026, loss_voxel_geo_scal_c_1_8: 0.0885, loss_voxel_lovasz_c_1_8: 0.0658, loss_voxel_ce_c_0: 0.4822, loss_voxel_sem_scal_c_0: 2.3054, loss_voxel_geo_scal_c_0: 1.6205, loss_voxel_lovasz_c_0: 0.6948, loss: 9.5373, grad_norm: 8.1651
2024-05-07 15:04:30,748 - mmdet - INFO - Epoch [8][200/440]	lr: 1.000e-04, eta: 9:11:33, time: 4.912, data_time: 0.159, memory: 17157, loss_depth: 0.1588, render_dep_loss_c_1_2: 0.3702, loss_voxel_ce_c_1_2: 0.3942, loss_voxel_sem_scal_c_1_2: 1.0130, loss_voxel_geo_scal_c_1_2: 0.6354, loss_voxel_lovasz_c_1_2: 0.3179, loss_voxel_ce_c_1_4: 0.2419, loss_voxel_sem_scal_c_1_4: 0.4458, loss_voxel_geo_scal_c_1_4: 0.2366, loss_voxel_lovasz_c_1_4: 0.1464, loss_voxel_ce_c_1_8: 0.1400, loss_voxel_sem_scal_c_1_8: 0.2064, loss_voxel_geo_scal_c_1_8: 0.0876, loss_voxel_lovasz_c_1_8: 0.0658, loss_voxel_ce_c_0: 0.4856, loss_voxel_sem_scal_c_0: 2.3638, loss_voxel_geo_scal_c_0: 1.5974, loss_voxel_lovasz_c_0: 0.6928, loss: 9.5995, grad_norm: 7.8785
2024-05-07 15:08:35,795 - mmdet - INFO - Epoch [8][250/440]	lr: 1.000e-04, eta: 9:08:24, time: 4.901, data_time: 0.160, memory: 17157, loss_depth: 0.1591, render_dep_loss_c_1_2: 0.3689, loss_voxel_ce_c_1_2: 0.3905, loss_voxel_sem_scal_c_1_2: 0.9778, loss_voxel_geo_scal_c_1_2: 0.6368, loss_voxel_lovasz_c_1_2: 0.3172, loss_voxel_ce_c_1_4: 0.2396, loss_voxel_sem_scal_c_1_4: 0.4401, loss_voxel_geo_scal_c_1_4: 0.2349, loss_voxel_lovasz_c_1_4: 0.1463, loss_voxel_ce_c_1_8: 0.1386, loss_voxel_sem_scal_c_1_8: 0.2035, loss_voxel_geo_scal_c_1_8: 0.0873, loss_voxel_lovasz_c_1_8: 0.0657, loss_voxel_ce_c_0: 0.4850, loss_voxel_sem_scal_c_0: 2.2996, loss_voxel_geo_scal_c_0: 1.6124, loss_voxel_lovasz_c_0: 0.6913, loss: 9.4946, grad_norm: 7.7712
2024-05-07 15:12:40,785 - mmdet - INFO - Epoch [8][300/440]	lr: 1.000e-04, eta: 9:05:13, time: 4.900, data_time: 0.163, memory: 17157, loss_depth: 0.1590, render_dep_loss_c_1_2: 0.3700, loss_voxel_ce_c_1_2: 0.3912, loss_voxel_sem_scal_c_1_2: 0.9733, loss_voxel_geo_scal_c_1_2: 0.6358, loss_voxel_lovasz_c_1_2: 0.3179, loss_voxel_ce_c_1_4: 0.2399, loss_voxel_sem_scal_c_1_4: 0.4344, loss_voxel_geo_scal_c_1_4: 0.2350, loss_voxel_lovasz_c_1_4: 0.1463, loss_voxel_ce_c_1_8: 0.1388, loss_voxel_sem_scal_c_1_8: 0.2021, loss_voxel_geo_scal_c_1_8: 0.0874, loss_voxel_lovasz_c_1_8: 0.0657, loss_voxel_ce_c_0: 0.4835, loss_voxel_sem_scal_c_0: 2.2907, loss_voxel_geo_scal_c_0: 1.6021, loss_voxel_lovasz_c_0: 0.6941, loss: 9.4672, grad_norm: 7.8845
2024-05-07 15:16:45,766 - mmdet - INFO - Epoch [8][350/440]	lr: 1.000e-04, eta: 9:02:01, time: 4.900, data_time: 0.157, memory: 17157, loss_depth: 0.1593, render_dep_loss_c_1_2: 0.3677, loss_voxel_ce_c_1_2: 0.3924, loss_voxel_sem_scal_c_1_2: 0.9752, loss_voxel_geo_scal_c_1_2: 0.6360, loss_voxel_lovasz_c_1_2: 0.3165, loss_voxel_ce_c_1_4: 0.2416, loss_voxel_sem_scal_c_1_4: 0.4398, loss_voxel_geo_scal_c_1_4: 0.2363, loss_voxel_lovasz_c_1_4: 0.1460, loss_voxel_ce_c_1_8: 0.1392, loss_voxel_sem_scal_c_1_8: 0.2057, loss_voxel_geo_scal_c_1_8: 0.0879, loss_voxel_lovasz_c_1_8: 0.0655, loss_voxel_ce_c_0: 0.4874, loss_voxel_sem_scal_c_0: 2.2641, loss_voxel_geo_scal_c_0: 1.6054, loss_voxel_lovasz_c_0: 0.6911, loss: 9.4572, grad_norm: 7.6292
2024-05-07 15:20:51,166 - mmdet - INFO - Epoch [8][400/440]	lr: 1.000e-04, eta: 8:58:48, time: 4.908, data_time: 0.165, memory: 17157, loss_depth: 0.1592, render_dep_loss_c_1_2: 0.3693, loss_voxel_ce_c_1_2: 0.3933, loss_voxel_sem_scal_c_1_2: 0.9850, loss_voxel_geo_scal_c_1_2: 0.6326, loss_voxel_lovasz_c_1_2: 0.3170, loss_voxel_ce_c_1_4: 0.2415, loss_voxel_sem_scal_c_1_4: 0.4382, loss_voxel_geo_scal_c_1_4: 0.2343, loss_voxel_lovasz_c_1_4: 0.1460, loss_voxel_ce_c_1_8: 0.1383, loss_voxel_sem_scal_c_1_8: 0.2038, loss_voxel_geo_scal_c_1_8: 0.0869, loss_voxel_lovasz_c_1_8: 0.0654, loss_voxel_ce_c_0: 0.4863, loss_voxel_sem_scal_c_0: 2.2986, loss_voxel_geo_scal_c_0: 1.5995, loss_voxel_lovasz_c_0: 0.6923, loss: 9.4876, grad_norm: 7.8384
2024-05-07 15:24:07,732 - mmdet - INFO - Saving checkpoint at 8 epochs
2024-05-07 15:24:09,001 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_8_ema.pth
2024-05-07 15:28:27,487 - mmdet - INFO - Epoch [9][50/440]	lr: 1.000e-04, eta: 8:46:59, time: 5.168, data_time: 0.386, memory: 17157, loss_depth: 0.1594, render_dep_loss_c_1_2: 0.3710, loss_voxel_ce_c_1_2: 0.3908, loss_voxel_sem_scal_c_1_2: 0.9878, loss_voxel_geo_scal_c_1_2: 0.6370, loss_voxel_lovasz_c_1_2: 0.3177, loss_voxel_ce_c_1_4: 0.2412, loss_voxel_sem_scal_c_1_4: 0.4465, loss_voxel_geo_scal_c_1_4: 0.2363, loss_voxel_lovasz_c_1_4: 0.1467, loss_voxel_ce_c_1_8: 0.1384, loss_voxel_sem_scal_c_1_8: 0.2046, loss_voxel_geo_scal_c_1_8: 0.0881, loss_voxel_lovasz_c_1_8: 0.0660, loss_voxel_ce_c_0: 0.4773, loss_voxel_sem_scal_c_0: 2.3066, loss_voxel_geo_scal_c_0: 1.6026, loss_voxel_lovasz_c_0: 0.6931, loss: 9.5112, grad_norm: 8.0979
2024-05-07 15:32:37,384 - mmdet - INFO - Epoch [9][100/440]	lr: 1.000e-04, eta: 8:43:58, time: 4.998, data_time: 0.208, memory: 17157, loss_depth: 0.1591, render_dep_loss_c_1_2: 0.3697, loss_voxel_ce_c_1_2: 0.3902, loss_voxel_sem_scal_c_1_2: 0.9806, loss_voxel_geo_scal_c_1_2: 0.6341, loss_voxel_lovasz_c_1_2: 0.3157, loss_voxel_ce_c_1_4: 0.2388, loss_voxel_sem_scal_c_1_4: 0.4316, loss_voxel_geo_scal_c_1_4: 0.2355, loss_voxel_lovasz_c_1_4: 0.1454, loss_voxel_ce_c_1_8: 0.1379, loss_voxel_sem_scal_c_1_8: 0.1990, loss_voxel_geo_scal_c_1_8: 0.0874, loss_voxel_lovasz_c_1_8: 0.0650, loss_voxel_ce_c_0: 0.4831, loss_voxel_sem_scal_c_0: 2.2956, loss_voxel_geo_scal_c_0: 1.5988, loss_voxel_lovasz_c_0: 0.6901, loss: 9.4573, grad_norm: 8.0256
2024-05-07 15:36:47,701 - mmdet - INFO - Epoch [9][150/440]	lr: 1.000e-04, eta: 8:40:56, time: 5.006, data_time: 0.212, memory: 17157, loss_depth: 0.1584, render_dep_loss_c_1_2: 0.3651, loss_voxel_ce_c_1_2: 0.3856, loss_voxel_sem_scal_c_1_2: 0.9490, loss_voxel_geo_scal_c_1_2: 0.6238, loss_voxel_lovasz_c_1_2: 0.3133, loss_voxel_ce_c_1_4: 0.2360, loss_voxel_sem_scal_c_1_4: 0.4276, loss_voxel_geo_scal_c_1_4: 0.2315, loss_voxel_lovasz_c_1_4: 0.1447, loss_voxel_ce_c_1_8: 0.1353, loss_voxel_sem_scal_c_1_8: 0.1994, loss_voxel_geo_scal_c_1_8: 0.0856, loss_voxel_lovasz_c_1_8: 0.0647, loss_voxel_ce_c_0: 0.4768, loss_voxel_sem_scal_c_0: 2.2146, loss_voxel_geo_scal_c_0: 1.5703, loss_voxel_lovasz_c_0: 0.6852, loss: 9.2669, grad_norm: 7.5216
2024-05-07 15:40:58,099 - mmdet - INFO - Epoch [9][200/440]	lr: 1.000e-04, eta: 8:37:53, time: 5.008, data_time: 0.211, memory: 17157, loss_depth: 0.1584, render_dep_loss_c_1_2: 0.3682, loss_voxel_ce_c_1_2: 0.3818, loss_voxel_sem_scal_c_1_2: 0.9594, loss_voxel_geo_scal_c_1_2: 0.6241, loss_voxel_lovasz_c_1_2: 0.3145, loss_voxel_ce_c_1_4: 0.2359, loss_voxel_sem_scal_c_1_4: 0.4260, loss_voxel_geo_scal_c_1_4: 0.2313, loss_voxel_lovasz_c_1_4: 0.1450, loss_voxel_ce_c_1_8: 0.1360, loss_voxel_sem_scal_c_1_8: 0.1990, loss_voxel_geo_scal_c_1_8: 0.0867, loss_voxel_lovasz_c_1_8: 0.0650, loss_voxel_ce_c_0: 0.4725, loss_voxel_sem_scal_c_0: 2.2450, loss_voxel_geo_scal_c_0: 1.5757, loss_voxel_lovasz_c_0: 0.6866, loss: 9.3109, grad_norm: 7.9223
2024-05-07 15:45:09,000 - mmdet - INFO - Epoch [9][250/440]	lr: 1.000e-04, eta: 8:34:48, time: 5.018, data_time: 0.210, memory: 17157, loss_depth: 0.1588, render_dep_loss_c_1_2: 0.3681, loss_voxel_ce_c_1_2: 0.3906, loss_voxel_sem_scal_c_1_2: 0.9549, loss_voxel_geo_scal_c_1_2: 0.6271, loss_voxel_lovasz_c_1_2: 0.3146, loss_voxel_ce_c_1_4: 0.2388, loss_voxel_sem_scal_c_1_4: 0.4297, loss_voxel_geo_scal_c_1_4: 0.2326, loss_voxel_lovasz_c_1_4: 0.1447, loss_voxel_ce_c_1_8: 0.1365, loss_voxel_sem_scal_c_1_8: 0.2003, loss_voxel_geo_scal_c_1_8: 0.0861, loss_voxel_lovasz_c_1_8: 0.0649, loss_voxel_ce_c_0: 0.4820, loss_voxel_sem_scal_c_0: 2.2370, loss_voxel_geo_scal_c_0: 1.5822, loss_voxel_lovasz_c_0: 0.6873, loss: 9.3364, grad_norm: 7.9711
2024-05-07 15:49:18,669 - mmdet - INFO - Epoch [9][300/440]	lr: 1.000e-04, eta: 8:31:40, time: 4.993, data_time: 0.202, memory: 17157, loss_depth: 0.1585, render_dep_loss_c_1_2: 0.3677, loss_voxel_ce_c_1_2: 0.3903, loss_voxel_sem_scal_c_1_2: 0.9728, loss_voxel_geo_scal_c_1_2: 0.6288, loss_voxel_lovasz_c_1_2: 0.3154, loss_voxel_ce_c_1_4: 0.2382, loss_voxel_sem_scal_c_1_4: 0.4298, loss_voxel_geo_scal_c_1_4: 0.2329, loss_voxel_lovasz_c_1_4: 0.1448, loss_voxel_ce_c_1_8: 0.1368, loss_voxel_sem_scal_c_1_8: 0.2000, loss_voxel_geo_scal_c_1_8: 0.0857, loss_voxel_lovasz_c_1_8: 0.0652, loss_voxel_ce_c_0: 0.4866, loss_voxel_sem_scal_c_0: 2.2877, loss_voxel_geo_scal_c_0: 1.5901, loss_voxel_lovasz_c_0: 0.6896, loss: 9.4209, grad_norm: 8.3015
2024-05-07 15:53:29,075 - mmdet - INFO - Epoch [9][350/440]	lr: 1.000e-04, eta: 8:28:32, time: 5.008, data_time: 0.210, memory: 17157, loss_depth: 0.1574, render_dep_loss_c_1_2: 0.3652, loss_voxel_ce_c_1_2: 0.3836, loss_voxel_sem_scal_c_1_2: 0.9631, loss_voxel_geo_scal_c_1_2: 0.6233, loss_voxel_lovasz_c_1_2: 0.3148, loss_voxel_ce_c_1_4: 0.2358, loss_voxel_sem_scal_c_1_4: 0.4286, loss_voxel_geo_scal_c_1_4: 0.2298, loss_voxel_lovasz_c_1_4: 0.1452, loss_voxel_ce_c_1_8: 0.1350, loss_voxel_sem_scal_c_1_8: 0.2011, loss_voxel_geo_scal_c_1_8: 0.0853, loss_voxel_lovasz_c_1_8: 0.0649, loss_voxel_ce_c_0: 0.4763, loss_voxel_sem_scal_c_0: 2.2576, loss_voxel_geo_scal_c_0: 1.5719, loss_voxel_lovasz_c_0: 0.6888, loss: 9.3277, grad_norm: 7.6614
2024-05-07 15:57:38,124 - mmdet - INFO - Epoch [9][400/440]	lr: 1.000e-04, eta: 8:25:19, time: 4.981, data_time: 0.213, memory: 17157, loss_depth: 0.1572, render_dep_loss_c_1_2: 0.3644, loss_voxel_ce_c_1_2: 0.3821, loss_voxel_sem_scal_c_1_2: 0.9562, loss_voxel_geo_scal_c_1_2: 0.6172, loss_voxel_lovasz_c_1_2: 0.3138, loss_voxel_ce_c_1_4: 0.2340, loss_voxel_sem_scal_c_1_4: 0.4282, loss_voxel_geo_scal_c_1_4: 0.2279, loss_voxel_lovasz_c_1_4: 0.1446, loss_voxel_ce_c_1_8: 0.1348, loss_voxel_sem_scal_c_1_8: 0.1957, loss_voxel_geo_scal_c_1_8: 0.0843, loss_voxel_lovasz_c_1_8: 0.0647, loss_voxel_ce_c_0: 0.4742, loss_voxel_sem_scal_c_0: 2.2323, loss_voxel_geo_scal_c_0: 1.5566, loss_voxel_lovasz_c_0: 0.6855, loss: 9.2536, grad_norm: 7.6450
2024-05-07 16:00:56,548 - mmdet - INFO - Saving checkpoint at 9 epochs
2024-05-07 16:00:57,803 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_9_ema.pth
2024-05-07 16:05:15,145 - mmdet - INFO - Epoch [10][50/440]	lr: 1.000e-04, eta: 8:14:17, time: 5.145, data_time: 0.392, memory: 17157, loss_depth: 0.1575, render_dep_loss_c_1_2: 0.3652, loss_voxel_ce_c_1_2: 0.3845, loss_voxel_sem_scal_c_1_2: 0.9224, loss_voxel_geo_scal_c_1_2: 0.6199, loss_voxel_lovasz_c_1_2: 0.3113, loss_voxel_ce_c_1_4: 0.2359, loss_voxel_sem_scal_c_1_4: 0.4127, loss_voxel_geo_scal_c_1_4: 0.2298, loss_voxel_lovasz_c_1_4: 0.1435, loss_voxel_ce_c_1_8: 0.1348, loss_voxel_sem_scal_c_1_8: 0.1919, loss_voxel_geo_scal_c_1_8: 0.0854, loss_voxel_lovasz_c_1_8: 0.0640, loss_voxel_ce_c_0: 0.4777, loss_voxel_sem_scal_c_0: 2.1716, loss_voxel_geo_scal_c_0: 1.5644, loss_voxel_lovasz_c_0: 0.6819, loss: 9.1544, grad_norm: 7.4416
2024-05-07 16:09:22,967 - mmdet - INFO - Epoch [10][100/440]	lr: 1.000e-04, eta: 8:11:05, time: 4.956, data_time: 0.196, memory: 17157, loss_depth: 0.1564, render_dep_loss_c_1_2: 0.3663, loss_voxel_ce_c_1_2: 0.3794, loss_voxel_sem_scal_c_1_2: 0.9370, loss_voxel_geo_scal_c_1_2: 0.6159, loss_voxel_lovasz_c_1_2: 0.3116, loss_voxel_ce_c_1_4: 0.2331, loss_voxel_sem_scal_c_1_4: 0.4155, loss_voxel_geo_scal_c_1_4: 0.2275, loss_voxel_lovasz_c_1_4: 0.1430, loss_voxel_ce_c_1_8: 0.1330, loss_voxel_sem_scal_c_1_8: 0.1912, loss_voxel_geo_scal_c_1_8: 0.0837, loss_voxel_lovasz_c_1_8: 0.0638, loss_voxel_ce_c_0: 0.4702, loss_voxel_sem_scal_c_0: 2.1990, loss_voxel_geo_scal_c_0: 1.5541, loss_voxel_lovasz_c_0: 0.6827, loss: 9.1637, grad_norm: 7.6649
2024-05-07 16:13:30,743 - mmdet - INFO - Epoch [10][150/440]	lr: 1.000e-04, eta: 8:07:51, time: 4.956, data_time: 0.203, memory: 17157, loss_depth: 0.1571, render_dep_loss_c_1_2: 0.3652, loss_voxel_ce_c_1_2: 0.3854, loss_voxel_sem_scal_c_1_2: 0.9219, loss_voxel_geo_scal_c_1_2: 0.6150, loss_voxel_lovasz_c_1_2: 0.3116, loss_voxel_ce_c_1_4: 0.2348, loss_voxel_sem_scal_c_1_4: 0.4138, loss_voxel_geo_scal_c_1_4: 0.2275, loss_voxel_lovasz_c_1_4: 0.1433, loss_voxel_ce_c_1_8: 0.1348, loss_voxel_sem_scal_c_1_8: 0.1967, loss_voxel_geo_scal_c_1_8: 0.0840, loss_voxel_lovasz_c_1_8: 0.0642, loss_voxel_ce_c_0: 0.4780, loss_voxel_sem_scal_c_0: 2.1684, loss_voxel_geo_scal_c_0: 1.5556, loss_voxel_lovasz_c_0: 0.6828, loss: 9.1399, grad_norm: 7.9385
2024-05-07 16:17:37,928 - mmdet - INFO - Epoch [10][200/440]	lr: 1.000e-04, eta: 8:04:36, time: 4.944, data_time: 0.195, memory: 17157, loss_depth: 0.1569, render_dep_loss_c_1_2: 0.3664, loss_voxel_ce_c_1_2: 0.3794, loss_voxel_sem_scal_c_1_2: 0.9356, loss_voxel_geo_scal_c_1_2: 0.6191, loss_voxel_lovasz_c_1_2: 0.3130, loss_voxel_ce_c_1_4: 0.2334, loss_voxel_sem_scal_c_1_4: 0.4237, loss_voxel_geo_scal_c_1_4: 0.2287, loss_voxel_lovasz_c_1_4: 0.1445, loss_voxel_ce_c_1_8: 0.1342, loss_voxel_sem_scal_c_1_8: 0.1934, loss_voxel_geo_scal_c_1_8: 0.0844, loss_voxel_lovasz_c_1_8: 0.0645, loss_voxel_ce_c_0: 0.4718, loss_voxel_sem_scal_c_0: 2.2003, loss_voxel_geo_scal_c_0: 1.5617, loss_voxel_lovasz_c_0: 0.6862, loss: 9.1972, grad_norm: 7.7322
2024-05-07 16:21:46,179 - mmdet - INFO - Epoch [10][250/440]	lr: 1.000e-04, eta: 8:01:20, time: 4.965, data_time: 0.202, memory: 17157, loss_depth: 0.1563, render_dep_loss_c_1_2: 0.3644, loss_voxel_ce_c_1_2: 0.3806, loss_voxel_sem_scal_c_1_2: 0.9349, loss_voxel_geo_scal_c_1_2: 0.6174, loss_voxel_lovasz_c_1_2: 0.3131, loss_voxel_ce_c_1_4: 0.2337, loss_voxel_sem_scal_c_1_4: 0.4157, loss_voxel_geo_scal_c_1_4: 0.2290, loss_voxel_lovasz_c_1_4: 0.1439, loss_voxel_ce_c_1_8: 0.1345, loss_voxel_sem_scal_c_1_8: 0.1946, loss_voxel_geo_scal_c_1_8: 0.0844, loss_voxel_lovasz_c_1_8: 0.0649, loss_voxel_ce_c_0: 0.4717, loss_voxel_sem_scal_c_0: 2.1913, loss_voxel_geo_scal_c_0: 1.5526, loss_voxel_lovasz_c_0: 0.6841, loss: 9.1672, grad_norm: 7.4216
2024-05-07 16:25:53,040 - mmdet - INFO - Epoch [10][300/440]	lr: 1.000e-04, eta: 7:58:02, time: 4.937, data_time: 0.197, memory: 17157, loss_depth: 0.1561, render_dep_loss_c_1_2: 0.3642, loss_voxel_ce_c_1_2: 0.3780, loss_voxel_sem_scal_c_1_2: 0.9244, loss_voxel_geo_scal_c_1_2: 0.6131, loss_voxel_lovasz_c_1_2: 0.3116, loss_voxel_ce_c_1_4: 0.2307, loss_voxel_sem_scal_c_1_4: 0.4063, loss_voxel_geo_scal_c_1_4: 0.2270, loss_voxel_lovasz_c_1_4: 0.1434, loss_voxel_ce_c_1_8: 0.1319, loss_voxel_sem_scal_c_1_8: 0.1934, loss_voxel_geo_scal_c_1_8: 0.0838, loss_voxel_lovasz_c_1_8: 0.0643, loss_voxel_ce_c_0: 0.4683, loss_voxel_sem_scal_c_0: 2.1857, loss_voxel_geo_scal_c_0: 1.5461, loss_voxel_lovasz_c_0: 0.6826, loss: 9.1112, grad_norm: 7.4607
2024-05-07 16:29:59,718 - mmdet - INFO - Epoch [10][350/440]	lr: 1.000e-04, eta: 7:54:42, time: 4.934, data_time: 0.188, memory: 17157, loss_depth: 0.1567, render_dep_loss_c_1_2: 0.3655, loss_voxel_ce_c_1_2: 0.3804, loss_voxel_sem_scal_c_1_2: 0.9317, loss_voxel_geo_scal_c_1_2: 0.6173, loss_voxel_lovasz_c_1_2: 0.3113, loss_voxel_ce_c_1_4: 0.2340, loss_voxel_sem_scal_c_1_4: 0.4136, loss_voxel_geo_scal_c_1_4: 0.2291, loss_voxel_lovasz_c_1_4: 0.1433, loss_voxel_ce_c_1_8: 0.1335, loss_voxel_sem_scal_c_1_8: 0.1895, loss_voxel_geo_scal_c_1_8: 0.0841, loss_voxel_lovasz_c_1_8: 0.0640, loss_voxel_ce_c_0: 0.4735, loss_voxel_sem_scal_c_0: 2.1925, loss_voxel_geo_scal_c_0: 1.5603, loss_voxel_lovasz_c_0: 0.6830, loss: 9.1635, grad_norm: 7.5226
2024-05-07 16:34:06,482 - mmdet - INFO - Epoch [10][400/440]	lr: 1.000e-04, eta: 7:51:21, time: 4.935, data_time: 0.187, memory: 17157, loss_depth: 0.1566, render_dep_loss_c_1_2: 0.3675, loss_voxel_ce_c_1_2: 0.3860, loss_voxel_sem_scal_c_1_2: 0.9249, loss_voxel_geo_scal_c_1_2: 0.6204, loss_voxel_lovasz_c_1_2: 0.3121, loss_voxel_ce_c_1_4: 0.2364, loss_voxel_sem_scal_c_1_4: 0.4104, loss_voxel_geo_scal_c_1_4: 0.2302, loss_voxel_lovasz_c_1_4: 0.1436, loss_voxel_ce_c_1_8: 0.1350, loss_voxel_sem_scal_c_1_8: 0.1972, loss_voxel_geo_scal_c_1_8: 0.0848, loss_voxel_lovasz_c_1_8: 0.0649, loss_voxel_ce_c_0: 0.4786, loss_voxel_sem_scal_c_0: 2.1792, loss_voxel_geo_scal_c_0: 1.5692, loss_voxel_lovasz_c_0: 0.6839, loss: 9.1808, grad_norm: 8.3319
2024-05-07 16:37:23,367 - mmdet - INFO - Saving checkpoint at 10 epochs
2024-05-07 16:37:24,909 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_10_ema.pth
2024-05-07 16:41:44,777 - mmdet - INFO - Epoch [11][50/440]	lr: 1.000e-04, eta: 7:41:03, time: 5.196, data_time: 0.423, memory: 17157, loss_depth: 0.1562, render_dep_loss_c_1_2: 0.3668, loss_voxel_ce_c_1_2: 0.3751, loss_voxel_sem_scal_c_1_2: 0.9216, loss_voxel_geo_scal_c_1_2: 0.6175, loss_voxel_lovasz_c_1_2: 0.3086, loss_voxel_ce_c_1_4: 0.2298, loss_voxel_sem_scal_c_1_4: 0.4134, loss_voxel_geo_scal_c_1_4: 0.2282, loss_voxel_lovasz_c_1_4: 0.1420, loss_voxel_ce_c_1_8: 0.1305, loss_voxel_sem_scal_c_1_8: 0.1881, loss_voxel_geo_scal_c_1_8: 0.0843, loss_voxel_lovasz_c_1_8: 0.0632, loss_voxel_ce_c_0: 0.4659, loss_voxel_sem_scal_c_0: 2.1702, loss_voxel_geo_scal_c_0: 1.5498, loss_voxel_lovasz_c_0: 0.6788, loss: 9.0902, grad_norm: 7.8396
2024-05-07 16:45:55,184 - mmdet - INFO - Epoch [11][100/440]	lr: 1.000e-04, eta: 7:37:49, time: 5.008, data_time: 0.235, memory: 17157, loss_depth: 0.1559, render_dep_loss_c_1_2: 0.3649, loss_voxel_ce_c_1_2: 0.3778, loss_voxel_sem_scal_c_1_2: 0.9108, loss_voxel_geo_scal_c_1_2: 0.6080, loss_voxel_lovasz_c_1_2: 0.3103, loss_voxel_ce_c_1_4: 0.2322, loss_voxel_sem_scal_c_1_4: 0.4092, loss_voxel_geo_scal_c_1_4: 0.2246, loss_voxel_lovasz_c_1_4: 0.1428, loss_voxel_ce_c_1_8: 0.1331, loss_voxel_sem_scal_c_1_8: 0.1923, loss_voxel_geo_scal_c_1_8: 0.0825, loss_voxel_lovasz_c_1_8: 0.0642, loss_voxel_ce_c_0: 0.4697, loss_voxel_sem_scal_c_0: 2.1424, loss_voxel_geo_scal_c_0: 1.5382, loss_voxel_lovasz_c_0: 0.6799, loss: 9.0386, grad_norm: 7.4285
2024-05-07 16:50:05,104 - mmdet - INFO - Epoch [11][150/440]	lr: 1.000e-04, eta: 7:34:33, time: 4.998, data_time: 0.237, memory: 17157, loss_depth: 0.1562, render_dep_loss_c_1_2: 0.3644, loss_voxel_ce_c_1_2: 0.3785, loss_voxel_sem_scal_c_1_2: 0.9029, loss_voxel_geo_scal_c_1_2: 0.6105, loss_voxel_lovasz_c_1_2: 0.3101, loss_voxel_ce_c_1_4: 0.2311, loss_voxel_sem_scal_c_1_4: 0.4040, loss_voxel_geo_scal_c_1_4: 0.2259, loss_voxel_lovasz_c_1_4: 0.1424, loss_voxel_ce_c_1_8: 0.1319, loss_voxel_sem_scal_c_1_8: 0.1872, loss_voxel_geo_scal_c_1_8: 0.0835, loss_voxel_lovasz_c_1_8: 0.0635, loss_voxel_ce_c_0: 0.4699, loss_voxel_sem_scal_c_0: 2.1198, loss_voxel_geo_scal_c_0: 1.5402, loss_voxel_lovasz_c_0: 0.6801, loss: 9.0021, grad_norm: 7.2136
2024-05-07 16:54:15,408 - mmdet - INFO - Epoch [11][200/440]	lr: 1.000e-04, eta: 7:31:17, time: 5.006, data_time: 0.236, memory: 17157, loss_depth: 0.1558, render_dep_loss_c_1_2: 0.3650, loss_voxel_ce_c_1_2: 0.3822, loss_voxel_sem_scal_c_1_2: 0.9179, loss_voxel_geo_scal_c_1_2: 0.6203, loss_voxel_lovasz_c_1_2: 0.3105, loss_voxel_ce_c_1_4: 0.2336, loss_voxel_sem_scal_c_1_4: 0.4042, loss_voxel_geo_scal_c_1_4: 0.2295, loss_voxel_lovasz_c_1_4: 0.1424, loss_voxel_ce_c_1_8: 0.1334, loss_voxel_sem_scal_c_1_8: 0.1905, loss_voxel_geo_scal_c_1_8: 0.0846, loss_voxel_lovasz_c_1_8: 0.0643, loss_voxel_ce_c_0: 0.4720, loss_voxel_sem_scal_c_0: 2.1700, loss_voxel_geo_scal_c_0: 1.5594, loss_voxel_lovasz_c_0: 0.6807, loss: 9.1162, grad_norm: 7.6685
2024-05-07 16:58:25,208 - mmdet - INFO - Epoch [11][250/440]	lr: 1.000e-04, eta: 7:27:58, time: 4.996, data_time: 0.235, memory: 17157, loss_depth: 0.1550, render_dep_loss_c_1_2: 0.3630, loss_voxel_ce_c_1_2: 0.3770, loss_voxel_sem_scal_c_1_2: 0.9036, loss_voxel_geo_scal_c_1_2: 0.6038, loss_voxel_lovasz_c_1_2: 0.3078, loss_voxel_ce_c_1_4: 0.2311, loss_voxel_sem_scal_c_1_4: 0.4018, loss_voxel_geo_scal_c_1_4: 0.2240, loss_voxel_lovasz_c_1_4: 0.1409, loss_voxel_ce_c_1_8: 0.1314, loss_voxel_sem_scal_c_1_8: 0.1875, loss_voxel_geo_scal_c_1_8: 0.0827, loss_voxel_lovasz_c_1_8: 0.0631, loss_voxel_ce_c_0: 0.4715, loss_voxel_sem_scal_c_0: 2.1414, loss_voxel_geo_scal_c_0: 1.5302, loss_voxel_lovasz_c_0: 0.6775, loss: 8.9931, grad_norm: 7.6694
2024-05-07 17:02:34,627 - mmdet - INFO - Epoch [11][300/440]	lr: 1.000e-04, eta: 7:24:38, time: 4.988, data_time: 0.240, memory: 17157, loss_depth: 0.1553, render_dep_loss_c_1_2: 0.3609, loss_voxel_ce_c_1_2: 0.3787, loss_voxel_sem_scal_c_1_2: 0.9140, loss_voxel_geo_scal_c_1_2: 0.6127, loss_voxel_lovasz_c_1_2: 0.3103, loss_voxel_ce_c_1_4: 0.2326, loss_voxel_sem_scal_c_1_4: 0.4043, loss_voxel_geo_scal_c_1_4: 0.2264, loss_voxel_lovasz_c_1_4: 0.1422, loss_voxel_ce_c_1_8: 0.1321, loss_voxel_sem_scal_c_1_8: 0.1864, loss_voxel_geo_scal_c_1_8: 0.0826, loss_voxel_lovasz_c_1_8: 0.0636, loss_voxel_ce_c_0: 0.4713, loss_voxel_sem_scal_c_0: 2.1578, loss_voxel_geo_scal_c_0: 1.5399, loss_voxel_lovasz_c_0: 0.6807, loss: 9.0518, grad_norm: 7.5829
2024-05-07 17:06:45,618 - mmdet - INFO - Epoch [11][350/440]	lr: 1.000e-04, eta: 7:21:19, time: 5.020, data_time: 0.234, memory: 17157, loss_depth: 0.1551, render_dep_loss_c_1_2: 0.3620, loss_voxel_ce_c_1_2: 0.3819, loss_voxel_sem_scal_c_1_2: 0.9065, loss_voxel_geo_scal_c_1_2: 0.6133, loss_voxel_lovasz_c_1_2: 0.3097, loss_voxel_ce_c_1_4: 0.2332, loss_voxel_sem_scal_c_1_4: 0.4046, loss_voxel_geo_scal_c_1_4: 0.2269, loss_voxel_lovasz_c_1_4: 0.1421, loss_voxel_ce_c_1_8: 0.1328, loss_voxel_sem_scal_c_1_8: 0.1905, loss_voxel_geo_scal_c_1_8: 0.0836, loss_voxel_lovasz_c_1_8: 0.0638, loss_voxel_ce_c_0: 0.4780, loss_voxel_sem_scal_c_0: 2.1383, loss_voxel_geo_scal_c_0: 1.5488, loss_voxel_lovasz_c_0: 0.6784, loss: 9.0495, grad_norm: 7.2442
2024-05-07 17:10:55,874 - mmdet - INFO - Epoch [11][400/440]	lr: 1.000e-04, eta: 7:17:58, time: 5.005, data_time: 0.242, memory: 17157, loss_depth: 0.1548, render_dep_loss_c_1_2: 0.3630, loss_voxel_ce_c_1_2: 0.3725, loss_voxel_sem_scal_c_1_2: 0.9059, loss_voxel_geo_scal_c_1_2: 0.6027, loss_voxel_lovasz_c_1_2: 0.3092, loss_voxel_ce_c_1_4: 0.2272, loss_voxel_sem_scal_c_1_4: 0.4049, loss_voxel_geo_scal_c_1_4: 0.2231, loss_voxel_lovasz_c_1_4: 0.1421, loss_voxel_ce_c_1_8: 0.1289, loss_voxel_sem_scal_c_1_8: 0.1844, loss_voxel_geo_scal_c_1_8: 0.0825, loss_voxel_lovasz_c_1_8: 0.0632, loss_voxel_ce_c_0: 0.4634, loss_voxel_sem_scal_c_0: 2.1386, loss_voxel_geo_scal_c_0: 1.5223, loss_voxel_lovasz_c_0: 0.6782, loss: 8.9670, grad_norm: 7.6542
2024-05-07 17:14:15,573 - mmdet - INFO - Saving checkpoint at 11 epochs
2024-05-07 17:14:16,862 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_11_ema.pth
2024-05-07 17:18:36,401 - mmdet - INFO - Epoch [12][50/440]	lr: 1.000e-04, eta: 7:08:12, time: 5.189, data_time: 0.446, memory: 17157, loss_depth: 0.1550, render_dep_loss_c_1_2: 0.3634, loss_voxel_ce_c_1_2: 0.3767, loss_voxel_sem_scal_c_1_2: 0.9129, loss_voxel_geo_scal_c_1_2: 0.6047, loss_voxel_lovasz_c_1_2: 0.3087, loss_voxel_ce_c_1_4: 0.2302, loss_voxel_sem_scal_c_1_4: 0.4048, loss_voxel_geo_scal_c_1_4: 0.2243, loss_voxel_lovasz_c_1_4: 0.1412, loss_voxel_ce_c_1_8: 0.1309, loss_voxel_sem_scal_c_1_8: 0.1899, loss_voxel_geo_scal_c_1_8: 0.0824, loss_voxel_lovasz_c_1_8: 0.0634, loss_voxel_ce_c_0: 0.4702, loss_voxel_sem_scal_c_0: 2.1469, loss_voxel_geo_scal_c_0: 1.5217, loss_voxel_lovasz_c_0: 0.6765, loss: 9.0036, grad_norm: 7.4629
2024-05-07 17:22:44,835 - mmdet - INFO - Epoch [12][100/440]	lr: 1.000e-04, eta: 7:04:51, time: 4.968, data_time: 0.236, memory: 17157, loss_depth: 0.1552, render_dep_loss_c_1_2: 0.3635, loss_voxel_ce_c_1_2: 0.3755, loss_voxel_sem_scal_c_1_2: 0.9154, loss_voxel_geo_scal_c_1_2: 0.6085, loss_voxel_lovasz_c_1_2: 0.3082, loss_voxel_ce_c_1_4: 0.2305, loss_voxel_sem_scal_c_1_4: 0.3996, loss_voxel_geo_scal_c_1_4: 0.2258, loss_voxel_lovasz_c_1_4: 0.1411, loss_voxel_ce_c_1_8: 0.1309, loss_voxel_sem_scal_c_1_8: 0.1854, loss_voxel_geo_scal_c_1_8: 0.0831, loss_voxel_lovasz_c_1_8: 0.0630, loss_voxel_ce_c_0: 0.4696, loss_voxel_sem_scal_c_0: 2.1518, loss_voxel_geo_scal_c_0: 1.5323, loss_voxel_lovasz_c_0: 0.6771, loss: 9.0165, grad_norm: 7.3744
2024-05-07 17:26:52,937 - mmdet - INFO - Epoch [12][150/440]	lr: 1.000e-04, eta: 7:01:28, time: 4.962, data_time: 0.231, memory: 17157, loss_depth: 0.1546, render_dep_loss_c_1_2: 0.3625, loss_voxel_ce_c_1_2: 0.3743, loss_voxel_sem_scal_c_1_2: 0.8830, loss_voxel_geo_scal_c_1_2: 0.5985, loss_voxel_lovasz_c_1_2: 0.3071, loss_voxel_ce_c_1_4: 0.2295, loss_voxel_sem_scal_c_1_4: 0.3950, loss_voxel_geo_scal_c_1_4: 0.2217, loss_voxel_lovasz_c_1_4: 0.1411, loss_voxel_ce_c_1_8: 0.1311, loss_voxel_sem_scal_c_1_8: 0.1826, loss_voxel_geo_scal_c_1_8: 0.0815, loss_voxel_lovasz_c_1_8: 0.0630, loss_voxel_ce_c_0: 0.4672, loss_voxel_sem_scal_c_0: 2.1009, loss_voxel_geo_scal_c_0: 1.5099, loss_voxel_lovasz_c_0: 0.6745, loss: 8.8780, grad_norm: 7.1520
2024-05-07 17:31:00,579 - mmdet - INFO - Epoch [12][200/440]	lr: 1.000e-04, eta: 6:58:03, time: 4.953, data_time: 0.233, memory: 17157, loss_depth: 0.1541, render_dep_loss_c_1_2: 0.3603, loss_voxel_ce_c_1_2: 0.3691, loss_voxel_sem_scal_c_1_2: 0.8830, loss_voxel_geo_scal_c_1_2: 0.6001, loss_voxel_lovasz_c_1_2: 0.3074, loss_voxel_ce_c_1_4: 0.2249, loss_voxel_sem_scal_c_1_4: 0.3938, loss_voxel_geo_scal_c_1_4: 0.2217, loss_voxel_lovasz_c_1_4: 0.1406, loss_voxel_ce_c_1_8: 0.1279, loss_voxel_sem_scal_c_1_8: 0.1802, loss_voxel_geo_scal_c_1_8: 0.0810, loss_voxel_lovasz_c_1_8: 0.0628, loss_voxel_ce_c_0: 0.4600, loss_voxel_sem_scal_c_0: 2.0976, loss_voxel_geo_scal_c_0: 1.5146, loss_voxel_lovasz_c_0: 0.6761, loss: 8.8552, grad_norm: 7.6371
2024-05-07 17:35:09,622 - mmdet - INFO - Epoch [12][250/440]	lr: 1.000e-04, eta: 6:54:40, time: 4.981, data_time: 0.231, memory: 17157, loss_depth: 0.1542, render_dep_loss_c_1_2: 0.3625, loss_voxel_ce_c_1_2: 0.3743, loss_voxel_sem_scal_c_1_2: 0.8831, loss_voxel_geo_scal_c_1_2: 0.6017, loss_voxel_lovasz_c_1_2: 0.3057, loss_voxel_ce_c_1_4: 0.2289, loss_voxel_sem_scal_c_1_4: 0.3908, loss_voxel_geo_scal_c_1_4: 0.2223, loss_voxel_lovasz_c_1_4: 0.1400, loss_voxel_ce_c_1_8: 0.1305, loss_voxel_sem_scal_c_1_8: 0.1769, loss_voxel_geo_scal_c_1_8: 0.0817, loss_voxel_lovasz_c_1_8: 0.0624, loss_voxel_ce_c_0: 0.4664, loss_voxel_sem_scal_c_0: 2.0944, loss_voxel_geo_scal_c_0: 1.5133, loss_voxel_lovasz_c_0: 0.6729, loss: 8.8620, grad_norm: 7.1153
2024-05-07 17:39:17,228 - mmdet - INFO - Epoch [12][300/440]	lr: 1.000e-04, eta: 6:51:13, time: 4.952, data_time: 0.210, memory: 17157, loss_depth: 0.1543, render_dep_loss_c_1_2: 0.3638, loss_voxel_ce_c_1_2: 0.3706, loss_voxel_sem_scal_c_1_2: 0.9135, loss_voxel_geo_scal_c_1_2: 0.6025, loss_voxel_lovasz_c_1_2: 0.3078, loss_voxel_ce_c_1_4: 0.2258, loss_voxel_sem_scal_c_1_4: 0.4007, loss_voxel_geo_scal_c_1_4: 0.2220, loss_voxel_lovasz_c_1_4: 0.1410, loss_voxel_ce_c_1_8: 0.1277, loss_voxel_sem_scal_c_1_8: 0.1821, loss_voxel_geo_scal_c_1_8: 0.0815, loss_voxel_lovasz_c_1_8: 0.0626, loss_voxel_ce_c_0: 0.4610, loss_voxel_sem_scal_c_0: 2.1634, loss_voxel_geo_scal_c_0: 1.5171, loss_voxel_lovasz_c_0: 0.6763, loss: 8.9737, grad_norm: 7.4768
2024-05-07 17:43:23,978 - mmdet - INFO - Epoch [12][350/440]	lr: 1.000e-04, eta: 6:47:46, time: 4.935, data_time: 0.209, memory: 17157, loss_depth: 0.1541, render_dep_loss_c_1_2: 0.3594, loss_voxel_ce_c_1_2: 0.3734, loss_voxel_sem_scal_c_1_2: 0.8968, loss_voxel_geo_scal_c_1_2: 0.6034, loss_voxel_lovasz_c_1_2: 0.3081, loss_voxel_ce_c_1_4: 0.2287, loss_voxel_sem_scal_c_1_4: 0.3974, loss_voxel_geo_scal_c_1_4: 0.2223, loss_voxel_lovasz_c_1_4: 0.1413, loss_voxel_ce_c_1_8: 0.1300, loss_voxel_sem_scal_c_1_8: 0.1833, loss_voxel_geo_scal_c_1_8: 0.0813, loss_voxel_lovasz_c_1_8: 0.0629, loss_voxel_ce_c_0: 0.4697, loss_voxel_sem_scal_c_0: 2.1219, loss_voxel_geo_scal_c_0: 1.5291, loss_voxel_lovasz_c_0: 0.6764, loss: 8.9395, grad_norm: 7.4498
2024-05-07 17:47:30,687 - mmdet - INFO - Epoch [12][400/440]	lr: 1.000e-04, eta: 6:44:17, time: 4.934, data_time: 0.206, memory: 17157, loss_depth: 0.1544, render_dep_loss_c_1_2: 0.3631, loss_voxel_ce_c_1_2: 0.3715, loss_voxel_sem_scal_c_1_2: 0.8903, loss_voxel_geo_scal_c_1_2: 0.6031, loss_voxel_lovasz_c_1_2: 0.3070, loss_voxel_ce_c_1_4: 0.2266, loss_voxel_sem_scal_c_1_4: 0.3991, loss_voxel_geo_scal_c_1_4: 0.2231, loss_voxel_lovasz_c_1_4: 0.1411, loss_voxel_ce_c_1_8: 0.1293, loss_voxel_sem_scal_c_1_8: 0.1833, loss_voxel_geo_scal_c_1_8: 0.0818, loss_voxel_lovasz_c_1_8: 0.0628, loss_voxel_ce_c_0: 0.4627, loss_voxel_sem_scal_c_0: 2.1026, loss_voxel_geo_scal_c_0: 1.5226, loss_voxel_lovasz_c_0: 0.6748, loss: 8.8995, grad_norm: 7.2550
2024-05-07 17:50:48,067 - mmdet - INFO - Saving checkpoint at 12 epochs
2024-05-07 17:50:49,351 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_12_ema.pth
2024-05-07 17:55:07,811 - mmdet - INFO - Epoch [13][50/440]	lr: 1.000e-04, eta: 6:34:57, time: 5.167, data_time: 0.387, memory: 17157, loss_depth: 0.1537, render_dep_loss_c_1_2: 0.3625, loss_voxel_ce_c_1_2: 0.3690, loss_voxel_sem_scal_c_1_2: 0.8829, loss_voxel_geo_scal_c_1_2: 0.5976, loss_voxel_lovasz_c_1_2: 0.3060, loss_voxel_ce_c_1_4: 0.2254, loss_voxel_sem_scal_c_1_4: 0.3891, loss_voxel_geo_scal_c_1_4: 0.2218, loss_voxel_lovasz_c_1_4: 0.1402, loss_voxel_ce_c_1_8: 0.1277, loss_voxel_sem_scal_c_1_8: 0.1795, loss_voxel_geo_scal_c_1_8: 0.0814, loss_voxel_lovasz_c_1_8: 0.0623, loss_voxel_ce_c_0: 0.4609, loss_voxel_sem_scal_c_0: 2.0939, loss_voxel_geo_scal_c_0: 1.5059, loss_voxel_lovasz_c_0: 0.6730, loss: 8.8326, grad_norm: 6.6707
2024-05-07 17:59:16,428 - mmdet - INFO - Epoch [13][100/440]	lr: 1.000e-04, eta: 6:31:32, time: 4.972, data_time: 0.193, memory: 17157, loss_depth: 0.1543, render_dep_loss_c_1_2: 0.3602, loss_voxel_ce_c_1_2: 0.3734, loss_voxel_sem_scal_c_1_2: 0.9061, loss_voxel_geo_scal_c_1_2: 0.6028, loss_voxel_lovasz_c_1_2: 0.3069, loss_voxel_ce_c_1_4: 0.2277, loss_voxel_sem_scal_c_1_4: 0.4011, loss_voxel_geo_scal_c_1_4: 0.2229, loss_voxel_lovasz_c_1_4: 0.1407, loss_voxel_ce_c_1_8: 0.1288, loss_voxel_sem_scal_c_1_8: 0.1802, loss_voxel_geo_scal_c_1_8: 0.0816, loss_voxel_lovasz_c_1_8: 0.0626, loss_voxel_ce_c_0: 0.4668, loss_voxel_sem_scal_c_0: 2.1272, loss_voxel_geo_scal_c_0: 1.5154, loss_voxel_lovasz_c_0: 0.6751, loss: 8.9338, grad_norm: 7.3849
2024-05-07 18:03:23,722 - mmdet - INFO - Epoch [13][150/440]	lr: 1.000e-04, eta: 6:28:05, time: 4.946, data_time: 0.193, memory: 17157, loss_depth: 0.1530, render_dep_loss_c_1_2: 0.3611, loss_voxel_ce_c_1_2: 0.3705, loss_voxel_sem_scal_c_1_2: 0.8607, loss_voxel_geo_scal_c_1_2: 0.5941, loss_voxel_lovasz_c_1_2: 0.3043, loss_voxel_ce_c_1_4: 0.2259, loss_voxel_sem_scal_c_1_4: 0.3914, loss_voxel_geo_scal_c_1_4: 0.2191, loss_voxel_lovasz_c_1_4: 0.1398, loss_voxel_ce_c_1_8: 0.1281, loss_voxel_sem_scal_c_1_8: 0.1825, loss_voxel_geo_scal_c_1_8: 0.0799, loss_voxel_lovasz_c_1_8: 0.0624, loss_voxel_ce_c_0: 0.4638, loss_voxel_sem_scal_c_0: 2.0532, loss_voxel_geo_scal_c_0: 1.5037, loss_voxel_lovasz_c_0: 0.6707, loss: 8.7642, grad_norm: 7.0947
2024-05-07 18:07:31,738 - mmdet - INFO - Epoch [13][200/440]	lr: 1.000e-04, eta: 6:24:37, time: 4.960, data_time: 0.192, memory: 17157, loss_depth: 0.1527, render_dep_loss_c_1_2: 0.3624, loss_voxel_ce_c_1_2: 0.3687, loss_voxel_sem_scal_c_1_2: 0.8601, loss_voxel_geo_scal_c_1_2: 0.5991, loss_voxel_lovasz_c_1_2: 0.3040, loss_voxel_ce_c_1_4: 0.2251, loss_voxel_sem_scal_c_1_4: 0.3813, loss_voxel_geo_scal_c_1_4: 0.2212, loss_voxel_lovasz_c_1_4: 0.1390, loss_voxel_ce_c_1_8: 0.1278, loss_voxel_sem_scal_c_1_8: 0.1819, loss_voxel_geo_scal_c_1_8: 0.0814, loss_voxel_lovasz_c_1_8: 0.0624, loss_voxel_ce_c_0: 0.4628, loss_voxel_sem_scal_c_0: 2.0436, loss_voxel_geo_scal_c_0: 1.5107, loss_voxel_lovasz_c_0: 0.6685, loss: 8.7527, grad_norm: 6.8167
2024-05-07 18:11:39,192 - mmdet - INFO - Epoch [13][250/440]	lr: 1.000e-04, eta: 6:21:09, time: 4.949, data_time: 0.193, memory: 17157, loss_depth: 0.1532, render_dep_loss_c_1_2: 0.3644, loss_voxel_ce_c_1_2: 0.3705, loss_voxel_sem_scal_c_1_2: 0.8660, loss_voxel_geo_scal_c_1_2: 0.5940, loss_voxel_lovasz_c_1_2: 0.3039, loss_voxel_ce_c_1_4: 0.2251, loss_voxel_sem_scal_c_1_4: 0.3827, loss_voxel_geo_scal_c_1_4: 0.2187, loss_voxel_lovasz_c_1_4: 0.1391, loss_voxel_ce_c_1_8: 0.1273, loss_voxel_sem_scal_c_1_8: 0.1773, loss_voxel_geo_scal_c_1_8: 0.0796, loss_voxel_lovasz_c_1_8: 0.0621, loss_voxel_ce_c_0: 0.4614, loss_voxel_sem_scal_c_0: 2.0544, loss_voxel_geo_scal_c_0: 1.5021, loss_voxel_lovasz_c_0: 0.6693, loss: 8.7510, grad_norm: 7.0136
2024-05-07 18:15:47,138 - mmdet - INFO - Epoch [13][300/440]	lr: 1.000e-04, eta: 6:17:40, time: 4.959, data_time: 0.191, memory: 17157, loss_depth: 0.1529, render_dep_loss_c_1_2: 0.3579, loss_voxel_ce_c_1_2: 0.3690, loss_voxel_sem_scal_c_1_2: 0.8624, loss_voxel_geo_scal_c_1_2: 0.5920, loss_voxel_lovasz_c_1_2: 0.3039, loss_voxel_ce_c_1_4: 0.2249, loss_voxel_sem_scal_c_1_4: 0.3867, loss_voxel_geo_scal_c_1_4: 0.2186, loss_voxel_lovasz_c_1_4: 0.1394, loss_voxel_ce_c_1_8: 0.1270, loss_voxel_sem_scal_c_1_8: 0.1757, loss_voxel_geo_scal_c_1_8: 0.0802, loss_voxel_lovasz_c_1_8: 0.0621, loss_voxel_ce_c_0: 0.4627, loss_voxel_sem_scal_c_0: 2.0484, loss_voxel_geo_scal_c_0: 1.4978, loss_voxel_lovasz_c_0: 0.6694, loss: 8.7311, grad_norm: 6.9908
2024-05-07 18:19:55,438 - mmdet - INFO - Epoch [13][350/440]	lr: 1.000e-04, eta: 6:14:10, time: 4.966, data_time: 0.188, memory: 17157, loss_depth: 0.1532, render_dep_loss_c_1_2: 0.3602, loss_voxel_ce_c_1_2: 0.3675, loss_voxel_sem_scal_c_1_2: 0.8870, loss_voxel_geo_scal_c_1_2: 0.5971, loss_voxel_lovasz_c_1_2: 0.3034, loss_voxel_ce_c_1_4: 0.2249, loss_voxel_sem_scal_c_1_4: 0.3881, loss_voxel_geo_scal_c_1_4: 0.2206, loss_voxel_lovasz_c_1_4: 0.1393, loss_voxel_ce_c_1_8: 0.1274, loss_voxel_sem_scal_c_1_8: 0.1757, loss_voxel_geo_scal_c_1_8: 0.0809, loss_voxel_lovasz_c_1_8: 0.0620, loss_voxel_ce_c_0: 0.4590, loss_voxel_sem_scal_c_0: 2.0942, loss_voxel_geo_scal_c_0: 1.5000, loss_voxel_lovasz_c_0: 0.6680, loss: 8.8084, grad_norm: 7.8159
2024-05-07 18:24:02,970 - mmdet - INFO - Epoch [13][400/440]	lr: 1.000e-04, eta: 6:10:40, time: 4.951, data_time: 0.171, memory: 17157, loss_depth: 0.1524, render_dep_loss_c_1_2: 0.3611, loss_voxel_ce_c_1_2: 0.3691, loss_voxel_sem_scal_c_1_2: 0.8719, loss_voxel_geo_scal_c_1_2: 0.5967, loss_voxel_lovasz_c_1_2: 0.3048, loss_voxel_ce_c_1_4: 0.2264, loss_voxel_sem_scal_c_1_4: 0.3895, loss_voxel_geo_scal_c_1_4: 0.2204, loss_voxel_lovasz_c_1_4: 0.1397, loss_voxel_ce_c_1_8: 0.1283, loss_voxel_sem_scal_c_1_8: 0.1767, loss_voxel_geo_scal_c_1_8: 0.0806, loss_voxel_lovasz_c_1_8: 0.0624, loss_voxel_ce_c_0: 0.4606, loss_voxel_sem_scal_c_0: 2.0703, loss_voxel_geo_scal_c_0: 1.5031, loss_voxel_lovasz_c_0: 0.6711, loss: 8.7853, grad_norm: 7.0913
2024-05-07 18:27:20,451 - mmdet - INFO - Saving checkpoint at 13 epochs
2024-05-07 18:27:21,730 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_13_ema.pth
2024-05-07 18:31:39,596 - mmdet - INFO - Epoch [14][50/440]	lr: 1.000e-04, eta: 6:01:43, time: 5.156, data_time: 0.393, memory: 17157, loss_depth: 0.1531, render_dep_loss_c_1_2: 0.3597, loss_voxel_ce_c_1_2: 0.3713, loss_voxel_sem_scal_c_1_2: 0.8478, loss_voxel_geo_scal_c_1_2: 0.6003, loss_voxel_lovasz_c_1_2: 0.3027, loss_voxel_ce_c_1_4: 0.2261, loss_voxel_sem_scal_c_1_4: 0.3703, loss_voxel_geo_scal_c_1_4: 0.2212, loss_voxel_lovasz_c_1_4: 0.1382, loss_voxel_ce_c_1_8: 0.1281, loss_voxel_sem_scal_c_1_8: 0.1774, loss_voxel_geo_scal_c_1_8: 0.0812, loss_voxel_lovasz_c_1_8: 0.0618, loss_voxel_ce_c_0: 0.4672, loss_voxel_sem_scal_c_0: 2.0145, loss_voxel_geo_scal_c_0: 1.5095, loss_voxel_lovasz_c_0: 0.6678, loss: 8.6982, grad_norm: 7.2903
2024-05-07 18:35:46,532 - mmdet - INFO - Epoch [14][100/440]	lr: 1.000e-04, eta: 5:58:13, time: 4.939, data_time: 0.198, memory: 17157, loss_depth: 0.1534, render_dep_loss_c_1_2: 0.3590, loss_voxel_ce_c_1_2: 0.3675, loss_voxel_sem_scal_c_1_2: 0.8550, loss_voxel_geo_scal_c_1_2: 0.5920, loss_voxel_lovasz_c_1_2: 0.3034, loss_voxel_ce_c_1_4: 0.2240, loss_voxel_sem_scal_c_1_4: 0.3887, loss_voxel_geo_scal_c_1_4: 0.2182, loss_voxel_lovasz_c_1_4: 0.1394, loss_voxel_ce_c_1_8: 0.1267, loss_voxel_sem_scal_c_1_8: 0.1778, loss_voxel_geo_scal_c_1_8: 0.0799, loss_voxel_lovasz_c_1_8: 0.0619, loss_voxel_ce_c_0: 0.4600, loss_voxel_sem_scal_c_0: 2.0341, loss_voxel_geo_scal_c_0: 1.4899, loss_voxel_lovasz_c_0: 0.6680, loss: 8.6989, grad_norm: 6.9606
2024-05-07 18:39:54,143 - mmdet - INFO - Epoch [14][150/440]	lr: 1.000e-04, eta: 5:54:43, time: 4.952, data_time: 0.202, memory: 17157, loss_depth: 0.1527, render_dep_loss_c_1_2: 0.3599, loss_voxel_ce_c_1_2: 0.3659, loss_voxel_sem_scal_c_1_2: 0.8577, loss_voxel_geo_scal_c_1_2: 0.5925, loss_voxel_lovasz_c_1_2: 0.3039, loss_voxel_ce_c_1_4: 0.2239, loss_voxel_sem_scal_c_1_4: 0.3835, loss_voxel_geo_scal_c_1_4: 0.2186, loss_voxel_lovasz_c_1_4: 0.1394, loss_voxel_ce_c_1_8: 0.1259, loss_voxel_sem_scal_c_1_8: 0.1767, loss_voxel_geo_scal_c_1_8: 0.0796, loss_voxel_lovasz_c_1_8: 0.0618, loss_voxel_ce_c_0: 0.4590, loss_voxel_sem_scal_c_0: 2.0626, loss_voxel_geo_scal_c_0: 1.4985, loss_voxel_lovasz_c_0: 0.6710, loss: 8.7332, grad_norm: 6.8716
2024-05-07 18:44:01,805 - mmdet - INFO - Epoch [14][200/440]	lr: 1.000e-04, eta: 5:51:13, time: 4.953, data_time: 0.202, memory: 17157, loss_depth: 0.1518, render_dep_loss_c_1_2: 0.3631, loss_voxel_ce_c_1_2: 0.3648, loss_voxel_sem_scal_c_1_2: 0.8531, loss_voxel_geo_scal_c_1_2: 0.5933, loss_voxel_lovasz_c_1_2: 0.3023, loss_voxel_ce_c_1_4: 0.2235, loss_voxel_sem_scal_c_1_4: 0.3771, loss_voxel_geo_scal_c_1_4: 0.2191, loss_voxel_lovasz_c_1_4: 0.1386, loss_voxel_ce_c_1_8: 0.1267, loss_voxel_sem_scal_c_1_8: 0.1742, loss_voxel_geo_scal_c_1_8: 0.0800, loss_voxel_lovasz_c_1_8: 0.0616, loss_voxel_ce_c_0: 0.4578, loss_voxel_sem_scal_c_0: 2.0353, loss_voxel_geo_scal_c_0: 1.4936, loss_voxel_lovasz_c_0: 0.6673, loss: 8.6832, grad_norm: 6.9441
2024-05-07 18:48:09,129 - mmdet - INFO - Epoch [14][250/440]	lr: 1.000e-04, eta: 5:47:41, time: 4.946, data_time: 0.202, memory: 17157, loss_depth: 0.1523, render_dep_loss_c_1_2: 0.3617, loss_voxel_ce_c_1_2: 0.3660, loss_voxel_sem_scal_c_1_2: 0.8596, loss_voxel_geo_scal_c_1_2: 0.5972, loss_voxel_lovasz_c_1_2: 0.3027, loss_voxel_ce_c_1_4: 0.2229, loss_voxel_sem_scal_c_1_4: 0.3887, loss_voxel_geo_scal_c_1_4: 0.2205, loss_voxel_lovasz_c_1_4: 0.1390, loss_voxel_ce_c_1_8: 0.1266, loss_voxel_sem_scal_c_1_8: 0.1754, loss_voxel_geo_scal_c_1_8: 0.0811, loss_voxel_lovasz_c_1_8: 0.0619, loss_voxel_ce_c_0: 0.4562, loss_voxel_sem_scal_c_0: 2.0336, loss_voxel_geo_scal_c_0: 1.5023, loss_voxel_lovasz_c_0: 0.6668, loss: 8.7145, grad_norm: 7.1565
2024-05-07 18:52:16,637 - mmdet - INFO - Epoch [14][300/440]	lr: 1.000e-04, eta: 5:44:09, time: 4.950, data_time: 0.203, memory: 17157, loss_depth: 0.1516, render_dep_loss_c_1_2: 0.3590, loss_voxel_ce_c_1_2: 0.3609, loss_voxel_sem_scal_c_1_2: 0.8622, loss_voxel_geo_scal_c_1_2: 0.5862, loss_voxel_lovasz_c_1_2: 0.3022, loss_voxel_ce_c_1_4: 0.2211, loss_voxel_sem_scal_c_1_4: 0.3788, loss_voxel_geo_scal_c_1_4: 0.2170, loss_voxel_lovasz_c_1_4: 0.1380, loss_voxel_ce_c_1_8: 0.1260, loss_voxel_sem_scal_c_1_8: 0.1775, loss_voxel_geo_scal_c_1_8: 0.0794, loss_voxel_lovasz_c_1_8: 0.0616, loss_voxel_ce_c_0: 0.4538, loss_voxel_sem_scal_c_0: 2.0378, loss_voxel_geo_scal_c_0: 1.4777, loss_voxel_lovasz_c_0: 0.6661, loss: 8.6569, grad_norm: 6.6789
2024-05-07 18:56:24,591 - mmdet - INFO - Epoch [14][350/440]	lr: 1.000e-04, eta: 5:40:37, time: 4.959, data_time: 0.206, memory: 17157, loss_depth: 0.1518, render_dep_loss_c_1_2: 0.3545, loss_voxel_ce_c_1_2: 0.3650, loss_voxel_sem_scal_c_1_2: 0.8558, loss_voxel_geo_scal_c_1_2: 0.5885, loss_voxel_lovasz_c_1_2: 0.3013, loss_voxel_ce_c_1_4: 0.2221, loss_voxel_sem_scal_c_1_4: 0.3814, loss_voxel_geo_scal_c_1_4: 0.2177, loss_voxel_lovasz_c_1_4: 0.1382, loss_voxel_ce_c_1_8: 0.1252, loss_voxel_sem_scal_c_1_8: 0.1751, loss_voxel_geo_scal_c_1_8: 0.0795, loss_voxel_lovasz_c_1_8: 0.0616, loss_voxel_ce_c_0: 0.4561, loss_voxel_sem_scal_c_0: 2.0330, loss_voxel_geo_scal_c_0: 1.4812, loss_voxel_lovasz_c_0: 0.6646, loss: 8.6526, grad_norm: 7.2394
2024-05-07 19:00:32,283 - mmdet - INFO - Epoch [14][400/440]	lr: 1.000e-04, eta: 5:37:04, time: 4.954, data_time: 0.205, memory: 17157, loss_depth: 0.1530, render_dep_loss_c_1_2: 0.3636, loss_voxel_ce_c_1_2: 0.3659, loss_voxel_sem_scal_c_1_2: 0.8562, loss_voxel_geo_scal_c_1_2: 0.5920, loss_voxel_lovasz_c_1_2: 0.3027, loss_voxel_ce_c_1_4: 0.2225, loss_voxel_sem_scal_c_1_4: 0.3769, loss_voxel_geo_scal_c_1_4: 0.2186, loss_voxel_lovasz_c_1_4: 0.1379, loss_voxel_ce_c_1_8: 0.1249, loss_voxel_sem_scal_c_1_8: 0.1746, loss_voxel_geo_scal_c_1_8: 0.0802, loss_voxel_lovasz_c_1_8: 0.0613, loss_voxel_ce_c_0: 0.4607, loss_voxel_sem_scal_c_0: 2.0357, loss_voxel_geo_scal_c_0: 1.4963, loss_voxel_lovasz_c_0: 0.6682, loss: 8.6909, grad_norm: 7.6213
2024-05-07 19:03:50,020 - mmdet - INFO - Saving checkpoint at 14 epochs
2024-05-07 19:03:51,645 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_14_ema.pth
2024-05-07 19:08:07,227 - mmdet - INFO - Epoch [15][50/440]	lr: 1.000e-04, eta: 5:28:26, time: 5.110, data_time: 0.390, memory: 17157, loss_depth: 0.1522, render_dep_loss_c_1_2: 0.3586, loss_voxel_ce_c_1_2: 0.3654, loss_voxel_sem_scal_c_1_2: 0.8419, loss_voxel_geo_scal_c_1_2: 0.5850, loss_voxel_lovasz_c_1_2: 0.3013, loss_voxel_ce_c_1_4: 0.2220, loss_voxel_sem_scal_c_1_4: 0.3705, loss_voxel_geo_scal_c_1_4: 0.2160, loss_voxel_lovasz_c_1_4: 0.1374, loss_voxel_ce_c_1_8: 0.1256, loss_voxel_sem_scal_c_1_8: 0.1713, loss_voxel_geo_scal_c_1_8: 0.0787, loss_voxel_lovasz_c_1_8: 0.0612, loss_voxel_ce_c_0: 0.4574, loss_voxel_sem_scal_c_0: 1.9822, loss_voxel_geo_scal_c_0: 1.4733, loss_voxel_lovasz_c_0: 0.6631, loss: 8.5629, grad_norm: 6.8678
2024-05-07 19:12:14,057 - mmdet - INFO - Epoch [15][100/440]	lr: 1.000e-04, eta: 5:24:53, time: 4.937, data_time: 0.211, memory: 17157, loss_depth: 0.1519, render_dep_loss_c_1_2: 0.3568, loss_voxel_ce_c_1_2: 0.3616, loss_voxel_sem_scal_c_1_2: 0.8414, loss_voxel_geo_scal_c_1_2: 0.5840, loss_voxel_lovasz_c_1_2: 0.3001, loss_voxel_ce_c_1_4: 0.2193, loss_voxel_sem_scal_c_1_4: 0.3800, loss_voxel_geo_scal_c_1_4: 0.2148, loss_voxel_lovasz_c_1_4: 0.1375, loss_voxel_ce_c_1_8: 0.1239, loss_voxel_sem_scal_c_1_8: 0.1755, loss_voxel_geo_scal_c_1_8: 0.0785, loss_voxel_lovasz_c_1_8: 0.0613, loss_voxel_ce_c_0: 0.4546, loss_voxel_sem_scal_c_0: 2.0074, loss_voxel_geo_scal_c_0: 1.4749, loss_voxel_lovasz_c_0: 0.6632, loss: 8.5866, grad_norm: 6.7473
2024-05-07 19:16:21,381 - mmdet - INFO - Epoch [15][150/440]	lr: 1.000e-04, eta: 5:21:20, time: 4.946, data_time: 0.220, memory: 17157, loss_depth: 0.1522, render_dep_loss_c_1_2: 0.3586, loss_voxel_ce_c_1_2: 0.3657, loss_voxel_sem_scal_c_1_2: 0.8482, loss_voxel_geo_scal_c_1_2: 0.5851, loss_voxel_lovasz_c_1_2: 0.3014, loss_voxel_ce_c_1_4: 0.2227, loss_voxel_sem_scal_c_1_4: 0.3768, loss_voxel_geo_scal_c_1_4: 0.2159, loss_voxel_lovasz_c_1_4: 0.1381, loss_voxel_ce_c_1_8: 0.1252, loss_voxel_sem_scal_c_1_8: 0.1729, loss_voxel_geo_scal_c_1_8: 0.0786, loss_voxel_lovasz_c_1_8: 0.0611, loss_voxel_ce_c_0: 0.4585, loss_voxel_sem_scal_c_0: 2.0264, loss_voxel_geo_scal_c_0: 1.4830, loss_voxel_lovasz_c_0: 0.6655, loss: 8.6358, grad_norm: 7.0306
2024-05-07 19:20:28,000 - mmdet - INFO - Epoch [15][200/440]	lr: 1.000e-04, eta: 5:17:47, time: 4.932, data_time: 0.218, memory: 17157, loss_depth: 0.1517, render_dep_loss_c_1_2: 0.3562, loss_voxel_ce_c_1_2: 0.3639, loss_voxel_sem_scal_c_1_2: 0.8526, loss_voxel_geo_scal_c_1_2: 0.5872, loss_voxel_lovasz_c_1_2: 0.3026, loss_voxel_ce_c_1_4: 0.2218, loss_voxel_sem_scal_c_1_4: 0.3799, loss_voxel_geo_scal_c_1_4: 0.2155, loss_voxel_lovasz_c_1_4: 0.1387, loss_voxel_ce_c_1_8: 0.1257, loss_voxel_sem_scal_c_1_8: 0.1741, loss_voxel_geo_scal_c_1_8: 0.0784, loss_voxel_lovasz_c_1_8: 0.0615, loss_voxel_ce_c_0: 0.4546, loss_voxel_sem_scal_c_0: 2.0290, loss_voxel_geo_scal_c_0: 1.4788, loss_voxel_lovasz_c_0: 0.6667, loss: 8.6391, grad_norm: 7.5378
2024-05-07 19:24:35,716 - mmdet - INFO - Epoch [15][250/440]	lr: 1.000e-04, eta: 5:14:13, time: 4.954, data_time: 0.208, memory: 17157, loss_depth: 0.1524, render_dep_loss_c_1_2: 0.3623, loss_voxel_ce_c_1_2: 0.3707, loss_voxel_sem_scal_c_1_2: 0.8614, loss_voxel_geo_scal_c_1_2: 0.5949, loss_voxel_lovasz_c_1_2: 0.3029, loss_voxel_ce_c_1_4: 0.2248, loss_voxel_sem_scal_c_1_4: 0.3899, loss_voxel_geo_scal_c_1_4: 0.2188, loss_voxel_lovasz_c_1_4: 0.1389, loss_voxel_ce_c_1_8: 0.1260, loss_voxel_sem_scal_c_1_8: 0.1741, loss_voxel_geo_scal_c_1_8: 0.0797, loss_voxel_lovasz_c_1_8: 0.0616, loss_voxel_ce_c_0: 0.4663, loss_voxel_sem_scal_c_0: 2.0383, loss_voxel_geo_scal_c_0: 1.4985, loss_voxel_lovasz_c_0: 0.6670, loss: 8.7286, grad_norm: 7.3458
2024-05-07 19:28:42,004 - mmdet - INFO - Epoch [15][300/440]	lr: 1.000e-04, eta: 5:10:38, time: 4.926, data_time: 0.206, memory: 17157, loss_depth: 0.1516, render_dep_loss_c_1_2: 0.3603, loss_voxel_ce_c_1_2: 0.3613, loss_voxel_sem_scal_c_1_2: 0.8536, loss_voxel_geo_scal_c_1_2: 0.5839, loss_voxel_lovasz_c_1_2: 0.3011, loss_voxel_ce_c_1_4: 0.2203, loss_voxel_sem_scal_c_1_4: 0.3792, loss_voxel_geo_scal_c_1_4: 0.2157, loss_voxel_lovasz_c_1_4: 0.1374, loss_voxel_ce_c_1_8: 0.1243, loss_voxel_sem_scal_c_1_8: 0.1773, loss_voxel_geo_scal_c_1_8: 0.0785, loss_voxel_lovasz_c_1_8: 0.0616, loss_voxel_ce_c_0: 0.4527, loss_voxel_sem_scal_c_0: 2.0080, loss_voxel_geo_scal_c_0: 1.4773, loss_voxel_lovasz_c_0: 0.6644, loss: 8.6086, grad_norm: 7.4323
2024-05-07 19:32:48,268 - mmdet - INFO - Epoch [15][350/440]	lr: 1.000e-04, eta: 5:07:02, time: 4.925, data_time: 0.209, memory: 17157, loss_depth: 0.1513, render_dep_loss_c_1_2: 0.3605, loss_voxel_ce_c_1_2: 0.3636, loss_voxel_sem_scal_c_1_2: 0.8289, loss_voxel_geo_scal_c_1_2: 0.5911, loss_voxel_lovasz_c_1_2: 0.3001, loss_voxel_ce_c_1_4: 0.2206, loss_voxel_sem_scal_c_1_4: 0.3745, loss_voxel_geo_scal_c_1_4: 0.2182, loss_voxel_lovasz_c_1_4: 0.1372, loss_voxel_ce_c_1_8: 0.1256, loss_voxel_sem_scal_c_1_8: 0.1678, loss_voxel_geo_scal_c_1_8: 0.0796, loss_voxel_lovasz_c_1_8: 0.0610, loss_voxel_ce_c_0: 0.4582, loss_voxel_sem_scal_c_0: 1.9761, loss_voxel_geo_scal_c_0: 1.4904, loss_voxel_lovasz_c_0: 0.6634, loss: 8.5679, grad_norm: 6.7057
2024-05-07 19:36:55,220 - mmdet - INFO - Epoch [15][400/440]	lr: 1.000e-04, eta: 5:03:27, time: 4.939, data_time: 0.206, memory: 17157, loss_depth: 0.1516, render_dep_loss_c_1_2: 0.3605, loss_voxel_ce_c_1_2: 0.3590, loss_voxel_sem_scal_c_1_2: 0.8461, loss_voxel_geo_scal_c_1_2: 0.5836, loss_voxel_lovasz_c_1_2: 0.2998, loss_voxel_ce_c_1_4: 0.2185, loss_voxel_sem_scal_c_1_4: 0.3769, loss_voxel_geo_scal_c_1_4: 0.2153, loss_voxel_lovasz_c_1_4: 0.1368, loss_voxel_ce_c_1_8: 0.1224, loss_voxel_sem_scal_c_1_8: 0.1695, loss_voxel_geo_scal_c_1_8: 0.0786, loss_voxel_lovasz_c_1_8: 0.0607, loss_voxel_ce_c_0: 0.4507, loss_voxel_sem_scal_c_0: 2.0162, loss_voxel_geo_scal_c_0: 1.4731, loss_voxel_lovasz_c_0: 0.6629, loss: 8.5822, grad_norm: 6.8602
2024-05-07 19:40:11,761 - mmdet - INFO - Saving checkpoint at 15 epochs
2024-05-07 19:40:13,060 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_15_ema.pth
2024-05-07 19:44:30,257 - mmdet - INFO - Epoch [16][50/440]	lr: 1.000e-04, eta: 4:55:08, time: 5.141, data_time: 0.372, memory: 17157, loss_depth: 0.1506, render_dep_loss_c_1_2: 0.3572, loss_voxel_ce_c_1_2: 0.3557, loss_voxel_sem_scal_c_1_2: 0.8105, loss_voxel_geo_scal_c_1_2: 0.5761, loss_voxel_lovasz_c_1_2: 0.2970, loss_voxel_ce_c_1_4: 0.2159, loss_voxel_sem_scal_c_1_4: 0.3589, loss_voxel_geo_scal_c_1_4: 0.2116, loss_voxel_lovasz_c_1_4: 0.1352, loss_voxel_ce_c_1_8: 0.1207, loss_voxel_sem_scal_c_1_8: 0.1649, loss_voxel_geo_scal_c_1_8: 0.0766, loss_voxel_lovasz_c_1_8: 0.0605, loss_voxel_ce_c_0: 0.4457, loss_voxel_sem_scal_c_0: 1.9465, loss_voxel_geo_scal_c_0: 1.4531, loss_voxel_lovasz_c_0: 0.6591, loss: 8.3960, grad_norm: 6.9904
2024-05-07 19:48:36,749 - mmdet - INFO - Epoch [16][100/440]	lr: 1.000e-04, eta: 4:51:33, time: 4.931, data_time: 0.172, memory: 17157, loss_depth: 0.1513, render_dep_loss_c_1_2: 0.3589, loss_voxel_ce_c_1_2: 0.3600, loss_voxel_sem_scal_c_1_2: 0.8307, loss_voxel_geo_scal_c_1_2: 0.5825, loss_voxel_lovasz_c_1_2: 0.3000, loss_voxel_ce_c_1_4: 0.2194, loss_voxel_sem_scal_c_1_4: 0.3669, loss_voxel_geo_scal_c_1_4: 0.2151, loss_voxel_lovasz_c_1_4: 0.1368, loss_voxel_ce_c_1_8: 0.1242, loss_voxel_sem_scal_c_1_8: 0.1690, loss_voxel_geo_scal_c_1_8: 0.0784, loss_voxel_lovasz_c_1_8: 0.0609, loss_voxel_ce_c_0: 0.4531, loss_voxel_sem_scal_c_0: 1.9914, loss_voxel_geo_scal_c_0: 1.4694, loss_voxel_lovasz_c_0: 0.6629, loss: 8.5310, grad_norm: 6.8745
2024-05-07 19:52:42,782 - mmdet - INFO - Epoch [16][150/440]	lr: 1.000e-04, eta: 4:47:57, time: 4.921, data_time: 0.172, memory: 17157, loss_depth: 0.1509, render_dep_loss_c_1_2: 0.3575, loss_voxel_ce_c_1_2: 0.3609, loss_voxel_sem_scal_c_1_2: 0.8471, loss_voxel_geo_scal_c_1_2: 0.5816, loss_voxel_lovasz_c_1_2: 0.3009, loss_voxel_ce_c_1_4: 0.2196, loss_voxel_sem_scal_c_1_4: 0.3723, loss_voxel_geo_scal_c_1_4: 0.2145, loss_voxel_lovasz_c_1_4: 0.1373, loss_voxel_ce_c_1_8: 0.1234, loss_voxel_sem_scal_c_1_8: 0.1707, loss_voxel_geo_scal_c_1_8: 0.0780, loss_voxel_lovasz_c_1_8: 0.0607, loss_voxel_ce_c_0: 0.4526, loss_voxel_sem_scal_c_0: 2.0273, loss_voxel_geo_scal_c_0: 1.4642, loss_voxel_lovasz_c_0: 0.6649, loss: 8.5844, grad_norm: 8.2875
2024-05-07 19:56:48,220 - mmdet - INFO - Epoch [16][200/440]	lr: 1.000e-04, eta: 4:44:21, time: 4.909, data_time: 0.172, memory: 17157, loss_depth: 0.1520, render_dep_loss_c_1_2: 0.3593, loss_voxel_ce_c_1_2: 0.3663, loss_voxel_sem_scal_c_1_2: 0.8584, loss_voxel_geo_scal_c_1_2: 0.5901, loss_voxel_lovasz_c_1_2: 0.3018, loss_voxel_ce_c_1_4: 0.2227, loss_voxel_sem_scal_c_1_4: 0.3833, loss_voxel_geo_scal_c_1_4: 0.2184, loss_voxel_lovasz_c_1_4: 0.1378, loss_voxel_ce_c_1_8: 0.1257, loss_voxel_sem_scal_c_1_8: 0.1704, loss_voxel_geo_scal_c_1_8: 0.0796, loss_voxel_lovasz_c_1_8: 0.0607, loss_voxel_ce_c_0: 0.4585, loss_voxel_sem_scal_c_0: 2.0404, loss_voxel_geo_scal_c_0: 1.4859, loss_voxel_lovasz_c_0: 0.6657, loss: 8.6771, grad_norm: 7.2721
2024-05-07 20:00:54,438 - mmdet - INFO - Epoch [16][250/440]	lr: 1.000e-04, eta: 4:40:44, time: 4.924, data_time: 0.176, memory: 17157, loss_depth: 0.1505, render_dep_loss_c_1_2: 0.3570, loss_voxel_ce_c_1_2: 0.3545, loss_voxel_sem_scal_c_1_2: 0.8166, loss_voxel_geo_scal_c_1_2: 0.5764, loss_voxel_lovasz_c_1_2: 0.2973, loss_voxel_ce_c_1_4: 0.2162, loss_voxel_sem_scal_c_1_4: 0.3627, loss_voxel_geo_scal_c_1_4: 0.2119, loss_voxel_lovasz_c_1_4: 0.1353, loss_voxel_ce_c_1_8: 0.1218, loss_voxel_sem_scal_c_1_8: 0.1662, loss_voxel_geo_scal_c_1_8: 0.0769, loss_voxel_lovasz_c_1_8: 0.0602, loss_voxel_ce_c_0: 0.4473, loss_voxel_sem_scal_c_0: 1.9364, loss_voxel_geo_scal_c_0: 1.4474, loss_voxel_lovasz_c_0: 0.6563, loss: 8.3910, grad_norm: 6.5092
2024-05-07 20:05:00,376 - mmdet - INFO - Epoch [16][300/440]	lr: 1.000e-04, eta: 4:37:07, time: 4.919, data_time: 0.173, memory: 17157, loss_depth: 0.1507, render_dep_loss_c_1_2: 0.3577, loss_voxel_ce_c_1_2: 0.3575, loss_voxel_sem_scal_c_1_2: 0.8312, loss_voxel_geo_scal_c_1_2: 0.5747, loss_voxel_lovasz_c_1_2: 0.3000, loss_voxel_ce_c_1_4: 0.2180, loss_voxel_sem_scal_c_1_4: 0.3695, loss_voxel_geo_scal_c_1_4: 0.2122, loss_voxel_lovasz_c_1_4: 0.1371, loss_voxel_ce_c_1_8: 0.1220, loss_voxel_sem_scal_c_1_8: 0.1721, loss_voxel_geo_scal_c_1_8: 0.0768, loss_voxel_lovasz_c_1_8: 0.0608, loss_voxel_ce_c_0: 0.4504, loss_voxel_sem_scal_c_0: 1.9773, loss_voxel_geo_scal_c_0: 1.4492, loss_voxel_lovasz_c_0: 0.6621, loss: 8.4791, grad_norm: 6.8694
2024-05-07 20:09:06,054 - mmdet - INFO - Epoch [16][350/440]	lr: 1.000e-04, eta: 4:33:30, time: 4.914, data_time: 0.173, memory: 17157, loss_depth: 0.1509, render_dep_loss_c_1_2: 0.3539, loss_voxel_ce_c_1_2: 0.3614, loss_voxel_sem_scal_c_1_2: 0.8314, loss_voxel_geo_scal_c_1_2: 0.5813, loss_voxel_lovasz_c_1_2: 0.2986, loss_voxel_ce_c_1_4: 0.2192, loss_voxel_sem_scal_c_1_4: 0.3701, loss_voxel_geo_scal_c_1_4: 0.2139, loss_voxel_lovasz_c_1_4: 0.1363, loss_voxel_ce_c_1_8: 0.1239, loss_voxel_sem_scal_c_1_8: 0.1656, loss_voxel_geo_scal_c_1_8: 0.0778, loss_voxel_lovasz_c_1_8: 0.0605, loss_voxel_ce_c_0: 0.4545, loss_voxel_sem_scal_c_0: 1.9890, loss_voxel_geo_scal_c_0: 1.4678, loss_voxel_lovasz_c_0: 0.6601, loss: 8.5160, grad_norm: 6.8651
2024-05-07 20:13:11,836 - mmdet - INFO - Exp name: sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d.py
2024-05-07 20:13:11,837 - mmdet - INFO - Epoch [16][400/440]	lr: 1.000e-04, eta: 4:29:52, time: 4.916, data_time: 0.173, memory: 17157, loss_depth: 0.1512, render_dep_loss_c_1_2: 0.3602, loss_voxel_ce_c_1_2: 0.3628, loss_voxel_sem_scal_c_1_2: 0.8317, loss_voxel_geo_scal_c_1_2: 0.5858, loss_voxel_lovasz_c_1_2: 0.3001, loss_voxel_ce_c_1_4: 0.2210, loss_voxel_sem_scal_c_1_4: 0.3717, loss_voxel_geo_scal_c_1_4: 0.2165, loss_voxel_lovasz_c_1_4: 0.1375, loss_voxel_ce_c_1_8: 0.1244, loss_voxel_sem_scal_c_1_8: 0.1678, loss_voxel_geo_scal_c_1_8: 0.0793, loss_voxel_lovasz_c_1_8: 0.0611, loss_voxel_ce_c_0: 0.4541, loss_voxel_sem_scal_c_0: 1.9813, loss_voxel_geo_scal_c_0: 1.4737, loss_voxel_lovasz_c_0: 0.6627, loss: 8.5429, grad_norm: 7.0952
2024-05-07 20:16:28,416 - mmdet - INFO - Saving checkpoint at 16 epochs
2024-05-07 20:16:29,729 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_16_ema.pth
2024-05-07 20:20:46,613 - mmdet - INFO - Epoch [17][50/440]	lr: 1.000e-04, eta: 4:21:48, time: 5.136, data_time: 0.362, memory: 17157, loss_depth: 0.1501, render_dep_loss_c_1_2: 0.3535, loss_voxel_ce_c_1_2: 0.3603, loss_voxel_sem_scal_c_1_2: 0.8119, loss_voxel_geo_scal_c_1_2: 0.5760, loss_voxel_lovasz_c_1_2: 0.2978, loss_voxel_ce_c_1_4: 0.2192, loss_voxel_sem_scal_c_1_4: 0.3621, loss_voxel_geo_scal_c_1_4: 0.2122, loss_voxel_lovasz_c_1_4: 0.1360, loss_voxel_ce_c_1_8: 0.1233, loss_voxel_sem_scal_c_1_8: 0.1719, loss_voxel_geo_scal_c_1_8: 0.0775, loss_voxel_lovasz_c_1_8: 0.0604, loss_voxel_ce_c_0: 0.4536, loss_voxel_sem_scal_c_0: 1.9393, loss_voxel_geo_scal_c_0: 1.4573, loss_voxel_lovasz_c_0: 0.6593, loss: 8.4219, grad_norm: 6.2462
2024-05-07 20:24:53,555 - mmdet - INFO - Epoch [17][100/440]	lr: 1.000e-04, eta: 4:18:11, time: 4.939, data_time: 0.181, memory: 17157, loss_depth: 0.1504, render_dep_loss_c_1_2: 0.3593, loss_voxel_ce_c_1_2: 0.3569, loss_voxel_sem_scal_c_1_2: 0.8198, loss_voxel_geo_scal_c_1_2: 0.5750, loss_voxel_lovasz_c_1_2: 0.2976, loss_voxel_ce_c_1_4: 0.2165, loss_voxel_sem_scal_c_1_4: 0.3646, loss_voxel_geo_scal_c_1_4: 0.2113, loss_voxel_lovasz_c_1_4: 0.1355, loss_voxel_ce_c_1_8: 0.1212, loss_voxel_sem_scal_c_1_8: 0.1746, loss_voxel_geo_scal_c_1_8: 0.0771, loss_voxel_lovasz_c_1_8: 0.0604, loss_voxel_ce_c_0: 0.4478, loss_voxel_sem_scal_c_0: 1.9574, loss_voxel_geo_scal_c_0: 1.4511, loss_voxel_lovasz_c_0: 0.6583, loss: 8.4348, grad_norm: 6.3218
2024-05-07 20:28:59,506 - mmdet - INFO - Epoch [17][150/440]	lr: 1.000e-04, eta: 4:14:34, time: 4.919, data_time: 0.166, memory: 17157, loss_depth: 0.1507, render_dep_loss_c_1_2: 0.3596, loss_voxel_ce_c_1_2: 0.3575, loss_voxel_sem_scal_c_1_2: 0.8001, loss_voxel_geo_scal_c_1_2: 0.5765, loss_voxel_lovasz_c_1_2: 0.2969, loss_voxel_ce_c_1_4: 0.2175, loss_voxel_sem_scal_c_1_4: 0.3529, loss_voxel_geo_scal_c_1_4: 0.2114, loss_voxel_lovasz_c_1_4: 0.1355, loss_voxel_ce_c_1_8: 0.1224, loss_voxel_sem_scal_c_1_8: 0.1669, loss_voxel_geo_scal_c_1_8: 0.0766, loss_voxel_lovasz_c_1_8: 0.0602, loss_voxel_ce_c_0: 0.4511, loss_voxel_sem_scal_c_0: 1.9251, loss_voxel_geo_scal_c_0: 1.4568, loss_voxel_lovasz_c_0: 0.6572, loss: 8.3747, grad_norm: 6.7379
2024-05-07 20:33:06,427 - mmdet - INFO - Epoch [17][200/440]	lr: 1.000e-04, eta: 4:10:57, time: 4.938, data_time: 0.169, memory: 17157, loss_depth: 0.1500, render_dep_loss_c_1_2: 0.3547, loss_voxel_ce_c_1_2: 0.3590, loss_voxel_sem_scal_c_1_2: 0.8146, loss_voxel_geo_scal_c_1_2: 0.5784, loss_voxel_lovasz_c_1_2: 0.2974, loss_voxel_ce_c_1_4: 0.2182, loss_voxel_sem_scal_c_1_4: 0.3633, loss_voxel_geo_scal_c_1_4: 0.2125, loss_voxel_lovasz_c_1_4: 0.1359, loss_voxel_ce_c_1_8: 0.1219, loss_voxel_sem_scal_c_1_8: 0.1668, loss_voxel_geo_scal_c_1_8: 0.0773, loss_voxel_lovasz_c_1_8: 0.0604, loss_voxel_ce_c_0: 0.4498, loss_voxel_sem_scal_c_0: 1.9483, loss_voxel_geo_scal_c_0: 1.4529, loss_voxel_lovasz_c_0: 0.6571, loss: 8.4185, grad_norm: 6.7573
2024-05-07 20:37:12,265 - mmdet - INFO - Epoch [17][250/440]	lr: 1.000e-04, eta: 4:07:18, time: 4.917, data_time: 0.163, memory: 17157, loss_depth: 0.1505, render_dep_loss_c_1_2: 0.3587, loss_voxel_ce_c_1_2: 0.3559, loss_voxel_sem_scal_c_1_2: 0.8051, loss_voxel_geo_scal_c_1_2: 0.5758, loss_voxel_lovasz_c_1_2: 0.2961, loss_voxel_ce_c_1_4: 0.2161, loss_voxel_sem_scal_c_1_4: 0.3598, loss_voxel_geo_scal_c_1_4: 0.2119, loss_voxel_lovasz_c_1_4: 0.1350, loss_voxel_ce_c_1_8: 0.1207, loss_voxel_sem_scal_c_1_8: 0.1664, loss_voxel_geo_scal_c_1_8: 0.0772, loss_voxel_lovasz_c_1_8: 0.0599, loss_voxel_ce_c_0: 0.4472, loss_voxel_sem_scal_c_0: 1.9349, loss_voxel_geo_scal_c_0: 1.4567, loss_voxel_lovasz_c_0: 0.6567, loss: 8.3845, grad_norm: 7.0496
2024-05-07 20:41:17,984 - mmdet - INFO - Epoch [17][300/440]	lr: 1.000e-04, eta: 4:03:40, time: 4.914, data_time: 0.163, memory: 17157, loss_depth: 0.1507, render_dep_loss_c_1_2: 0.3570, loss_voxel_ce_c_1_2: 0.3555, loss_voxel_sem_scal_c_1_2: 0.8245, loss_voxel_geo_scal_c_1_2: 0.5740, loss_voxel_lovasz_c_1_2: 0.2989, loss_voxel_ce_c_1_4: 0.2163, loss_voxel_sem_scal_c_1_4: 0.3648, loss_voxel_geo_scal_c_1_4: 0.2114, loss_voxel_lovasz_c_1_4: 0.1365, loss_voxel_ce_c_1_8: 0.1207, loss_voxel_sem_scal_c_1_8: 0.1657, loss_voxel_geo_scal_c_1_8: 0.0766, loss_voxel_lovasz_c_1_8: 0.0602, loss_voxel_ce_c_0: 0.4453, loss_voxel_sem_scal_c_0: 1.9687, loss_voxel_geo_scal_c_0: 1.4433, loss_voxel_lovasz_c_0: 0.6601, loss: 8.4302, grad_norm: 7.0290
2024-05-07 20:45:23,763 - mmdet - INFO - Epoch [17][350/440]	lr: 1.000e-04, eta: 4:00:01, time: 4.916, data_time: 0.165, memory: 17157, loss_depth: 0.1496, render_dep_loss_c_1_2: 0.3553, loss_voxel_ce_c_1_2: 0.3552, loss_voxel_sem_scal_c_1_2: 0.8041, loss_voxel_geo_scal_c_1_2: 0.5755, loss_voxel_lovasz_c_1_2: 0.2962, loss_voxel_ce_c_1_4: 0.2160, loss_voxel_sem_scal_c_1_4: 0.3641, loss_voxel_geo_scal_c_1_4: 0.2119, loss_voxel_lovasz_c_1_4: 0.1355, loss_voxel_ce_c_1_8: 0.1209, loss_voxel_sem_scal_c_1_8: 0.1668, loss_voxel_geo_scal_c_1_8: 0.0773, loss_voxel_lovasz_c_1_8: 0.0599, loss_voxel_ce_c_0: 0.4478, loss_voxel_sem_scal_c_0: 1.9343, loss_voxel_geo_scal_c_0: 1.4467, loss_voxel_lovasz_c_0: 0.6558, loss: 8.3728, grad_norm: 6.4166
2024-05-07 20:49:29,657 - mmdet - INFO - Epoch [17][400/440]	lr: 1.000e-04, eta: 3:56:22, time: 4.918, data_time: 0.163, memory: 17157, loss_depth: 0.1505, render_dep_loss_c_1_2: 0.3578, loss_voxel_ce_c_1_2: 0.3592, loss_voxel_sem_scal_c_1_2: 0.8114, loss_voxel_geo_scal_c_1_2: 0.5759, loss_voxel_lovasz_c_1_2: 0.2973, loss_voxel_ce_c_1_4: 0.2184, loss_voxel_sem_scal_c_1_4: 0.3603, loss_voxel_geo_scal_c_1_4: 0.2123, loss_voxel_lovasz_c_1_4: 0.1359, loss_voxel_ce_c_1_8: 0.1225, loss_voxel_sem_scal_c_1_8: 0.1664, loss_voxel_geo_scal_c_1_8: 0.0771, loss_voxel_lovasz_c_1_8: 0.0606, loss_voxel_ce_c_0: 0.4493, loss_voxel_sem_scal_c_0: 1.9433, loss_voxel_geo_scal_c_0: 1.4553, loss_voxel_lovasz_c_0: 0.6579, loss: 8.4116, grad_norm: 6.7895
2024-05-07 20:52:45,756 - mmdet - INFO - Saving checkpoint at 17 epochs
2024-05-07 20:52:47,036 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_17_ema.pth
2024-05-07 20:57:03,555 - mmdet - INFO - Epoch [18][50/440]	lr: 1.000e-04, eta: 3:48:31, time: 5.129, data_time: 0.358, memory: 17157, loss_depth: 0.1505, render_dep_loss_c_1_2: 0.3552, loss_voxel_ce_c_1_2: 0.3566, loss_voxel_sem_scal_c_1_2: 0.8027, loss_voxel_geo_scal_c_1_2: 0.5740, loss_voxel_lovasz_c_1_2: 0.2960, loss_voxel_ce_c_1_4: 0.2164, loss_voxel_sem_scal_c_1_4: 0.3483, loss_voxel_geo_scal_c_1_4: 0.2105, loss_voxel_lovasz_c_1_4: 0.1345, loss_voxel_ce_c_1_8: 0.1205, loss_voxel_sem_scal_c_1_8: 0.1621, loss_voxel_geo_scal_c_1_8: 0.0761, loss_voxel_lovasz_c_1_8: 0.0596, loss_voxel_ce_c_0: 0.4496, loss_voxel_sem_scal_c_0: 1.9250, loss_voxel_geo_scal_c_0: 1.4465, loss_voxel_lovasz_c_0: 0.6552, loss: 8.3393, grad_norm: 6.4395
2024-05-07 21:01:08,040 - mmdet - INFO - Epoch [18][100/440]	lr: 1.000e-04, eta: 3:44:52, time: 4.890, data_time: 0.155, memory: 17157, loss_depth: 0.1498, render_dep_loss_c_1_2: 0.3541, loss_voxel_ce_c_1_2: 0.3551, loss_voxel_sem_scal_c_1_2: 0.8232, loss_voxel_geo_scal_c_1_2: 0.5737, loss_voxel_lovasz_c_1_2: 0.2968, loss_voxel_ce_c_1_4: 0.2152, loss_voxel_sem_scal_c_1_4: 0.3572, loss_voxel_geo_scal_c_1_4: 0.2108, loss_voxel_lovasz_c_1_4: 0.1345, loss_voxel_ce_c_1_8: 0.1212, loss_voxel_sem_scal_c_1_8: 0.1635, loss_voxel_geo_scal_c_1_8: 0.0765, loss_voxel_lovasz_c_1_8: 0.0598, loss_voxel_ce_c_0: 0.4477, loss_voxel_sem_scal_c_0: 1.9722, loss_voxel_geo_scal_c_0: 1.4476, loss_voxel_lovasz_c_0: 0.6572, loss: 8.4161, grad_norm: 7.3056
2024-05-07 21:05:12,860 - mmdet - INFO - Epoch [18][150/440]	lr: 1.000e-04, eta: 3:41:13, time: 4.896, data_time: 0.156, memory: 17157, loss_depth: 0.1502, render_dep_loss_c_1_2: 0.3564, loss_voxel_ce_c_1_2: 0.3567, loss_voxel_sem_scal_c_1_2: 0.7922, loss_voxel_geo_scal_c_1_2: 0.5751, loss_voxel_lovasz_c_1_2: 0.2943, loss_voxel_ce_c_1_4: 0.2161, loss_voxel_sem_scal_c_1_4: 0.3560, loss_voxel_geo_scal_c_1_4: 0.2118, loss_voxel_lovasz_c_1_4: 0.1344, loss_voxel_ce_c_1_8: 0.1211, loss_voxel_sem_scal_c_1_8: 0.1652, loss_voxel_geo_scal_c_1_8: 0.0767, loss_voxel_lovasz_c_1_8: 0.0595, loss_voxel_ce_c_0: 0.4509, loss_voxel_sem_scal_c_0: 1.9053, loss_voxel_geo_scal_c_0: 1.4487, loss_voxel_lovasz_c_0: 0.6532, loss: 8.3238, grad_norm: 7.5825
2024-05-07 21:09:18,835 - mmdet - INFO - Epoch [18][200/440]	lr: 1.000e-04, eta: 3:37:33, time: 4.920, data_time: 0.154, memory: 17157, loss_depth: 0.1494, render_dep_loss_c_1_2: 0.3553, loss_voxel_ce_c_1_2: 0.3520, loss_voxel_sem_scal_c_1_2: 0.7945, loss_voxel_geo_scal_c_1_2: 0.5717, loss_voxel_lovasz_c_1_2: 0.2963, loss_voxel_ce_c_1_4: 0.2140, loss_voxel_sem_scal_c_1_4: 0.3550, loss_voxel_geo_scal_c_1_4: 0.2106, loss_voxel_lovasz_c_1_4: 0.1349, loss_voxel_ce_c_1_8: 0.1199, loss_voxel_sem_scal_c_1_8: 0.1621, loss_voxel_geo_scal_c_1_8: 0.0760, loss_voxel_lovasz_c_1_8: 0.0596, loss_voxel_ce_c_0: 0.4416, loss_voxel_sem_scal_c_0: 1.9146, loss_voxel_geo_scal_c_0: 1.4422, loss_voxel_lovasz_c_0: 0.6555, loss: 8.3052, grad_norm: 6.1636
2024-05-07 21:13:23,380 - mmdet - INFO - Epoch [18][250/440]	lr: 1.000e-04, eta: 3:33:53, time: 4.891, data_time: 0.152, memory: 17157, loss_depth: 0.1494, render_dep_loss_c_1_2: 0.3551, loss_voxel_ce_c_1_2: 0.3518, loss_voxel_sem_scal_c_1_2: 0.7912, loss_voxel_geo_scal_c_1_2: 0.5692, loss_voxel_lovasz_c_1_2: 0.2957, loss_voxel_ce_c_1_4: 0.2135, loss_voxel_sem_scal_c_1_4: 0.3508, loss_voxel_geo_scal_c_1_4: 0.2087, loss_voxel_lovasz_c_1_4: 0.1347, loss_voxel_ce_c_1_8: 0.1191, loss_voxel_sem_scal_c_1_8: 0.1617, loss_voxel_geo_scal_c_1_8: 0.0755, loss_voxel_lovasz_c_1_8: 0.0596, loss_voxel_ce_c_0: 0.4423, loss_voxel_sem_scal_c_0: 1.9153, loss_voxel_geo_scal_c_0: 1.4349, loss_voxel_lovasz_c_0: 0.6560, loss: 8.2846, grad_norm: 6.8749
2024-05-07 21:17:27,485 - mmdet - INFO - Epoch [18][300/440]	lr: 1.000e-04, eta: 3:30:13, time: 4.882, data_time: 0.135, memory: 17157, loss_depth: 0.1498, render_dep_loss_c_1_2: 0.3577, loss_voxel_ce_c_1_2: 0.3553, loss_voxel_sem_scal_c_1_2: 0.8013, loss_voxel_geo_scal_c_1_2: 0.5751, loss_voxel_lovasz_c_1_2: 0.2956, loss_voxel_ce_c_1_4: 0.2166, loss_voxel_sem_scal_c_1_4: 0.3496, loss_voxel_geo_scal_c_1_4: 0.2117, loss_voxel_lovasz_c_1_4: 0.1343, loss_voxel_ce_c_1_8: 0.1209, loss_voxel_sem_scal_c_1_8: 0.1614, loss_voxel_geo_scal_c_1_8: 0.0767, loss_voxel_lovasz_c_1_8: 0.0593, loss_voxel_ce_c_0: 0.4487, loss_voxel_sem_scal_c_0: 1.9095, loss_voxel_geo_scal_c_0: 1.4532, loss_voxel_lovasz_c_0: 0.6550, loss: 8.3316, grad_norm: 6.3152
2024-05-07 21:21:32,243 - mmdet - INFO - Epoch [18][350/440]	lr: 1.000e-04, eta: 3:26:32, time: 4.895, data_time: 0.136, memory: 17157, loss_depth: 0.1499, render_dep_loss_c_1_2: 0.3581, loss_voxel_ce_c_1_2: 0.3598, loss_voxel_sem_scal_c_1_2: 0.8085, loss_voxel_geo_scal_c_1_2: 0.5753, loss_voxel_lovasz_c_1_2: 0.2957, loss_voxel_ce_c_1_4: 0.2182, loss_voxel_sem_scal_c_1_4: 0.3585, loss_voxel_geo_scal_c_1_4: 0.2113, loss_voxel_lovasz_c_1_4: 0.1350, loss_voxel_ce_c_1_8: 0.1224, loss_voxel_sem_scal_c_1_8: 0.1626, loss_voxel_geo_scal_c_1_8: 0.0765, loss_voxel_lovasz_c_1_8: 0.0601, loss_voxel_ce_c_0: 0.4536, loss_voxel_sem_scal_c_0: 1.9317, loss_voxel_geo_scal_c_0: 1.4501, loss_voxel_lovasz_c_0: 0.6553, loss: 8.3826, grad_norm: 6.5760
2024-05-07 21:25:35,970 - mmdet - INFO - Epoch [18][400/440]	lr: 1.000e-04, eta: 3:22:51, time: 4.875, data_time: 0.132, memory: 17157, loss_depth: 0.1488, render_dep_loss_c_1_2: 0.3577, loss_voxel_ce_c_1_2: 0.3557, loss_voxel_sem_scal_c_1_2: 0.8114, loss_voxel_geo_scal_c_1_2: 0.5707, loss_voxel_lovasz_c_1_2: 0.2964, loss_voxel_ce_c_1_4: 0.2167, loss_voxel_sem_scal_c_1_4: 0.3513, loss_voxel_geo_scal_c_1_4: 0.2097, loss_voxel_lovasz_c_1_4: 0.1342, loss_voxel_ce_c_1_8: 0.1213, loss_voxel_sem_scal_c_1_8: 0.1617, loss_voxel_geo_scal_c_1_8: 0.0761, loss_voxel_lovasz_c_1_8: 0.0597, loss_voxel_ce_c_0: 0.4476, loss_voxel_sem_scal_c_0: 1.9269, loss_voxel_geo_scal_c_0: 1.4412, loss_voxel_lovasz_c_0: 0.6546, loss: 8.3416, grad_norm: 6.2777
2024-05-07 21:28:50,871 - mmdet - INFO - Saving checkpoint at 18 epochs
2024-05-07 21:28:52,148 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_18_ema.pth
2024-05-07 21:33:08,256 - mmdet - INFO - Epoch [19][50/440]	lr: 1.000e-04, eta: 3:15:13, time: 5.121, data_time: 0.373, memory: 17157, loss_depth: 0.1490, render_dep_loss_c_1_2: 0.3560, loss_voxel_ce_c_1_2: 0.3533, loss_voxel_sem_scal_c_1_2: 0.7877, loss_voxel_geo_scal_c_1_2: 0.5672, loss_voxel_lovasz_c_1_2: 0.2921, loss_voxel_ce_c_1_4: 0.2145, loss_voxel_sem_scal_c_1_4: 0.3478, loss_voxel_geo_scal_c_1_4: 0.2081, loss_voxel_lovasz_c_1_4: 0.1330, loss_voxel_ce_c_1_8: 0.1191, loss_voxel_sem_scal_c_1_8: 0.1606, loss_voxel_geo_scal_c_1_8: 0.0755, loss_voxel_lovasz_c_1_8: 0.0589, loss_voxel_ce_c_0: 0.4443, loss_voxel_sem_scal_c_0: 1.8962, loss_voxel_geo_scal_c_0: 1.4312, loss_voxel_lovasz_c_0: 0.6482, loss: 8.2426, grad_norm: 7.0497
2024-05-07 21:37:13,817 - mmdet - INFO - Epoch [19][100/440]	lr: 1.000e-04, eta: 3:11:33, time: 4.911, data_time: 0.185, memory: 17157, loss_depth: 0.1489, render_dep_loss_c_1_2: 0.3530, loss_voxel_ce_c_1_2: 0.3490, loss_voxel_sem_scal_c_1_2: 0.7703, loss_voxel_geo_scal_c_1_2: 0.5603, loss_voxel_lovasz_c_1_2: 0.2935, loss_voxel_ce_c_1_4: 0.2115, loss_voxel_sem_scal_c_1_4: 0.3380, loss_voxel_geo_scal_c_1_4: 0.2059, loss_voxel_lovasz_c_1_4: 0.1335, loss_voxel_ce_c_1_8: 0.1183, loss_voxel_sem_scal_c_1_8: 0.1599, loss_voxel_geo_scal_c_1_8: 0.0747, loss_voxel_lovasz_c_1_8: 0.0589, loss_voxel_ce_c_0: 0.4379, loss_voxel_sem_scal_c_0: 1.8693, loss_voxel_geo_scal_c_0: 1.4184, loss_voxel_lovasz_c_0: 0.6511, loss: 8.1523, grad_norm: 6.3428
2024-05-07 21:41:20,565 - mmdet - INFO - Epoch [19][150/440]	lr: 1.000e-04, eta: 3:07:53, time: 4.935, data_time: 0.177, memory: 17157, loss_depth: 0.1489, render_dep_loss_c_1_2: 0.3532, loss_voxel_ce_c_1_2: 0.3521, loss_voxel_sem_scal_c_1_2: 0.8028, loss_voxel_geo_scal_c_1_2: 0.5705, loss_voxel_lovasz_c_1_2: 0.2944, loss_voxel_ce_c_1_4: 0.2137, loss_voxel_sem_scal_c_1_4: 0.3508, loss_voxel_geo_scal_c_1_4: 0.2098, loss_voxel_lovasz_c_1_4: 0.1340, loss_voxel_ce_c_1_8: 0.1187, loss_voxel_sem_scal_c_1_8: 0.1658, loss_voxel_geo_scal_c_1_8: 0.0763, loss_voxel_lovasz_c_1_8: 0.0596, loss_voxel_ce_c_0: 0.4422, loss_voxel_sem_scal_c_0: 1.9193, loss_voxel_geo_scal_c_0: 1.4375, loss_voxel_lovasz_c_0: 0.6525, loss: 8.3021, grad_norm: 7.1272
2024-05-07 21:45:26,136 - mmdet - INFO - Epoch [19][200/440]	lr: 1.000e-04, eta: 3:04:12, time: 4.911, data_time: 0.188, memory: 17157, loss_depth: 0.1495, render_dep_loss_c_1_2: 0.3560, loss_voxel_ce_c_1_2: 0.3579, loss_voxel_sem_scal_c_1_2: 0.7964, loss_voxel_geo_scal_c_1_2: 0.5704, loss_voxel_lovasz_c_1_2: 0.2950, loss_voxel_ce_c_1_4: 0.2164, loss_voxel_sem_scal_c_1_4: 0.3523, loss_voxel_geo_scal_c_1_4: 0.2103, loss_voxel_lovasz_c_1_4: 0.1348, loss_voxel_ce_c_1_8: 0.1211, loss_voxel_sem_scal_c_1_8: 0.1622, loss_voxel_geo_scal_c_1_8: 0.0762, loss_voxel_lovasz_c_1_8: 0.0599, loss_voxel_ce_c_0: 0.4483, loss_voxel_sem_scal_c_0: 1.8976, loss_voxel_geo_scal_c_0: 1.4344, loss_voxel_lovasz_c_0: 0.6525, loss: 8.2915, grad_norm: 6.8513
2024-05-07 21:49:31,561 - mmdet - INFO - Epoch [19][250/440]	lr: 1.000e-04, eta: 3:00:31, time: 4.909, data_time: 0.176, memory: 17157, loss_depth: 0.1500, render_dep_loss_c_1_2: 0.3555, loss_voxel_ce_c_1_2: 0.3513, loss_voxel_sem_scal_c_1_2: 0.7898, loss_voxel_geo_scal_c_1_2: 0.5708, loss_voxel_lovasz_c_1_2: 0.2937, loss_voxel_ce_c_1_4: 0.2135, loss_voxel_sem_scal_c_1_4: 0.3428, loss_voxel_geo_scal_c_1_4: 0.2088, loss_voxel_lovasz_c_1_4: 0.1334, loss_voxel_ce_c_1_8: 0.1202, loss_voxel_sem_scal_c_1_8: 0.1589, loss_voxel_geo_scal_c_1_8: 0.0751, loss_voxel_lovasz_c_1_8: 0.0593, loss_voxel_ce_c_0: 0.4453, loss_voxel_sem_scal_c_0: 1.8936, loss_voxel_geo_scal_c_0: 1.4458, loss_voxel_lovasz_c_0: 0.6513, loss: 8.2589, grad_norm: 6.8004
2024-05-07 21:53:38,070 - mmdet - INFO - Epoch [19][300/440]	lr: 1.000e-04, eta: 2:56:50, time: 4.930, data_time: 0.177, memory: 17157, loss_depth: 0.1487, render_dep_loss_c_1_2: 0.3588, loss_voxel_ce_c_1_2: 0.3503, loss_voxel_sem_scal_c_1_2: 0.7980, loss_voxel_geo_scal_c_1_2: 0.5670, loss_voxel_lovasz_c_1_2: 0.2937, loss_voxel_ce_c_1_4: 0.2128, loss_voxel_sem_scal_c_1_4: 0.3505, loss_voxel_geo_scal_c_1_4: 0.2084, loss_voxel_lovasz_c_1_4: 0.1335, loss_voxel_ce_c_1_8: 0.1188, loss_voxel_sem_scal_c_1_8: 0.1628, loss_voxel_geo_scal_c_1_8: 0.0751, loss_voxel_lovasz_c_1_8: 0.0595, loss_voxel_ce_c_0: 0.4445, loss_voxel_sem_scal_c_0: 1.8952, loss_voxel_geo_scal_c_0: 1.4245, loss_voxel_lovasz_c_0: 0.6506, loss: 8.2527, grad_norm: 6.4649
2024-05-07 21:57:43,319 - mmdet - INFO - Epoch [19][350/440]	lr: 1.000e-04, eta: 2:53:09, time: 4.905, data_time: 0.179, memory: 17157, loss_depth: 0.1490, render_dep_loss_c_1_2: 0.3529, loss_voxel_ce_c_1_2: 0.3511, loss_voxel_sem_scal_c_1_2: 0.8015, loss_voxel_geo_scal_c_1_2: 0.5656, loss_voxel_lovasz_c_1_2: 0.2947, loss_voxel_ce_c_1_4: 0.2129, loss_voxel_sem_scal_c_1_4: 0.3576, loss_voxel_geo_scal_c_1_4: 0.2079, loss_voxel_lovasz_c_1_4: 0.1344, loss_voxel_ce_c_1_8: 0.1190, loss_voxel_sem_scal_c_1_8: 0.1580, loss_voxel_geo_scal_c_1_8: 0.0751, loss_voxel_lovasz_c_1_8: 0.0593, loss_voxel_ce_c_0: 0.4406, loss_voxel_sem_scal_c_0: 1.9170, loss_voxel_geo_scal_c_0: 1.4299, loss_voxel_lovasz_c_0: 0.6537, loss: 8.2801, grad_norm: 7.1244
2024-05-07 22:01:48,874 - mmdet - INFO - Epoch [19][400/440]	lr: 1.000e-04, eta: 2:49:27, time: 4.911, data_time: 0.175, memory: 17157, loss_depth: 0.1491, render_dep_loss_c_1_2: 0.3582, loss_voxel_ce_c_1_2: 0.3513, loss_voxel_sem_scal_c_1_2: 0.8045, loss_voxel_geo_scal_c_1_2: 0.5708, loss_voxel_lovasz_c_1_2: 0.2946, loss_voxel_ce_c_1_4: 0.2129, loss_voxel_sem_scal_c_1_4: 0.3540, loss_voxel_geo_scal_c_1_4: 0.2094, loss_voxel_lovasz_c_1_4: 0.1338, loss_voxel_ce_c_1_8: 0.1188, loss_voxel_sem_scal_c_1_8: 0.1596, loss_voxel_geo_scal_c_1_8: 0.0758, loss_voxel_lovasz_c_1_8: 0.0594, loss_voxel_ce_c_0: 0.4417, loss_voxel_sem_scal_c_0: 1.9092, loss_voxel_geo_scal_c_0: 1.4401, loss_voxel_lovasz_c_0: 0.6526, loss: 8.2958, grad_norm: 6.9007
2024-05-07 22:05:05,083 - mmdet - INFO - Saving checkpoint at 19 epochs
2024-05-07 22:05:06,812 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_19_ema.pth
2024-05-07 22:09:24,597 - mmdet - INFO - Epoch [20][50/440]	lr: 1.000e-04, eta: 2:42:00, time: 5.154, data_time: 0.418, memory: 17157, loss_depth: 0.1485, render_dep_loss_c_1_2: 0.3545, loss_voxel_ce_c_1_2: 0.3496, loss_voxel_sem_scal_c_1_2: 0.7904, loss_voxel_geo_scal_c_1_2: 0.5619, loss_voxel_lovasz_c_1_2: 0.2924, loss_voxel_ce_c_1_4: 0.2132, loss_voxel_sem_scal_c_1_4: 0.3412, loss_voxel_geo_scal_c_1_4: 0.2061, loss_voxel_lovasz_c_1_4: 0.1326, loss_voxel_ce_c_1_8: 0.1187, loss_voxel_sem_scal_c_1_8: 0.1585, loss_voxel_geo_scal_c_1_8: 0.0746, loss_voxel_lovasz_c_1_8: 0.0589, loss_voxel_ce_c_0: 0.4423, loss_voxel_sem_scal_c_0: 1.8995, loss_voxel_geo_scal_c_0: 1.4209, loss_voxel_lovasz_c_0: 0.6499, loss: 8.2136, grad_norm: 6.7349
2024-05-07 22:13:32,467 - mmdet - INFO - Epoch [20][100/440]	lr: 1.000e-04, eta: 2:38:19, time: 4.957, data_time: 0.231, memory: 17157, loss_depth: 0.1493, render_dep_loss_c_1_2: 0.3554, loss_voxel_ce_c_1_2: 0.3512, loss_voxel_sem_scal_c_1_2: 0.7801, loss_voxel_geo_scal_c_1_2: 0.5691, loss_voxel_lovasz_c_1_2: 0.2927, loss_voxel_ce_c_1_4: 0.2128, loss_voxel_sem_scal_c_1_4: 0.3523, loss_voxel_geo_scal_c_1_4: 0.2089, loss_voxel_lovasz_c_1_4: 0.1336, loss_voxel_ce_c_1_8: 0.1188, loss_voxel_sem_scal_c_1_8: 0.1637, loss_voxel_geo_scal_c_1_8: 0.0758, loss_voxel_lovasz_c_1_8: 0.0595, loss_voxel_ce_c_0: 0.4434, loss_voxel_sem_scal_c_0: 1.8889, loss_voxel_geo_scal_c_0: 1.4366, loss_voxel_lovasz_c_0: 0.6499, loss: 8.2417, grad_norm: 6.5695
2024-05-07 22:17:40,393 - mmdet - INFO - Epoch [20][150/440]	lr: 1.000e-04, eta: 2:34:38, time: 4.958, data_time: 0.230, memory: 17157, loss_depth: 0.1485, render_dep_loss_c_1_2: 0.3543, loss_voxel_ce_c_1_2: 0.3515, loss_voxel_sem_scal_c_1_2: 0.8042, loss_voxel_geo_scal_c_1_2: 0.5679, loss_voxel_lovasz_c_1_2: 0.2943, loss_voxel_ce_c_1_4: 0.2140, loss_voxel_sem_scal_c_1_4: 0.3522, loss_voxel_geo_scal_c_1_4: 0.2089, loss_voxel_lovasz_c_1_4: 0.1341, loss_voxel_ce_c_1_8: 0.1197, loss_voxel_sem_scal_c_1_8: 0.1644, loss_voxel_geo_scal_c_1_8: 0.0757, loss_voxel_lovasz_c_1_8: 0.0594, loss_voxel_ce_c_0: 0.4434, loss_voxel_sem_scal_c_0: 1.9220, loss_voxel_geo_scal_c_0: 1.4302, loss_voxel_lovasz_c_0: 0.6510, loss: 8.2953, grad_norm: 6.6766
2024-05-07 22:21:48,095 - mmdet - INFO - Epoch [20][200/440]	lr: 1.000e-04, eta: 2:30:57, time: 4.954, data_time: 0.231, memory: 17157, loss_depth: 0.1489, render_dep_loss_c_1_2: 0.3566, loss_voxel_ce_c_1_2: 0.3523, loss_voxel_sem_scal_c_1_2: 0.7719, loss_voxel_geo_scal_c_1_2: 0.5648, loss_voxel_lovasz_c_1_2: 0.2927, loss_voxel_ce_c_1_4: 0.2127, loss_voxel_sem_scal_c_1_4: 0.3468, loss_voxel_geo_scal_c_1_4: 0.2072, loss_voxel_lovasz_c_1_4: 0.1335, loss_voxel_ce_c_1_8: 0.1190, loss_voxel_sem_scal_c_1_8: 0.1614, loss_voxel_geo_scal_c_1_8: 0.0748, loss_voxel_lovasz_c_1_8: 0.0592, loss_voxel_ce_c_0: 0.4441, loss_voxel_sem_scal_c_0: 1.8563, loss_voxel_geo_scal_c_0: 1.4226, loss_voxel_lovasz_c_0: 0.6491, loss: 8.1739, grad_norm: 6.4848
2024-05-07 22:25:57,401 - mmdet - INFO - Epoch [20][250/440]	lr: 1.000e-04, eta: 2:27:16, time: 4.986, data_time: 0.228, memory: 17157, loss_depth: 0.1488, render_dep_loss_c_1_2: 0.3522, loss_voxel_ce_c_1_2: 0.3484, loss_voxel_sem_scal_c_1_2: 0.7825, loss_voxel_geo_scal_c_1_2: 0.5616, loss_voxel_lovasz_c_1_2: 0.2935, loss_voxel_ce_c_1_4: 0.2102, loss_voxel_sem_scal_c_1_4: 0.3450, loss_voxel_geo_scal_c_1_4: 0.2065, loss_voxel_lovasz_c_1_4: 0.1333, loss_voxel_ce_c_1_8: 0.1168, loss_voxel_sem_scal_c_1_8: 0.1588, loss_voxel_geo_scal_c_1_8: 0.0744, loss_voxel_lovasz_c_1_8: 0.0586, loss_voxel_ce_c_0: 0.4386, loss_voxel_sem_scal_c_0: 1.8791, loss_voxel_geo_scal_c_0: 1.4156, loss_voxel_lovasz_c_0: 0.6510, loss: 8.1747, grad_norm: 6.7451
2024-05-07 22:30:05,448 - mmdet - INFO - Epoch [20][300/440]	lr: 1.000e-04, eta: 2:23:34, time: 4.961, data_time: 0.230, memory: 17157, loss_depth: 0.1488, render_dep_loss_c_1_2: 0.3548, loss_voxel_ce_c_1_2: 0.3526, loss_voxel_sem_scal_c_1_2: 0.7741, loss_voxel_geo_scal_c_1_2: 0.5694, loss_voxel_lovasz_c_1_2: 0.2926, loss_voxel_ce_c_1_4: 0.2134, loss_voxel_sem_scal_c_1_4: 0.3436, loss_voxel_geo_scal_c_1_4: 0.2087, loss_voxel_lovasz_c_1_4: 0.1329, loss_voxel_ce_c_1_8: 0.1186, loss_voxel_sem_scal_c_1_8: 0.1580, loss_voxel_geo_scal_c_1_8: 0.0755, loss_voxel_lovasz_c_1_8: 0.0588, loss_voxel_ce_c_0: 0.4450, loss_voxel_sem_scal_c_0: 1.8588, loss_voxel_geo_scal_c_0: 1.4342, loss_voxel_lovasz_c_0: 0.6494, loss: 8.1892, grad_norm: 6.5291
2024-05-07 22:34:13,256 - mmdet - INFO - Epoch [20][350/440]	lr: 1.000e-04, eta: 2:19:52, time: 4.956, data_time: 0.230, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3562, loss_voxel_ce_c_1_2: 0.3462, loss_voxel_sem_scal_c_1_2: 0.7819, loss_voxel_geo_scal_c_1_2: 0.5634, loss_voxel_lovasz_c_1_2: 0.2930, loss_voxel_ce_c_1_4: 0.2103, loss_voxel_sem_scal_c_1_4: 0.3458, loss_voxel_geo_scal_c_1_4: 0.2068, loss_voxel_lovasz_c_1_4: 0.1332, loss_voxel_ce_c_1_8: 0.1169, loss_voxel_sem_scal_c_1_8: 0.1606, loss_voxel_geo_scal_c_1_8: 0.0750, loss_voxel_lovasz_c_1_8: 0.0591, loss_voxel_ce_c_0: 0.4340, loss_voxel_sem_scal_c_0: 1.8854, loss_voxel_geo_scal_c_0: 1.4223, loss_voxel_lovasz_c_0: 0.6508, loss: 8.1889, grad_norm: 6.4249
2024-05-07 22:38:21,349 - mmdet - INFO - Epoch [20][400/440]	lr: 1.000e-04, eta: 2:16:09, time: 4.962, data_time: 0.229, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3534, loss_voxel_ce_c_1_2: 0.3488, loss_voxel_sem_scal_c_1_2: 0.7916, loss_voxel_geo_scal_c_1_2: 0.5634, loss_voxel_lovasz_c_1_2: 0.2939, loss_voxel_ce_c_1_4: 0.2115, loss_voxel_sem_scal_c_1_4: 0.3467, loss_voxel_geo_scal_c_1_4: 0.2070, loss_voxel_lovasz_c_1_4: 0.1332, loss_voxel_ce_c_1_8: 0.1175, loss_voxel_sem_scal_c_1_8: 0.1597, loss_voxel_geo_scal_c_1_8: 0.0747, loss_voxel_lovasz_c_1_8: 0.0592, loss_voxel_ce_c_0: 0.4411, loss_voxel_sem_scal_c_0: 1.8977, loss_voxel_geo_scal_c_0: 1.4184, loss_voxel_lovasz_c_0: 0.6504, loss: 8.2160, grad_norm: 7.0468
2024-05-07 22:41:39,823 - mmdet - INFO - Saving checkpoint at 20 epochs
2024-05-07 22:41:41,123 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_20_ema.pth
2024-05-07 22:45:56,573 - mmdet - INFO - Epoch [21][50/440]	lr: 1.000e-04, eta: 2:08:51, time: 5.107, data_time: 0.353, memory: 17157, loss_depth: 0.1489, render_dep_loss_c_1_2: 0.3515, loss_voxel_ce_c_1_2: 0.3494, loss_voxel_sem_scal_c_1_2: 0.7726, loss_voxel_geo_scal_c_1_2: 0.5658, loss_voxel_lovasz_c_1_2: 0.2925, loss_voxel_ce_c_1_4: 0.2100, loss_voxel_sem_scal_c_1_4: 0.3412, loss_voxel_geo_scal_c_1_4: 0.2066, loss_voxel_lovasz_c_1_4: 0.1329, loss_voxel_ce_c_1_8: 0.1163, loss_voxel_sem_scal_c_1_8: 0.1548, loss_voxel_geo_scal_c_1_8: 0.0746, loss_voxel_lovasz_c_1_8: 0.0586, loss_voxel_ce_c_0: 0.4437, loss_voxel_sem_scal_c_0: 1.8524, loss_voxel_geo_scal_c_0: 1.4323, loss_voxel_lovasz_c_0: 0.6493, loss: 8.1532, grad_norm: 7.2147
2024-05-07 22:50:01,841 - mmdet - INFO - Epoch [21][100/440]	lr: 1.000e-04, eta: 2:05:09, time: 4.906, data_time: 0.158, memory: 17157, loss_depth: 0.1483, render_dep_loss_c_1_2: 0.3530, loss_voxel_ce_c_1_2: 0.3488, loss_voxel_sem_scal_c_1_2: 0.7829, loss_voxel_geo_scal_c_1_2: 0.5622, loss_voxel_lovasz_c_1_2: 0.2921, loss_voxel_ce_c_1_4: 0.2120, loss_voxel_sem_scal_c_1_4: 0.3401, loss_voxel_geo_scal_c_1_4: 0.2062, loss_voxel_lovasz_c_1_4: 0.1324, loss_voxel_ce_c_1_8: 0.1170, loss_voxel_sem_scal_c_1_8: 0.1578, loss_voxel_geo_scal_c_1_8: 0.0742, loss_voxel_lovasz_c_1_8: 0.0585, loss_voxel_ce_c_0: 0.4399, loss_voxel_sem_scal_c_0: 1.8696, loss_voxel_geo_scal_c_0: 1.4182, loss_voxel_lovasz_c_0: 0.6488, loss: 8.1622, grad_norm: 6.5655
2024-05-07 22:54:07,150 - mmdet - INFO - Epoch [21][150/440]	lr: 1.000e-04, eta: 2:01:26, time: 4.906, data_time: 0.161, memory: 17157, loss_depth: 0.1483, render_dep_loss_c_1_2: 0.3559, loss_voxel_ce_c_1_2: 0.3484, loss_voxel_sem_scal_c_1_2: 0.7657, loss_voxel_geo_scal_c_1_2: 0.5648, loss_voxel_lovasz_c_1_2: 0.2913, loss_voxel_ce_c_1_4: 0.2122, loss_voxel_sem_scal_c_1_4: 0.3346, loss_voxel_geo_scal_c_1_4: 0.2083, loss_voxel_lovasz_c_1_4: 0.1324, loss_voxel_ce_c_1_8: 0.1172, loss_voxel_sem_scal_c_1_8: 0.1531, loss_voxel_geo_scal_c_1_8: 0.0754, loss_voxel_lovasz_c_1_8: 0.0585, loss_voxel_ce_c_0: 0.4395, loss_voxel_sem_scal_c_0: 1.8484, loss_voxel_geo_scal_c_0: 1.4185, loss_voxel_lovasz_c_0: 0.6475, loss: 8.1201, grad_norm: 6.5822
2024-05-07 22:58:12,441 - mmdet - INFO - Exp name: sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d.py
2024-05-07 22:58:12,441 - mmdet - INFO - Epoch [21][200/440]	lr: 1.000e-04, eta: 1:57:43, time: 4.906, data_time: 0.160, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3508, loss_voxel_ce_c_1_2: 0.3493, loss_voxel_sem_scal_c_1_2: 0.7761, loss_voxel_geo_scal_c_1_2: 0.5601, loss_voxel_lovasz_c_1_2: 0.2928, loss_voxel_ce_c_1_4: 0.2113, loss_voxel_sem_scal_c_1_4: 0.3423, loss_voxel_geo_scal_c_1_4: 0.2059, loss_voxel_lovasz_c_1_4: 0.1330, loss_voxel_ce_c_1_8: 0.1182, loss_voxel_sem_scal_c_1_8: 0.1595, loss_voxel_geo_scal_c_1_8: 0.0742, loss_voxel_lovasz_c_1_8: 0.0591, loss_voxel_ce_c_0: 0.4417, loss_voxel_sem_scal_c_0: 1.8496, loss_voxel_geo_scal_c_0: 1.4184, loss_voxel_lovasz_c_0: 0.6495, loss: 8.1397, grad_norm: 7.0118
2024-05-07 23:02:18,120 - mmdet - INFO - Epoch [21][250/440]	lr: 1.000e-04, eta: 1:54:00, time: 4.914, data_time: 0.167, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3519, loss_voxel_ce_c_1_2: 0.3479, loss_voxel_sem_scal_c_1_2: 0.7724, loss_voxel_geo_scal_c_1_2: 0.5588, loss_voxel_lovasz_c_1_2: 0.2925, loss_voxel_ce_c_1_4: 0.2104, loss_voxel_sem_scal_c_1_4: 0.3449, loss_voxel_geo_scal_c_1_4: 0.2047, loss_voxel_lovasz_c_1_4: 0.1332, loss_voxel_ce_c_1_8: 0.1171, loss_voxel_sem_scal_c_1_8: 0.1567, loss_voxel_geo_scal_c_1_8: 0.0736, loss_voxel_lovasz_c_1_8: 0.0588, loss_voxel_ce_c_0: 0.4378, loss_voxel_sem_scal_c_0: 1.8532, loss_voxel_geo_scal_c_0: 1.4097, loss_voxel_lovasz_c_0: 0.6481, loss: 8.1195, grad_norm: 6.3397
2024-05-07 23:06:23,714 - mmdet - INFO - Epoch [21][300/440]	lr: 1.000e-04, eta: 1:50:16, time: 4.912, data_time: 0.162, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3539, loss_voxel_ce_c_1_2: 0.3429, loss_voxel_sem_scal_c_1_2: 0.7674, loss_voxel_geo_scal_c_1_2: 0.5603, loss_voxel_lovasz_c_1_2: 0.2903, loss_voxel_ce_c_1_4: 0.2077, loss_voxel_sem_scal_c_1_4: 0.3334, loss_voxel_geo_scal_c_1_4: 0.2043, loss_voxel_lovasz_c_1_4: 0.1310, loss_voxel_ce_c_1_8: 0.1153, loss_voxel_sem_scal_c_1_8: 0.1529, loss_voxel_geo_scal_c_1_8: 0.0735, loss_voxel_lovasz_c_1_8: 0.0581, loss_voxel_ce_c_0: 0.4345, loss_voxel_sem_scal_c_0: 1.8669, loss_voxel_geo_scal_c_0: 1.4197, loss_voxel_lovasz_c_0: 0.6473, loss: 8.1071, grad_norm: 6.4783
2024-05-07 23:10:29,743 - mmdet - INFO - Epoch [21][350/440]	lr: 1.000e-04, eta: 1:46:33, time: 4.921, data_time: 0.165, memory: 17157, loss_depth: 0.1482, render_dep_loss_c_1_2: 0.3562, loss_voxel_ce_c_1_2: 0.3486, loss_voxel_sem_scal_c_1_2: 0.7806, loss_voxel_geo_scal_c_1_2: 0.5604, loss_voxel_lovasz_c_1_2: 0.2930, loss_voxel_ce_c_1_4: 0.2117, loss_voxel_sem_scal_c_1_4: 0.3471, loss_voxel_geo_scal_c_1_4: 0.2053, loss_voxel_lovasz_c_1_4: 0.1333, loss_voxel_ce_c_1_8: 0.1175, loss_voxel_sem_scal_c_1_8: 0.1558, loss_voxel_geo_scal_c_1_8: 0.0740, loss_voxel_lovasz_c_1_8: 0.0588, loss_voxel_ce_c_0: 0.4413, loss_voxel_sem_scal_c_0: 1.8749, loss_voxel_geo_scal_c_0: 1.4160, loss_voxel_lovasz_c_0: 0.6497, loss: 8.1726, grad_norm: 7.1438
2024-05-07 23:14:35,699 - mmdet - INFO - Epoch [21][400/440]	lr: 1.000e-04, eta: 1:42:49, time: 4.919, data_time: 0.161, memory: 17157, loss_depth: 0.1488, render_dep_loss_c_1_2: 0.3568, loss_voxel_ce_c_1_2: 0.3505, loss_voxel_sem_scal_c_1_2: 0.7784, loss_voxel_geo_scal_c_1_2: 0.5642, loss_voxel_lovasz_c_1_2: 0.2916, loss_voxel_ce_c_1_4: 0.2120, loss_voxel_sem_scal_c_1_4: 0.3477, loss_voxel_geo_scal_c_1_4: 0.2078, loss_voxel_lovasz_c_1_4: 0.1327, loss_voxel_ce_c_1_8: 0.1183, loss_voxel_sem_scal_c_1_8: 0.1564, loss_voxel_geo_scal_c_1_8: 0.0751, loss_voxel_lovasz_c_1_8: 0.0585, loss_voxel_ce_c_0: 0.4425, loss_voxel_sem_scal_c_0: 1.8672, loss_voxel_geo_scal_c_0: 1.4203, loss_voxel_lovasz_c_0: 0.6474, loss: 8.1760, grad_norm: 6.5622
2024-05-07 23:17:51,551 - mmdet - INFO - Saving checkpoint at 21 epochs
2024-05-07 23:17:52,819 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_21_ema.pth
2024-05-07 23:22:08,376 - mmdet - INFO - Epoch [22][50/440]	lr: 1.000e-04, eta: 1:35:40, time: 5.110, data_time: 0.350, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3518, loss_voxel_ce_c_1_2: 0.3451, loss_voxel_sem_scal_c_1_2: 0.7526, loss_voxel_geo_scal_c_1_2: 0.5588, loss_voxel_lovasz_c_1_2: 0.2901, loss_voxel_ce_c_1_4: 0.2091, loss_voxel_sem_scal_c_1_4: 0.3334, loss_voxel_geo_scal_c_1_4: 0.2050, loss_voxel_lovasz_c_1_4: 0.1319, loss_voxel_ce_c_1_8: 0.1160, loss_voxel_sem_scal_c_1_8: 0.1509, loss_voxel_geo_scal_c_1_8: 0.0735, loss_voxel_lovasz_c_1_8: 0.0579, loss_voxel_ce_c_0: 0.4346, loss_voxel_sem_scal_c_0: 1.8250, loss_voxel_geo_scal_c_0: 1.4078, loss_voxel_lovasz_c_0: 0.6449, loss: 8.0363, grad_norm: 6.6338
2024-05-07 23:26:13,357 - mmdet - INFO - Epoch [22][100/440]	lr: 1.000e-04, eta: 1:31:56, time: 4.900, data_time: 0.157, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3524, loss_voxel_ce_c_1_2: 0.3463, loss_voxel_sem_scal_c_1_2: 0.7699, loss_voxel_geo_scal_c_1_2: 0.5609, loss_voxel_lovasz_c_1_2: 0.2897, loss_voxel_ce_c_1_4: 0.2100, loss_voxel_sem_scal_c_1_4: 0.3411, loss_voxel_geo_scal_c_1_4: 0.2058, loss_voxel_lovasz_c_1_4: 0.1318, loss_voxel_ce_c_1_8: 0.1162, loss_voxel_sem_scal_c_1_8: 0.1541, loss_voxel_geo_scal_c_1_8: 0.0744, loss_voxel_lovasz_c_1_8: 0.0579, loss_voxel_ce_c_0: 0.4381, loss_voxel_sem_scal_c_0: 1.8487, loss_voxel_geo_scal_c_0: 1.4170, loss_voxel_lovasz_c_0: 0.6443, loss: 8.1063, grad_norm: 6.2680
2024-05-07 23:30:18,880 - mmdet - INFO - Epoch [22][150/440]	lr: 1.000e-04, eta: 1:28:12, time: 4.910, data_time: 0.155, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3542, loss_voxel_ce_c_1_2: 0.3477, loss_voxel_sem_scal_c_1_2: 0.7609, loss_voxel_geo_scal_c_1_2: 0.5591, loss_voxel_lovasz_c_1_2: 0.2902, loss_voxel_ce_c_1_4: 0.2105, loss_voxel_sem_scal_c_1_4: 0.3332, loss_voxel_geo_scal_c_1_4: 0.2049, loss_voxel_lovasz_c_1_4: 0.1315, loss_voxel_ce_c_1_8: 0.1168, loss_voxel_sem_scal_c_1_8: 0.1522, loss_voxel_geo_scal_c_1_8: 0.0739, loss_voxel_lovasz_c_1_8: 0.0584, loss_voxel_ce_c_0: 0.4407, loss_voxel_sem_scal_c_0: 1.8365, loss_voxel_geo_scal_c_0: 1.4085, loss_voxel_lovasz_c_0: 0.6455, loss: 8.0726, grad_norm: 6.7109
2024-05-07 23:34:24,463 - mmdet - INFO - Epoch [22][200/440]	lr: 1.000e-04, eta: 1:24:29, time: 4.912, data_time: 0.152, memory: 17157, loss_depth: 0.1473, render_dep_loss_c_1_2: 0.3510, loss_voxel_ce_c_1_2: 0.3453, loss_voxel_sem_scal_c_1_2: 0.7700, loss_voxel_geo_scal_c_1_2: 0.5553, loss_voxel_lovasz_c_1_2: 0.2910, loss_voxel_ce_c_1_4: 0.2088, loss_voxel_sem_scal_c_1_4: 0.3442, loss_voxel_geo_scal_c_1_4: 0.2038, loss_voxel_lovasz_c_1_4: 0.1323, loss_voxel_ce_c_1_8: 0.1153, loss_voxel_sem_scal_c_1_8: 0.1554, loss_voxel_geo_scal_c_1_8: 0.0733, loss_voxel_lovasz_c_1_8: 0.0584, loss_voxel_ce_c_0: 0.4371, loss_voxel_sem_scal_c_0: 1.8497, loss_voxel_geo_scal_c_0: 1.3979, loss_voxel_lovasz_c_0: 0.6457, loss: 8.0817, grad_norm: 7.0974
2024-05-07 23:38:28,837 - mmdet - INFO - Epoch [22][250/440]	lr: 1.000e-04, eta: 1:20:44, time: 4.887, data_time: 0.156, memory: 17157, loss_depth: 0.1472, render_dep_loss_c_1_2: 0.3520, loss_voxel_ce_c_1_2: 0.3464, loss_voxel_sem_scal_c_1_2: 0.7728, loss_voxel_geo_scal_c_1_2: 0.5602, loss_voxel_lovasz_c_1_2: 0.2916, loss_voxel_ce_c_1_4: 0.2091, loss_voxel_sem_scal_c_1_4: 0.3396, loss_voxel_geo_scal_c_1_4: 0.2054, loss_voxel_lovasz_c_1_4: 0.1324, loss_voxel_ce_c_1_8: 0.1158, loss_voxel_sem_scal_c_1_8: 0.1551, loss_voxel_geo_scal_c_1_8: 0.0739, loss_voxel_lovasz_c_1_8: 0.0585, loss_voxel_ce_c_0: 0.4371, loss_voxel_sem_scal_c_0: 1.8589, loss_voxel_geo_scal_c_0: 1.4085, loss_voxel_lovasz_c_0: 0.6466, loss: 8.1112, grad_norm: 6.9739
2024-05-07 23:42:33,876 - mmdet - INFO - Epoch [22][300/440]	lr: 1.000e-04, eta: 1:17:00, time: 4.901, data_time: 0.155, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3533, loss_voxel_ce_c_1_2: 0.3515, loss_voxel_sem_scal_c_1_2: 0.7768, loss_voxel_geo_scal_c_1_2: 0.5632, loss_voxel_lovasz_c_1_2: 0.2924, loss_voxel_ce_c_1_4: 0.2133, loss_voxel_sem_scal_c_1_4: 0.3399, loss_voxel_geo_scal_c_1_4: 0.2074, loss_voxel_lovasz_c_1_4: 0.1327, loss_voxel_ce_c_1_8: 0.1188, loss_voxel_sem_scal_c_1_8: 0.1589, loss_voxel_geo_scal_c_1_8: 0.0749, loss_voxel_lovasz_c_1_8: 0.0592, loss_voxel_ce_c_0: 0.4431, loss_voxel_sem_scal_c_0: 1.8594, loss_voxel_geo_scal_c_0: 1.4146, loss_voxel_lovasz_c_0: 0.6483, loss: 8.1554, grad_norm: 6.7383
2024-05-07 23:46:38,320 - mmdet - INFO - Epoch [22][350/440]	lr: 1.000e-04, eta: 1:13:15, time: 4.889, data_time: 0.156, memory: 17157, loss_depth: 0.1476, render_dep_loss_c_1_2: 0.3546, loss_voxel_ce_c_1_2: 0.3494, loss_voxel_sem_scal_c_1_2: 0.7643, loss_voxel_geo_scal_c_1_2: 0.5608, loss_voxel_lovasz_c_1_2: 0.2900, loss_voxel_ce_c_1_4: 0.2116, loss_voxel_sem_scal_c_1_4: 0.3362, loss_voxel_geo_scal_c_1_4: 0.2056, loss_voxel_lovasz_c_1_4: 0.1316, loss_voxel_ce_c_1_8: 0.1170, loss_voxel_sem_scal_c_1_8: 0.1564, loss_voxel_geo_scal_c_1_8: 0.0745, loss_voxel_lovasz_c_1_8: 0.0584, loss_voxel_ce_c_0: 0.4409, loss_voxel_sem_scal_c_0: 1.8570, loss_voxel_geo_scal_c_0: 1.4170, loss_voxel_lovasz_c_0: 0.6448, loss: 8.1177, grad_norm: 7.0207
2024-05-07 23:50:43,050 - mmdet - INFO - Epoch [22][400/440]	lr: 1.000e-04, eta: 1:09:30, time: 4.895, data_time: 0.156, memory: 17157, loss_depth: 0.1482, render_dep_loss_c_1_2: 0.3561, loss_voxel_ce_c_1_2: 0.3449, loss_voxel_sem_scal_c_1_2: 0.7670, loss_voxel_geo_scal_c_1_2: 0.5629, loss_voxel_lovasz_c_1_2: 0.2898, loss_voxel_ce_c_1_4: 0.2093, loss_voxel_sem_scal_c_1_4: 0.3339, loss_voxel_geo_scal_c_1_4: 0.2075, loss_voxel_lovasz_c_1_4: 0.1312, loss_voxel_ce_c_1_8: 0.1162, loss_voxel_sem_scal_c_1_8: 0.1532, loss_voxel_geo_scal_c_1_8: 0.0749, loss_voxel_lovasz_c_1_8: 0.0580, loss_voxel_ce_c_0: 0.4358, loss_voxel_sem_scal_c_0: 1.8478, loss_voxel_geo_scal_c_0: 1.4195, loss_voxel_lovasz_c_0: 0.6450, loss: 8.1008, grad_norm: 7.7940
2024-05-07 23:53:58,266 - mmdet - INFO - Saving checkpoint at 22 epochs
2024-05-07 23:53:59,861 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_22_ema.pth
2024-05-07 23:58:18,701 - mmdet - INFO - Epoch [23][50/440]	lr: 1.000e-04, eta: 1:02:30, time: 5.175, data_time: 0.412, memory: 17157, loss_depth: 0.1476, render_dep_loss_c_1_2: 0.3535, loss_voxel_ce_c_1_2: 0.3404, loss_voxel_sem_scal_c_1_2: 0.7741, loss_voxel_geo_scal_c_1_2: 0.5544, loss_voxel_lovasz_c_1_2: 0.2907, loss_voxel_ce_c_1_4: 0.2074, loss_voxel_sem_scal_c_1_4: 0.3417, loss_voxel_geo_scal_c_1_4: 0.2035, loss_voxel_lovasz_c_1_4: 0.1321, loss_voxel_ce_c_1_8: 0.1145, loss_voxel_sem_scal_c_1_8: 0.1536, loss_voxel_geo_scal_c_1_8: 0.0731, loss_voxel_lovasz_c_1_8: 0.0582, loss_voxel_ce_c_0: 0.4285, loss_voxel_sem_scal_c_0: 1.8720, loss_voxel_geo_scal_c_0: 1.3958, loss_voxel_lovasz_c_0: 0.6458, loss: 8.0868, grad_norm: 6.2634
2024-05-08 00:02:26,759 - mmdet - INFO - Epoch [23][100/440]	lr: 1.000e-04, eta: 0:58:45, time: 4.961, data_time: 0.218, memory: 17157, loss_depth: 0.1478, render_dep_loss_c_1_2: 0.3546, loss_voxel_ce_c_1_2: 0.3457, loss_voxel_sem_scal_c_1_2: 0.7679, loss_voxel_geo_scal_c_1_2: 0.5582, loss_voxel_lovasz_c_1_2: 0.2891, loss_voxel_ce_c_1_4: 0.2090, loss_voxel_sem_scal_c_1_4: 0.3300, loss_voxel_geo_scal_c_1_4: 0.2049, loss_voxel_lovasz_c_1_4: 0.1308, loss_voxel_ce_c_1_8: 0.1159, loss_voxel_sem_scal_c_1_8: 0.1514, loss_voxel_geo_scal_c_1_8: 0.0738, loss_voxel_lovasz_c_1_8: 0.0578, loss_voxel_ce_c_0: 0.4378, loss_voxel_sem_scal_c_0: 1.8614, loss_voxel_geo_scal_c_0: 1.4057, loss_voxel_lovasz_c_0: 0.6450, loss: 8.0868, grad_norm: 6.7169
2024-05-08 00:06:35,304 - mmdet - INFO - Epoch [23][150/440]	lr: 1.000e-04, eta: 0:55:01, time: 4.971, data_time: 0.215, memory: 17157, loss_depth: 0.1483, render_dep_loss_c_1_2: 0.3538, loss_voxel_ce_c_1_2: 0.3457, loss_voxel_sem_scal_c_1_2: 0.7563, loss_voxel_geo_scal_c_1_2: 0.5564, loss_voxel_lovasz_c_1_2: 0.2889, loss_voxel_ce_c_1_4: 0.2091, loss_voxel_sem_scal_c_1_4: 0.3389, loss_voxel_geo_scal_c_1_4: 0.2042, loss_voxel_lovasz_c_1_4: 0.1313, loss_voxel_ce_c_1_8: 0.1162, loss_voxel_sem_scal_c_1_8: 0.1531, loss_voxel_geo_scal_c_1_8: 0.0732, loss_voxel_lovasz_c_1_8: 0.0578, loss_voxel_ce_c_0: 0.4350, loss_voxel_sem_scal_c_0: 1.8195, loss_voxel_geo_scal_c_0: 1.4053, loss_voxel_lovasz_c_0: 0.6423, loss: 8.0353, grad_norm: 5.8183
2024-05-08 00:10:43,254 - mmdet - INFO - Epoch [23][200/440]	lr: 1.000e-04, eta: 0:51:16, time: 4.959, data_time: 0.216, memory: 17157, loss_depth: 0.1477, render_dep_loss_c_1_2: 0.3526, loss_voxel_ce_c_1_2: 0.3445, loss_voxel_sem_scal_c_1_2: 0.7734, loss_voxel_geo_scal_c_1_2: 0.5546, loss_voxel_lovasz_c_1_2: 0.2904, loss_voxel_ce_c_1_4: 0.2082, loss_voxel_sem_scal_c_1_4: 0.3339, loss_voxel_geo_scal_c_1_4: 0.2033, loss_voxel_lovasz_c_1_4: 0.1317, loss_voxel_ce_c_1_8: 0.1156, loss_voxel_sem_scal_c_1_8: 0.1533, loss_voxel_geo_scal_c_1_8: 0.0732, loss_voxel_lovasz_c_1_8: 0.0579, loss_voxel_ce_c_0: 0.4371, loss_voxel_sem_scal_c_0: 1.8570, loss_voxel_geo_scal_c_0: 1.3992, loss_voxel_lovasz_c_0: 0.6457, loss: 8.0793, grad_norm: 6.6683
2024-05-08 00:14:52,123 - mmdet - INFO - Epoch [23][250/440]	lr: 1.000e-04, eta: 0:47:32, time: 4.978, data_time: 0.214, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3516, loss_voxel_ce_c_1_2: 0.3434, loss_voxel_sem_scal_c_1_2: 0.7684, loss_voxel_geo_scal_c_1_2: 0.5559, loss_voxel_lovasz_c_1_2: 0.2885, loss_voxel_ce_c_1_4: 0.2084, loss_voxel_sem_scal_c_1_4: 0.3344, loss_voxel_geo_scal_c_1_4: 0.2037, loss_voxel_lovasz_c_1_4: 0.1307, loss_voxel_ce_c_1_8: 0.1154, loss_voxel_sem_scal_c_1_8: 0.1542, loss_voxel_geo_scal_c_1_8: 0.0736, loss_voxel_lovasz_c_1_8: 0.0578, loss_voxel_ce_c_0: 0.4348, loss_voxel_sem_scal_c_0: 1.8515, loss_voxel_geo_scal_c_0: 1.4028, loss_voxel_lovasz_c_0: 0.6423, loss: 8.0653, grad_norm: 6.8527
2024-05-08 00:18:59,940 - mmdet - INFO - Epoch [23][300/440]	lr: 1.000e-04, eta: 0:43:47, time: 4.956, data_time: 0.216, memory: 17157, loss_depth: 0.1476, render_dep_loss_c_1_2: 0.3514, loss_voxel_ce_c_1_2: 0.3413, loss_voxel_sem_scal_c_1_2: 0.7702, loss_voxel_geo_scal_c_1_2: 0.5579, loss_voxel_lovasz_c_1_2: 0.2898, loss_voxel_ce_c_1_4: 0.2066, loss_voxel_sem_scal_c_1_4: 0.3365, loss_voxel_geo_scal_c_1_4: 0.2045, loss_voxel_lovasz_c_1_4: 0.1314, loss_voxel_ce_c_1_8: 0.1142, loss_voxel_sem_scal_c_1_8: 0.1535, loss_voxel_geo_scal_c_1_8: 0.0740, loss_voxel_lovasz_c_1_8: 0.0578, loss_voxel_ce_c_0: 0.4309, loss_voxel_sem_scal_c_0: 1.8677, loss_voxel_geo_scal_c_0: 1.4060, loss_voxel_lovasz_c_0: 0.6451, loss: 8.0864, grad_norm: 6.8167
2024-05-08 00:23:08,217 - mmdet - INFO - Epoch [23][350/440]	lr: 1.000e-04, eta: 0:40:01, time: 4.966, data_time: 0.220, memory: 17157, loss_depth: 0.1479, render_dep_loss_c_1_2: 0.3559, loss_voxel_ce_c_1_2: 0.3477, loss_voxel_sem_scal_c_1_2: 0.7533, loss_voxel_geo_scal_c_1_2: 0.5581, loss_voxel_lovasz_c_1_2: 0.2903, loss_voxel_ce_c_1_4: 0.2112, loss_voxel_sem_scal_c_1_4: 0.3377, loss_voxel_geo_scal_c_1_4: 0.2046, loss_voxel_lovasz_c_1_4: 0.1321, loss_voxel_ce_c_1_8: 0.1170, loss_voxel_sem_scal_c_1_8: 0.1528, loss_voxel_geo_scal_c_1_8: 0.0739, loss_voxel_lovasz_c_1_8: 0.0585, loss_voxel_ce_c_0: 0.4403, loss_voxel_sem_scal_c_0: 1.8054, loss_voxel_geo_scal_c_0: 1.4050, loss_voxel_lovasz_c_0: 0.6445, loss: 8.0362, grad_norm: 6.1496
2024-05-08 00:27:16,494 - mmdet - INFO - Epoch [23][400/440]	lr: 1.000e-04, eta: 0:36:16, time: 4.966, data_time: 0.213, memory: 17157, loss_depth: 0.1480, render_dep_loss_c_1_2: 0.3515, loss_voxel_ce_c_1_2: 0.3507, loss_voxel_sem_scal_c_1_2: 0.7632, loss_voxel_geo_scal_c_1_2: 0.5566, loss_voxel_lovasz_c_1_2: 0.2900, loss_voxel_ce_c_1_4: 0.2121, loss_voxel_sem_scal_c_1_4: 0.3366, loss_voxel_geo_scal_c_1_4: 0.2046, loss_voxel_lovasz_c_1_4: 0.1314, loss_voxel_ce_c_1_8: 0.1179, loss_voxel_sem_scal_c_1_8: 0.1523, loss_voxel_geo_scal_c_1_8: 0.0737, loss_voxel_lovasz_c_1_8: 0.0579, loss_voxel_ce_c_0: 0.4426, loss_voxel_sem_scal_c_0: 1.8442, loss_voxel_geo_scal_c_0: 1.4043, loss_voxel_lovasz_c_0: 0.6455, loss: 8.0832, grad_norm: 6.3656
2024-05-08 00:30:34,641 - mmdet - INFO - Saving checkpoint at 23 epochs
2024-05-08 00:30:35,945 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_23_ema.pth
2024-05-08 00:34:53,287 - mmdet - INFO - Epoch [24][50/440]	lr: 1.000e-04, eta: 0:29:22, time: 5.145, data_time: 0.373, memory: 17157, loss_depth: 0.1472, render_dep_loss_c_1_2: 0.3540, loss_voxel_ce_c_1_2: 0.3412, loss_voxel_sem_scal_c_1_2: 0.7425, loss_voxel_geo_scal_c_1_2: 0.5520, loss_voxel_lovasz_c_1_2: 0.2871, loss_voxel_ce_c_1_4: 0.2062, loss_voxel_sem_scal_c_1_4: 0.3306, loss_voxel_geo_scal_c_1_4: 0.2022, loss_voxel_lovasz_c_1_4: 0.1302, loss_voxel_ce_c_1_8: 0.1134, loss_voxel_sem_scal_c_1_8: 0.1478, loss_voxel_geo_scal_c_1_8: 0.0727, loss_voxel_lovasz_c_1_8: 0.0574, loss_voxel_ce_c_0: 0.4314, loss_voxel_sem_scal_c_0: 1.7739, loss_voxel_geo_scal_c_0: 1.3890, loss_voxel_lovasz_c_0: 0.6378, loss: 7.9168, grad_norm: 5.6124
2024-05-08 00:39:01,437 - mmdet - INFO - Epoch [24][100/440]	lr: 1.000e-04, eta: 0:25:37, time: 4.963, data_time: 0.191, memory: 17157, loss_depth: 0.1470, render_dep_loss_c_1_2: 0.3530, loss_voxel_ce_c_1_2: 0.3430, loss_voxel_sem_scal_c_1_2: 0.7459, loss_voxel_geo_scal_c_1_2: 0.5487, loss_voxel_lovasz_c_1_2: 0.2877, loss_voxel_ce_c_1_4: 0.2070, loss_voxel_sem_scal_c_1_4: 0.3303, loss_voxel_geo_scal_c_1_4: 0.2012, loss_voxel_lovasz_c_1_4: 0.1305, loss_voxel_ce_c_1_8: 0.1146, loss_voxel_sem_scal_c_1_8: 0.1498, loss_voxel_geo_scal_c_1_8: 0.0723, loss_voxel_lovasz_c_1_8: 0.0575, loss_voxel_ce_c_0: 0.4333, loss_voxel_sem_scal_c_0: 1.8015, loss_voxel_geo_scal_c_0: 1.3897, loss_voxel_lovasz_c_0: 0.6418, loss: 7.9547, grad_norm: 6.5639
2024-05-08 00:43:09,863 - mmdet - INFO - Epoch [24][150/440]	lr: 1.000e-04, eta: 0:21:51, time: 4.969, data_time: 0.191, memory: 17157, loss_depth: 0.1473, render_dep_loss_c_1_2: 0.3499, loss_voxel_ce_c_1_2: 0.3399, loss_voxel_sem_scal_c_1_2: 0.7645, loss_voxel_geo_scal_c_1_2: 0.5517, loss_voxel_lovasz_c_1_2: 0.2875, loss_voxel_ce_c_1_4: 0.2052, loss_voxel_sem_scal_c_1_4: 0.3308, loss_voxel_geo_scal_c_1_4: 0.2026, loss_voxel_lovasz_c_1_4: 0.1304, loss_voxel_ce_c_1_8: 0.1132, loss_voxel_sem_scal_c_1_8: 0.1521, loss_voxel_geo_scal_c_1_8: 0.0729, loss_voxel_lovasz_c_1_8: 0.0574, loss_voxel_ce_c_0: 0.4296, loss_voxel_sem_scal_c_0: 1.8398, loss_voxel_geo_scal_c_0: 1.3920, loss_voxel_lovasz_c_0: 0.6408, loss: 8.0078, grad_norm: 6.6583
2024-05-08 00:47:17,161 - mmdet - INFO - Epoch [24][200/440]	lr: 1.000e-04, eta: 0:18:06, time: 4.946, data_time: 0.198, memory: 17157, loss_depth: 0.1473, render_dep_loss_c_1_2: 0.3511, loss_voxel_ce_c_1_2: 0.3453, loss_voxel_sem_scal_c_1_2: 0.7575, loss_voxel_geo_scal_c_1_2: 0.5541, loss_voxel_lovasz_c_1_2: 0.2868, loss_voxel_ce_c_1_4: 0.2079, loss_voxel_sem_scal_c_1_4: 0.3292, loss_voxel_geo_scal_c_1_4: 0.2029, loss_voxel_lovasz_c_1_4: 0.1300, loss_voxel_ce_c_1_8: 0.1147, loss_voxel_sem_scal_c_1_8: 0.1472, loss_voxel_geo_scal_c_1_8: 0.0729, loss_voxel_lovasz_c_1_8: 0.0568, loss_voxel_ce_c_0: 0.4345, loss_voxel_sem_scal_c_0: 1.8165, loss_voxel_geo_scal_c_0: 1.4028, loss_voxel_lovasz_c_0: 0.6388, loss: 7.9964, grad_norm: 5.9204
2024-05-08 00:51:24,342 - mmdet - INFO - Epoch [24][250/440]	lr: 1.000e-04, eta: 0:14:20, time: 4.944, data_time: 0.192, memory: 17157, loss_depth: 0.1474, render_dep_loss_c_1_2: 0.3512, loss_voxel_ce_c_1_2: 0.3428, loss_voxel_sem_scal_c_1_2: 0.7616, loss_voxel_geo_scal_c_1_2: 0.5538, loss_voxel_lovasz_c_1_2: 0.2892, loss_voxel_ce_c_1_4: 0.2079, loss_voxel_sem_scal_c_1_4: 0.3377, loss_voxel_geo_scal_c_1_4: 0.2029, loss_voxel_lovasz_c_1_4: 0.1313, loss_voxel_ce_c_1_8: 0.1147, loss_voxel_sem_scal_c_1_8: 0.1504, loss_voxel_geo_scal_c_1_8: 0.0729, loss_voxel_lovasz_c_1_8: 0.0574, loss_voxel_ce_c_0: 0.4344, loss_voxel_sem_scal_c_0: 1.8341, loss_voxel_geo_scal_c_0: 1.3938, loss_voxel_lovasz_c_0: 0.6439, loss: 8.0274, grad_norm: 6.2282
2024-05-08 00:55:31,346 - mmdet - INFO - Epoch [24][300/440]	lr: 1.000e-04, eta: 0:10:34, time: 4.940, data_time: 0.183, memory: 17157, loss_depth: 0.1476, render_dep_loss_c_1_2: 0.3523, loss_voxel_ce_c_1_2: 0.3442, loss_voxel_sem_scal_c_1_2: 0.7544, loss_voxel_geo_scal_c_1_2: 0.5539, loss_voxel_lovasz_c_1_2: 0.2887, loss_voxel_ce_c_1_4: 0.2077, loss_voxel_sem_scal_c_1_4: 0.3325, loss_voxel_geo_scal_c_1_4: 0.2038, loss_voxel_lovasz_c_1_4: 0.1313, loss_voxel_ce_c_1_8: 0.1158, loss_voxel_sem_scal_c_1_8: 0.1532, loss_voxel_geo_scal_c_1_8: 0.0732, loss_voxel_lovasz_c_1_8: 0.0577, loss_voxel_ce_c_0: 0.4358, loss_voxel_sem_scal_c_0: 1.8307, loss_voxel_geo_scal_c_0: 1.3952, loss_voxel_lovasz_c_0: 0.6428, loss: 8.0207, grad_norm: 6.5443
2024-05-08 00:59:38,465 - mmdet - INFO - Epoch [24][350/440]	lr: 1.000e-04, eta: 0:06:47, time: 4.942, data_time: 0.179, memory: 17157, loss_depth: 0.1472, render_dep_loss_c_1_2: 0.3499, loss_voxel_ce_c_1_2: 0.3434, loss_voxel_sem_scal_c_1_2: 0.7496, loss_voxel_geo_scal_c_1_2: 0.5517, loss_voxel_lovasz_c_1_2: 0.2885, loss_voxel_ce_c_1_4: 0.2082, loss_voxel_sem_scal_c_1_4: 0.3264, loss_voxel_geo_scal_c_1_4: 0.2023, loss_voxel_lovasz_c_1_4: 0.1306, loss_voxel_ce_c_1_8: 0.1157, loss_voxel_sem_scal_c_1_8: 0.1497, loss_voxel_geo_scal_c_1_8: 0.0728, loss_voxel_lovasz_c_1_8: 0.0576, loss_voxel_ce_c_0: 0.4363, loss_voxel_sem_scal_c_0: 1.7970, loss_voxel_geo_scal_c_0: 1.3899, loss_voxel_lovasz_c_0: 0.6414, loss: 7.9581, grad_norm: 6.1200
2024-05-08 01:03:44,428 - mmdet - INFO - Epoch [24][400/440]	lr: 1.000e-04, eta: 0:03:01, time: 4.919, data_time: 0.185, memory: 17157, loss_depth: 0.1474, render_dep_loss_c_1_2: 0.3527, loss_voxel_ce_c_1_2: 0.3464, loss_voxel_sem_scal_c_1_2: 0.7546, loss_voxel_geo_scal_c_1_2: 0.5547, loss_voxel_lovasz_c_1_2: 0.2887, loss_voxel_ce_c_1_4: 0.2090, loss_voxel_sem_scal_c_1_4: 0.3324, loss_voxel_geo_scal_c_1_4: 0.2037, loss_voxel_lovasz_c_1_4: 0.1311, loss_voxel_ce_c_1_8: 0.1153, loss_voxel_sem_scal_c_1_8: 0.1489, loss_voxel_geo_scal_c_1_8: 0.0731, loss_voxel_lovasz_c_1_8: 0.0574, loss_voxel_ce_c_0: 0.4376, loss_voxel_sem_scal_c_0: 1.8141, loss_voxel_geo_scal_c_0: 1.3971, loss_voxel_lovasz_c_0: 0.6429, loss: 8.0071, grad_norm: 6.3296
2024-05-08 01:07:02,100 - mmdet - INFO - Saving checkpoint at 24 epochs
2024-05-08 01:07:03,420 - mmdet - INFO - Saving ema checkpoint at /data/lzm/projects/BEVDetOcc/work_dirs/sparsemsfbocc-r50-stereo4d-longterm8f-soa-nerf-occ3d/epoch_24_ema.pth
